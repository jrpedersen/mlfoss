
@misc{100MeatAnalysis,
  title = {100\% Meat Analysis with {{FOSS X}}-Ray Analyser Installed Inline},
  abstract = {The MeatMaster\texttrademark{} II X-ray meat analysis system can scan fresh, frozen or packed meat for high accuracy determination of fat, weight and foreign objects},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2HWWITIS\\meatmaster-ii.html},
  howpublished = {https://www.fossanalytics.com/en/products/meatmaster-ii},
  language = {en}
}

@misc{200308505Metric,
  title = {[2003.08505] {{A Metric Learning Reality Check}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SAW6Z87K\\2003.html},
  howpublished = {https://arxiv.org/abs/2003.08505}
}

@misc{AddDeformableConvolution,
  title = {Add {{Deformable Convolution}} Operation. by Pedrofreire {$\cdot$} {{Pull Request}} \#1586 {$\cdot$} Pytorch/Vision},
  abstract = {For \#1416, this adds the deformable convolution operation, as described in Deformable Convolutional Networks (https://arxiv.org/abs/1703.06211). The code is based on https://github.com/open-mmlab...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\5X7KFNUV\\1586.html},
  howpublished = {https://github.com/pytorch/vision/pull/1586},
  journal = {GitHub},
  language = {en}
}

@article{advaniHighdimensionalDynamicsGeneralization2017,
  title = {High-Dimensional Dynamics of Generalization Error in Neural Networks},
  author = {Advani, Madhu S. and Saxe, Andrew M.},
  year = {2017},
  month = oct,
  abstract = {We perform an average case analysis of the generalization dynamics of large neural networks trained using gradient descent. We study the practically-relevant ``high-dimensional'' regime where the number of free parameters in the network is on the order of or even larger than the number of examples in the dataset. Using random matrix theory and exact solutions in linear models, we derive the generalization error and training error dynamics of learning and analyze how they depend on the dimensionality of data and signal to noise ratio of the learning problem. We find that the dynamics of gradient descent learning naturally protect against overtraining and overfitting in large networks. Overtraining is worst at intermediate network sizes, when the effective number of free parameters equals the number of samples, and thus can be reduced by making a network smaller or larger. Additionally, in the high-dimensional regime, low generalization error requires starting with small initial weights. We then turn to non-linear neural networks, and show that making networks very large does not harm their generalization performance. On the contrary, it can in fact reduce overtraining, even without early stopping or regularization of any sort. We identify two novel phenomena underlying this behavior in overcomplete models: first, there is a frozen subspace of the weights in which no learning occurs under gradient descent; and second, the statistical properties of the high-dimensional regime yield better-conditioned input correlations which protect against overtraining. We demonstrate that naive application of worst-case theories such as Rademacher complexity are inaccurate in predicting the generalization performance of deep neural networks, and derive an alternative bound which incorporates the frozen subspace and conditioning effects and qualitatively matches the behavior observed in simulation.},
  archivePrefix = {arXiv},
  eprint = {1710.03667},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\MXYLECBS\\Advani and Saxe - 2017 - High-dimensional dynamics of generalization error .pdf},
  journal = {arXiv:1710.03667 [physics, q-bio, stat]},
  keywords = {Computer Science - Machine Learning,Physics - Data Analysis; Statistics and Probability,Quantitative Biology - Neurons and Cognition,read-next,Statistics - Machine Learning},
  language = {en},
  primaryClass = {physics, q-bio, stat}
}

@misc{AgeExtremesEric,
  title = {The {{Age}} of {{Extremes}} by {{Eric Hobsbawm}}: 9780679730057 | {{PenguinRandomHouse}}.Com: {{Books}}},
  shorttitle = {The {{Age}} of {{Extremes}} by {{Eric Hobsbawm}}},
  abstract = {Dividing the century into the Age of Catastrophe, 1914\textendash 1950, the Golden Age, 1950\textendash 1973, and the Landslide, 1973\textendash 1991, Hobsbawm marshals a vast array of data into a volume of unparalleled...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\QCX7RYDF\\the-age-of-extremes-by-eric-hobsbawm.html},
  howpublished = {https://www.penguinrandomhouse.com/books/80963/the-age-of-extremes-by-eric-hobsbawm/},
  journal = {PenguinRandomhouse.com},
  language = {en-US}
}

@article{agrawalDifferentiableConvexOptimization2019,
  title = {Differentiable {{Convex Optimization Layers}}},
  author = {Agrawal, Akshay and Amos, Brandon and Barratt, Shane and Boyd, Stephen and Diamond, Steven and Kolter, Zico},
  year = {2019},
  month = oct,
  abstract = {Recent work has shown how to embed differentiable optimization problems (that is, problems whose solutions can be backpropagated through) as layers within deep learning architectures. This method provides a useful inductive bias for certain problems, but existing software for differentiable optimization layers is rigid and difficult to apply to new settings. In this paper, we propose an approach to differentiating through disciplined convex programs, a subclass of convex optimization problems used by domain-specific languages (DSLs) for convex optimization. We introduce disciplined parametrized programming, a subset of disciplined convex programming, and we show that every disciplined parametrized program can be represented as the composition of an affine map from parameters to problem data, a solver, and an affine map from the solver's solution to a solution of the original problem (a new form we refer to as affine-solver-affine form). We then demonstrate how to efficiently differentiate through each of these components, allowing for end-to-end analytical differentiation through the entire convex program. We implement our methodology in version 1.1 of CVXPY, a popular Python-embedded DSL for convex optimization, and additionally implement differentiable layers for disciplined convex programs in PyTorch and TensorFlow 2.0. Our implementation significantly lowers the barrier to using convex optimization problems in differentiable programs. We present applications in linear machine learning models and in stochastic control, and we show that our layer is competitive (in execution time) compared to specialized differentiable solvers from past work.},
  archivePrefix = {arXiv},
  eprint = {1910.12430},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\DSH4MJZL\\Agrawal et al. - 2019 - Differentiable Convex Optimization Layers.pdf},
  journal = {arXiv:1910.12430 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,read-soon,Statistics - Machine Learning},
  language = {en},
  note = {Comment: In NeurIPS 2019. Code available at https://www.github.com/cvxgrp/cvxpylayers. Authors in alphabetical order},
  primaryClass = {cs, math, stat}
}

@article{agrawalLearningPokePoking2016,
  title = {Learning to {{Poke}} by {{Poking}}: {{Experiential Learning}} of {{Intuitive Physics}}},
  shorttitle = {Learning to {{Poke}} by {{Poking}}},
  author = {Agrawal, Pulkit and Nair, Ashvin V. and Abbeel, Pieter and Malik, Jitendra and Levine, Sergey},
  year = {2016},
  volume = {29},
  pages = {5074--5082},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\39SH5K7I\\Agrawal et al. - 2016 - Learning to Poke by Poking Experiential Learning .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\EVVY42FU\\c203d8a151612acf12457e4d67635a95-Abstract.html;C\:\\Users\\ext1150\\Zotero\\storage\\IUC85ZD3\\c203d8a151612acf12457e4d67635a95-Abstract.html},
  journal = {Advances in Neural Information Processing Systems},
  language = {en}
}

@article{albanieStateofArtReviewingRadicalProposal2020,
  title = {State-of-{{Art}}-{{Reviewing}}: {{A Radical Proposal}} to {{Improve Scientific Publication}}},
  shorttitle = {State-of-{{Art}}-{{Reviewing}}},
  author = {Albanie, Samuel and Thewmore, Jaime and McCraith, Robert and Henriques, Joao F.},
  year = {2020},
  month = mar,
  abstract = {Peer review forms the backbone of modern scientific manuscript evaluation. But after two hundred and eighty-nine years of egalitarian service to the scientific community, does this protocol remain fit for purpose in 2020? In this work, we answer this question in the negative (strong reject, high confidence) and propose instead State-Of-the-Art Review (SOAR), a neoteric reviewing pipeline that serves as a ``plug-and-play'' replacement for peer review. At the heart of our approach is an interpretation of the review process as a multi-objective, massively distributed and extremely-high-latency optimisation, which we scalarise and solve efficiently for PAC and CMT-optimal solutions.},
  archivePrefix = {arXiv},
  eprint = {2003.14415},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FHIGKADW\\Albanie et al. - 2020 - State-of-Art-Reviewing A Radical Proposal to Impr.pdf},
  journal = {arXiv:2003.14415 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  note = {Comment: SIGBOVIK 2020},
  primaryClass = {cs}
}

@misc{AnalysesDeepLearning,
  title = {Analyses of {{Deep Learning}} ({{STATS}} 385)},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6EE487W7\\lecture_videos.html},
  howpublished = {https://stats385.github.io/lecture\_videos}
}

@article{andrychowiczLearningLearnGradient2016,
  title = {Learning to Learn by Gradient Descent by Gradient Descent},
  author = {Andrychowicz, Marcin and Denil, Misha and Gomez, Sergio and Hoffman, Matthew W. and Pfau, David and Schaul, Tom and Shillingford, Brendan and {de Freitas}, Nando},
  year = {2016},
  month = nov,
  abstract = {The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.},
  archivePrefix = {arXiv},
  eprint = {1606.04474},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FF5PFBLY\\Andrychowicz et al. - 2016 - Learning to learn by gradient descent by gradient .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\PAX6GA8F\\1606.html},
  journal = {arXiv:1606.04474 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,lr},
  primaryClass = {cs}
}

@inproceedings{anonymousClassNormalizationZeroShot2020,
  title = {Class {{Normalization}} for {{Zero}}-{{Shot Learning}}},
  booktitle = {Submitted to {{International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2020},
  month = sep,
  abstract = {Normalization techniques have proved to be a crucial ingredient of successful training in a traditional supervised learning regime. However, in zero-shot learning (ZSL) world, these ideas have...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\9IU5P98G\\Anonymous - 2020 - Class Normalization for Zero-Shot Learning.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\NS6NDQ3X\\forum.html},
  language = {en}
}

@inproceedings{anonymousImageWorth16x162020,
  title = {An {{Image}} Is {{Worth}} 16x16 {{Words}}: {{Transformers}} for {{Image Recognition}} at {{Scale}}},
  shorttitle = {An {{Image}} Is {{Worth}} 16x16 {{Words}}},
  booktitle = {Submitted to {{International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2020},
  month = sep,
  abstract = {While the Transformer architecture has become the de-facto standard for natural language processing tasks, its applications to computer vision remain limited. In vision, attention is either applied...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\EGVGAQBN\\Anonymous - 2020 - An Image is Worth 16x16 Words Transformers for Im.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\U2PUHH8T\\forum.html},
  keywords = {read-next},
  language = {en},
  note = {https://github.com/jeonsworld/ViT-pytorch}
}

@inproceedings{anonymousLambdaNetworksModelingLongrange2020,
  title = {{{LambdaNetworks}}: {{Modeling}} Long-Range {{Interactions}} without {{Attention}}},
  shorttitle = {{{LambdaNetworks}}},
  booktitle = {Submitted to {{International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2020},
  month = sep,
  abstract = {We present a general framework for capturing long-range interactions between an input and structured contextual information (e.g. a pixel surrounded by other pixels). Our method, called the lambda...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\XL7B8JTR\\Anonymous - 2020 - LambdaNetworks Modeling long-range Interactions w.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\YNI9PV8X\\forum.html},
  language = {en}
}

@inproceedings{anonymousPredictiveCodingApproximates2020,
  title = {Predictive {{Coding Approximates Backprop}} along {{Arbitrary Computation Graphs}}},
  booktitle = {Submitted to {{International Conference}} on {{Learning Representations}}},
  author = {Anonymous},
  year = {2020},
  month = sep,
  abstract = {The backpropagation of error (backprop) is a powerful algorithm for training machine learning architectures through end-to-end differentiation. Recently it has been shown that backprop in...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TEPB2MKE\\Anonymous - 2020 - Predictive Coding Approximates Backprop along Arbi.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\GMHNM457\\forum.html},
  language = {en}
}

@article{anscombeTRANSFORMATIONPOISSONBINOMIAL,
  title = {{{THE TRANSFORMATION OF POISSON}}, {{BINOMIAL AND NEGATIVE}}-{{BINOMIAL DATA}}},
  author = {Anscombe, F J},
  pages = {9},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RNE5RRGQ\\Anscombe - THE TRANSFORMATION OF POISSON, BINOMIAL AND NEGATI.pdf},
  language = {en}
}

@article{aroraExactComputationInfinitely2019,
  title = {On {{Exact Computation}} with an {{Infinitely Wide Neural Net}}},
  author = {Arora, Sanjeev and Du, Simon S. and Hu, Wei and Li, Zhiyuan and Salakhutdinov, Ruslan and Wang, Ruosong},
  year = {2019},
  month = nov,
  abstract = {How well does a classic deep net architecture like AlexNet or VGG19 classify on a standard dataset such as CIFAR-10 when its width --- namely, number of channels in convolutional layers, and number of nodes in fully-connected internal layers --- is allowed to increase to infinity? Such questions have come to the forefront in the quest to theoretically understand deep learning and its mysteries about optimization and generalization. They also connect deep learning to notions such as Gaussian processes and kernels. A recent paper [Jacot et al., 2018] introduced the Neural Tangent Kernel (NTK) which captures the behavior of fully-connected deep nets in the infinite width limit trained by gradient descent; this object was implicit in some other recent papers. An attraction of such ideas is that a pure kernel-based method is used to capture the power of a fully-trained deep net of infinite width. The current paper gives the first efficient exact algorithm for computing the extension of NTK to convolutional neural nets, which we call Convolutional NTK (CNTK), as well as an efficient GPU implementation of this algorithm. This results in a significant new benchmark for the performance of a pure kernel-based method on CIFAR-10, being \$10\textbackslash\%\$ higher than the methods reported in [Novak et al., 2019], and only \$6\textbackslash\%\$ lower than the performance of the corresponding finite deep net architecture (once batch normalization, etc. are turned off). Theoretically, we also give the first non-asymptotic proof showing that a fully-trained sufficiently wide net is indeed equivalent to the kernel regression predictor using NTK.},
  archivePrefix = {arXiv},
  eprint = {1904.11955},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\EIFU7ZJF\\Arora et al. - 2019 - On Exact Computation with an Infinitely Wide Neura.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\CYMRIGDQ\\1904.html},
  journal = {arXiv:1904.11955 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,generalization,read-soon,Statistics - Machine Learning},
  note = {Comment: In NeurIPS 2019. Code available: https://github.com/ruosongwang/cntk},
  primaryClass = {cs, stat}
}

@article{arulkumaranBriefSurveyDeep2017,
  title = {A {{Brief Survey}} of {{Deep Reinforcement Learning}}},
  author = {Arulkumaran, Kai and Deisenroth, Marc Peter and Brundage, Miles and Bharath, Anil Anthony},
  year = {2017},
  month = nov,
  volume = {34},
  pages = {26--38},
  issn = {1053-5888},
  doi = {10.1109/MSP.2017.2743240},
  abstract = {Deep reinforcement learning is poised to revolutionise the field of AI and represents a step towards building autonomous systems with a higher level understanding of the visual world. Currently, deep learning is enabling reinforcement learning to scale to problems that were previously intractable, such as learning to play video games directly from pixels. Deep reinforcement learning algorithms are also applied to robotics, allowing control policies for robots to be learned directly from camera inputs in the real world. In this survey, we begin with an introduction to the general field of reinforcement learning, then progress to the main streams of value-based and policybased methods. Our survey will cover central algorithms in deep reinforcement learning, including the deep Q-network, trust region policy optimisation, and asynchronous advantage actor-critic. In parallel, we highlight the unique advantages of deep neural networks, focusing on visual understanding via reinforcement learning. To conclude, we describe several current areas of research within the field.},
  archivePrefix = {arXiv},
  eprint = {1708.05866},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\A33L6HWS\\Arulkumaran et al. - 2017 - A Brief Survey of Deep Reinforcement Learning.pdf},
  journal = {IEEE Signal Processing Magazine},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,rl,Statistics - Machine Learning},
  language = {en},
  note = {Comment: IEEE Signal Processing Magazine, Special Issue on Deep Learning for Image Understanding (arXiv extended version)},
  number = {6}
}

@misc{AutoML,
  title = {{{AutoML}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JW6I6A5N\\automl.html},
  language = {en-US}
}

@article{azulayWhyDeepConvolutional2019,
  title = {Why Do Deep Convolutional Networks Generalize so Poorly to Small Image Transformations?},
  author = {Azulay, Aharon and Weiss, Yair},
  year = {2019},
  month = dec,
  abstract = {Convolutional Neural Networks (CNNs) are commonly assumed to be invariant to small image transformations: either because of the convolutional architecture or because they were trained using data augmentation. Recently, several authors have shown that this is not the case: small translations or rescalings of the input image can drastically change the network's prediction. In this paper, we quantify this phenomena and ask why neither the convolutional architecture nor data augmentation are sufficient to achieve the desired invariance. Specifically, we show that the convolutional architecture does not give invariance since architectures ignore the classical sampling theorem, and data augmentation does not give invariance because the CNNs learn to be invariant to transformations only for images that are very similar to typical images from the training set. We discuss two possible solutions to this problem: (1) antialiasing the intermediate representations and (2) increasing data augmentation and show that they provide only a partial solution at best. Taken together, our results indicate that the problem of insuring invariance to small image transformations in neural networks while preserving high accuracy remains unsolved.},
  archivePrefix = {arXiv},
  eprint = {1805.12177},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Y7VW2ERQ\\Azulay and Weiss - 2019 - Why do deep convolutional networks generalize so p.pdf},
  journal = {arXiv:1805.12177 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-soon},
  language = {en},
  primaryClass = {cs}
}

@article{baeDoesAdamOptimizer2019,
  title = {Does {{Adam}} Optimizer Keep Close to the Optimal Point?},
  author = {Bae, Kiwook and Ryu, Heechang and Shin, Hayong},
  year = {2019},
  month = nov,
  abstract = {The adaptive optimizer for training neural networks has continually evolved to overcome the limitations of the previously proposed adaptive methods. Recent studies have found the rare counterexamples that Adam cannot converge to the optimal point. Those counterexamples reveal the distortion of Adam due to a small second momentum from a small gradient. Unlike previous studies, we show Adam cannot keep closer to the optimal point for not only the counterexamples but also a general convex region when the effective learning rate exceeds the certain bound. Subsequently, we propose an algorithm that overcomes Adam's limitation and ensures that it can reach and stay at the optimal point region.},
  archivePrefix = {arXiv},
  eprint = {1911.00289},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\M3Y4QJAK\\Bae et al. - 2019 - Does Adam optimizer keep close to the optimal poin.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\XBNJE4EN\\1911.html},
  journal = {arXiv:1911.00289 [cs, stat]},
  keywords = {Computer Science - Machine Learning,lr,Statistics - Machine Learning},
  note = {Comment: Accepted as a workshop paper at the 33rd Conference on Neural Information Processing Systems (NeurIPS 2019), Vancouver, Canada},
  primaryClass = {cs, stat}
}

@article{bartlettSpectrallynormalizedMarginBounds2017,
  title = {Spectrally-Normalized Margin Bounds for Neural Networks},
  author = {Bartlett, Peter and Foster, Dylan J. and Telgarsky, Matus},
  year = {2017},
  month = dec,
  abstract = {This paper presents a margin-based multiclass generalization bound for neural networks that scales with their margin-normalized "spectral complexity": their Lipschitz constant, meaning the product of the spectral norms of the weight matrices, times a certain correction factor. This bound is empirically investigated for a standard AlexNet network trained with SGD on the mnist and cifar10 datasets, with both original and random labels; the bound, the Lipschitz constants, and the excess risks are all in direct correlation, suggesting both that SGD selects predictors whose complexity scales with the difficulty of the learning task, and secondly that the presented bound is sensitive to this complexity.},
  archivePrefix = {arXiv},
  eprint = {1706.08498},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LI6RNVUE\\Bartlett et al. - 2017 - Spectrally-normalized margin bounds for neural net.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\QBUCB5HA\\1706.html},
  journal = {arXiv:1706.08498 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: Comparison to arXiv v1: 1-norm in main bound refined to (2,1)-group-norm. Comparison to NIPS camera ready: typo fixes},
  primaryClass = {cs, stat}
}

@article{battagliaRelationalInductiveBiases2018,
  title = {Relational Inductive Biases, Deep Learning, and Graph Networks},
  author = {Battaglia, Peter W. and Hamrick, Jessica B. and Bapst, Victor and {Sanchez-Gonzalez}, Alvaro and Zambaldi, Vinicius and Malinowski, Mateusz and Tacchetti, Andrea and Raposo, David and Santoro, Adam and Faulkner, Ryan and Gulcehre, Caglar and Song, Francis and Ballard, Andrew and Gilmer, Justin and Dahl, George and Vaswani, Ashish and Allen, Kelsey and Nash, Charles and Langston, Victoria and Dyer, Chris and Heess, Nicolas and Wierstra, Daan and Kohli, Pushmeet and Botvinick, Matt and Vinyals, Oriol and Li, Yujia and Pascanu, Razvan},
  year = {2018},
  month = oct,
  abstract = {Artificial intelligence (AI) has undergone a renaissance recently, making major progress in key domains such as vision, language, control, and decision-making. This has been due, in part, to cheap data and cheap compute resources, which have fit the natural strengths of deep learning. However, many defining characteristics of human intelligence, which developed under much different pressures, remain out of reach for current approaches. In particular, generalizing beyond one's experiences\textemdash a hallmark of human intelligence from infancy\textemdash remains a formidable challenge for modern AI.},
  archivePrefix = {arXiv},
  eprint = {1806.01261},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SHBB6R95\\Battaglia et al. - 2018 - Relational inductive biases, deep learning, and gr.pdf},
  journal = {arXiv:1806.01261 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{beaulieuLearningContinuallyLearn2020,
  title = {Learning to {{Continually Learn}}},
  author = {Beaulieu, Shawn and Frati, Lapo and Miconi, Thomas and Lehman, Joel and Stanley, Kenneth O. and Clune, Jeff and Cheney, Nick},
  year = {2020},
  month = mar,
  abstract = {Continual lifelong learning requires an agent or model to learn many sequentially ordered tasks, building on previous knowledge without catastrophically forgetting it. Much work has gone towards preventing the default tendency of machine learning models to catastrophically forget, yet virtually all such work involves manually-designed solutions to the problem. We instead advocate meta-learning a solution to catastrophic forgetting, allowing AI to learn to continually learn. Inspired by neuromodulatory processes in the brain, we propose A Neuromodulated Meta-Learning Algorithm (ANML). It differentiates through a sequential learning process to meta-learn an activation-gating function that enables contextdependent selective activation within a deep neural network. Specifically, a neuromodulatory (NM) neural network gates the forward pass of another (otherwise normal) neural network called the prediction learning network (PLN). The NM network also thus indirectly controls selective plasticity (i.e. the backward pass of) the PLN. ANML enables continual learning without catastrophic forgetting at scale: it produces state-of-the-art continual learning performance, sequentially learning as many as 600 classes (over 9,000 SGD updates).},
  archivePrefix = {arXiv},
  eprint = {2002.09571},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GZGG9XPE\\Beaulieu et al. - 2020 - Learning to Continually Learn.pdf},
  journal = {arXiv:2002.09571 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-soon,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{BeginnerGuideDifferentiable,
  title = {A {{Beginner}}'s {{Guide}} to {{Differentiable Programming}}},
  abstract = {A new kind of software by assembling networks of parameterized functional blocks and by training them from examples using some form of gradient-based optimization.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\V9JM8X8L\\differentiableprogramming.html},
  howpublished = {http://pathmind.com/wiki/differentiableprogramming},
  journal = {Pathmind},
  language = {en}
}

@article{belkinReconcilingModernMachine2019,
  title = {Reconciling Modern Machine Learning Practice and the Bias-Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  month = sep,
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias-variance trade-off, appears to be at odds with the observed behavior of methods used in the modern machine learning practice. The bias-variance trade-off implies that a model should balance under-fitting and over-fitting: rich enough to express underlying structure in data, simple enough to avoid fitting spurious patterns. However, in the modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered over-fit, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This "double descent" curve subsumes the textbook U-shaped bias-variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine learning models delineates the limits of classical analyses, and has implications for both the theory and practice of machine learning.},
  archivePrefix = {arXiv},
  eprint = {1812.11118},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\K97PD3K2\\Belkin et al. - 2019 - Reconciling modern machine learning practice and t.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\V5J985Y4\\1812.html},
  journal = {arXiv:1812.11118 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-next,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{belkinReconcilingModernMachinelearning2019,
  title = {Reconciling Modern Machine-Learning Practice and the Classical Bias\textendash Variance Trade-Off},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2019},
  month = aug,
  volume = {116},
  pages = {15849--15854},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.1903070116},
  abstract = {Breakthroughs in machine learning are rapidly changing science and society, yet our fundamental understanding of this technology has lagged far behind. Indeed, one of the central tenets of the field, the bias\textendash variance trade-off, appears to be at odds with the observed behavior of methods used in modern machine-learning practice. The bias\textendash variance trade-off implies that a model should balance underfitting and overfitting: Rich enough to express underlying structure in data and simple enough to avoid fitting spurious patterns. However, in modern practice, very rich models such as neural networks are trained to exactly fit (i.e., interpolate) the data. Classically, such models would be considered overfitted, and yet they often obtain high accuracy on test data. This apparent contradiction has raised questions about the mathematical foundations of machine learning and their relevance to practitioners. In this paper, we reconcile the classical understanding and the modern practice within a unified performance curve. This ``double-descent'' curve subsumes the textbook U-shaped bias\textendash variance trade-off curve by showing how increasing model capacity beyond the point of interpolation results in improved performance. We provide evidence for the existence and ubiquity of double descent for a wide spectrum of models and datasets, and we posit a mechanism for its emergence. This connection between the performance and the structure of machine-learning models delineates the limits of classical analyses and has implications for both the theory and the practice of machine learning.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZMT2P74B\\Belkin et al. - 2019 - Reconciling modern machine-learning practice and t.pdf},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {generalization},
  language = {en},
  number = {32}
}

@article{belkinReplyLoogLooking2020,
  title = {Reply to {{Loog}} et al.: {{Looking}} beyond the Peaking Phenomenon},
  shorttitle = {Reply to {{Loog}} et Al.},
  author = {Belkin, Mikhail and Hsu, Daniel and Ma, Siyuan and Mandal, Soumik},
  year = {2020},
  month = may,
  volume = {117},
  pages = {10627--10627},
  publisher = {{National Academy of Sciences}},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2003206117},
  abstract = {The letter ``A brief prehistory of double descent'' (1) written in response to our article ``Reconciling modern machine-learning practice and the classical bias\textendash variance trade-off'' (2) brings a number of interesting points and important references. We agree that the \ldots{}  [{$\carriagereturn$}][1]1To whom correspondence may be addressed. Email: mbelkin\{at\}cse.ohio-state.edu.  [1]: \#xref-corresp-1-1},
  chapter = {Letter},
  copyright = {\textcopyright{} 2020 . https://www.pnas.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Z8C92XV5\\10627.html},
  journal = {Proceedings of the National Academy of Sciences},
  keywords = {generalization},
  language = {en},
  number = {20},
  pmid = {32371494}
}

@article{belloAttentionAugmentedConvolutional2019,
  title = {Attention {{Augmented Convolutional Networks}}},
  author = {Bello, Irwan and Zoph, Barret and Vaswani, Ashish and Shlens, Jonathon and Le, Quoc V.},
  year = {2019},
  month = sep,
  abstract = {Convolutional networks have been the paradigm of choice in many computer vision applications. The convolution operation however has a significant weakness in that it only operates on a local neighborhood, thus missing global information. Self-attention, on the other hand, has emerged as a recent advance to capture long range interactions, but has mostly been applied to sequence modeling and generative modeling tasks. In this paper, we consider the use of self-attention for discriminative visual tasks as an alternative to convolutions. We introduce a novel two-dimensional relative self-attention mechanism that proves competitive in replacing convolutions as a stand-alone computational primitive for image classification. We find in control experiments that the best results are obtained when combining both convolutions and self-attention. We therefore propose to augment convolutional operators with this self-attention mechanism by concatenating convolutional feature maps with a set of feature maps produced via self-attention. Extensive experiments show that Attention Augmentation leads to consistent improvements in image classification on ImageNet and object detection on COCO across many different models and scales, including ResNets and a state-of-the art mobile constrained network, while keeping the number of parameters similar. In particular, our method achieves a \$1.3\textbackslash\%\$ top-1 accuracy improvement on ImageNet classification over a ResNet50 baseline and outperforms other attention mechanisms for images such as Squeeze-and-Excitation. It also achieves an improvement of 1.4 mAP in COCO Object Detection on top of a RetinaNet baseline.},
  archivePrefix = {arXiv},
  eprint = {1904.09925},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GGIVJ37U\\Bello et al. - 2019 - Attention Augmented Convolutional Networks.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\6UU8FLRZ\\1904.html},
  journal = {arXiv:1904.09925 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-soon},
  note = {Comment: ICCV 2019},
  primaryClass = {cs}
}

@inproceedings{bengioDeepGenerativeStochastic2014,
  title = {Deep Generative Stochastic Networks Trainable by Backprop},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Bengio, Yoshua and Laufer, Eric and Alain, Guillaume and Yosinski, Jason},
  year = {2014},
  pages = {226--234}
}

@article{bergstraRandomSearchHyperParameter,
  title = {Random {{Search}} for {{Hyper}}-{{Parameter Optimization}}},
  author = {Bergstra, James and Bengio, Yoshua},
  pages = {25},
  abstract = {Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent ``High Throughput'' methods achieve surprising success\textemdash they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4SFPVNUZ\\Bergstra and Bengio - Random Search for Hyper-Parameter Optimization.pdf},
  keywords = {read-next},
  language = {en}
}

@misc{BestTensorBoardAlternatives2020,
  title = {The {{Best TensorBoard Alternatives}} (2020 {{Update}})},
  year = {2020},
  month = mar,
  abstract = {A good TensorBoard alternative will make your machine learning experiments more efficient and transparent. Find the best TensorBoard alternatives.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7UJST9IF\\the-best-tensorboard-alternatives.html},
  howpublished = {https://neptune.ai/blog/the-best-tensorboard-alternatives},
  journal = {neptune.ai},
  language = {en-US}
}

@misc{BestToolsLibraries2020,
  title = {The {{Best Tools}}, {{Libraries}}, {{Frameworks}} and {{Methodologies}} That {{Machine Learning Teams Actually Use}} - {{Things We Learned}} from 41 {{ML Startups}} [{{ROUNDUP}}]},
  year = {2020},
  month = may,
  abstract = {Setting up a good tool stack for your Machine Learning team is important to work efficiently and be able to focus on delivering results. If you work at a startup you know that setting up an environment that can grow with your team, needs of the users and rapidly evolving ML landscape is especially important.~ [\ldots ]},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7YQS6FPT\\tools-libraries-frameworks-methodologies-ml-startups-roundup.html},
  howpublished = {https://neptune.ai/blog/tools-libraries-frameworks-methodologies-ml-startups-roundup},
  journal = {neptune.ai},
  language = {en-US}
}

@article{BezierCurve2020,
  title = {B\'ezier Curve},
  year = {2020},
  month = nov,
  abstract = {A B\'ezier curve ( BEH-zee-ay) is a parametric curve used in computer graphics and related fields. The curve, which is related to the Bernstein polynomial, is named after Pierre B\'ezier, who used it in the 1960s for designing curves for the bodywork of Renault cars. Other uses include the design of computer fonts and animation. B\'ezier curves can be combined to form a B\'ezier spline, or generalized to higher dimensions to form B\'ezier surfaces. The B\'ezier triangle is a special case of the latter. In vector graphics, B\'ezier curves are used to model smooth curves that can be scaled indefinitely. "Paths", as they are commonly referred to in image manipulation programs, are combinations of linked B\'ezier curves. Paths are not bound by the limits of rasterized images and are intuitive to modify. B\'ezier curves are also used in the time domain, particularly in animation, user interface design and smoothing cursor trajectory in eye gaze controlled interfaces. For example, a B\'ezier curve can be used to specify the velocity over time of an object such as an icon moving from A to B, rather than simply moving at a fixed number of pixels per step. When animators or interface designers talk about the "physics" or "feel" of an operation, they may be referring to the particular B\'ezier curve used to control the velocity over time of the move in question. This also applies to robotics where the motion of a welding arm, for example, should be smooth to avoid unnecessary wear.},
  annotation = {Page Version ID: 987202125},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\DHL8J3W5\\index.html},
  journal = {Wikipedia},
  keywords = {bezier},
  language = {en}
}

@article{bezosTitlesecTitlepsTitletoc,
  title = {The Titlesec, Titleps and Titletoc {{Packages}}},
  author = {Bezos, Javier},
  pages = {25},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RAXAMNX8\\Bezos - The titlesec, titleps and titletoc Packages.pdf},
  keywords = {latex},
  language = {en}
}

@misc{BitsillaWeavingCode,
  title = {/* Bitsilla \textasciitilde{} Weaving Code */},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\WWW63YTT\\blog.html},
  howpublished = {https://bitsilla.com/blog/},
  journal = {/* bitsilla \textasciitilde{} weaving code */},
  language = {en-US}
}

@article{bochkovskiyYOLOv4OptimalSpeed2020,
  title = {{{YOLOv4}}: {{Optimal Speed}} and {{Accuracy}} of {{Object Detection}}},
  shorttitle = {{{YOLOv4}}},
  author = {Bochkovskiy, Alexey and Wang, Chien-Yao and Liao, Hong-Yuan Mark},
  year = {2020},
  month = apr,
  abstract = {There are a huge number of features which are said to improve Convolutional Neural Network (CNN) accuracy. Practical testing of combinations of such features on large datasets, and theoretical justification of the result, is required. Some features operate on certain models exclusively and for certain problems exclusively, or only for small-scale datasets; while some features, such as batch-normalization and residual-connections, are applicable to the majority of models, tasks, and datasets. We assume that such universal features include Weighted-Residual-Connections (WRC), Cross-Stage-Partial-connections (CSP), Cross mini-Batch Normalization (CmBN), Self-adversarial-training (SAT) and Mish-activation. We use new features: WRC, CSP, CmBN, SAT, Mish activation, Mosaic data augmentation, CmBN, DropBlock regularization, and CIoU loss, and combine some of them to achieve state-of-the-art results: 43.5\% AP (65.7\% AP50) for the MS COCO dataset at a realtime speed of {$\sim$}65 FPS on Tesla V100. Source code is at https://github.com/AlexeyAB/darknet.},
  archivePrefix = {arXiv},
  eprint = {2004.10934},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZEL54A3H\\Bochkovskiy et al. - 2020 - YOLOv4 Optimal Speed and Accuracy of Object Detec.pdf},
  journal = {arXiv:2004.10934 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Electrical Engineering and Systems Science - Image and Video Processing,read},
  language = {en},
  primaryClass = {cs, eess}
}

@article{bottouOptimizationMethodsLargeScale2018,
  title = {Optimization {{Methods}} for {{Large}}-{{Scale Machine Learning}}},
  author = {Bottou, L{\'e}on and Curtis, Frank E. and Nocedal, Jorge},
  year = {2018},
  month = jan,
  volume = {60},
  pages = {223--311},
  issn = {0036-1445, 1095-7200},
  doi = {10.1137/16M1080173},
  abstract = {This paper provides a review and commentary on the past, present, and future of numerical optimization algorithms in the context of machine learning applications. Through case studies on text classification and the training of deep neural networks, we discuss how optimization problems arise in machine learning and what makes them challenging. A major theme of our study is that large-scale machine learning represents a distinctive setting in which the stochastic gradient (SG) method has traditionally played a central role while conventional gradient-based nonlinear optimization techniques typically falter. Based on this viewpoint, we present a comprehensive theory of a straightforward, yet versatile SG algorithm, discuss its practical behavior, and highlight opportunities for designing algorithms with improved performance. This leads to a discussion about the next generation of optimization methods for large-scale machine learning, including an investigation of two main streams of research on techniques that diminish noise in the stochastic directions and methods that make use of second-order derivative approximations.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\C49VKVI4\\Bottou et al. - 2018 - Optimization Methods for Large-Scale Machine Learn.pdf},
  journal = {SIAM Review},
  keywords = {lr},
  language = {en},
  number = {2}
}

@article{boutilierConstraintbasedOptimizationUtility2006,
  title = {Constraint-Based Optimization and Utility Elicitation Using the Minimax Decision Criterion},
  author = {Boutilier, Craig and Patrascu, Relu and Poupart, Pascal and Schuurmans, Dale},
  year = {2006},
  month = jun,
  volume = {170},
  pages = {686--713},
  issn = {0004-3702},
  doi = {10.1016/j.artint.2006.02.003},
  abstract = {In many situations, a set of hard constraints encodes the feasible configurations of some system or product over which multiple users have distinct preferences. However, making suitable decisions requires that the preferences of a specific user for different configurations be articulated or elicited, something generally acknowledged to be onerous. We address two problems associated with preference elicitation: computing a best feasible solution when the user's utilities are imprecisely specified; and developing useful elicitation procedures that reduce utility uncertainty, with minimal user interaction, to a point where (approximately) optimal decisions can be made. Our main contributions are threefold. First, we propose the use of minimax regret as a suitable decision criterion for decision making in the presence of such utility function uncertainty. Second, we devise several different procedures, all relying on mixed integer linear programs, that can be used to compute minimax regret and regret-optimizing solutions effectively. In particular, our methods exploit generalized additive structure in a user's utility function to ensure tractable computation. Third, we propose various elicitation methods that can be used to refine utility uncertainty in such a way as to quickly (i.e., with as few questions as possible) reduce minimax regret. Empirical study suggests that several of these methods are quite successful in minimizing the number of user queries, while remaining computationally practical so as to admit real-time user interaction.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Q32X8T79\\Boutilier et al. - 2006 - Constraint-based optimization and utility elicitat.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\FCPVDZNM\\S0004370206000245.html},
  journal = {Artificial Intelligence},
  keywords = {Constraint satisfaction,Decision theory,Imprecise utility,Minimax regret,Optimization,Preference elicitation},
  language = {en},
  number = {8}
}

@article{Bremsstrahlung2020,
  title = {Bremsstrahlung},
  year = {2020},
  month = nov,
  abstract = {Bremsstrahlung  (German pronunciation: [{$\Elzverts$}bʁ{$\varepsilon$}ms.{$\Elzesh$}tʁa{$\Elzlmrk$}l{$\Elzpupsil$}\ng ] (listen)), from bremsen "to brake" and Strahlung "radiation"; i.e., "braking radiation" or "deceleration radiation", is electromagnetic radiation produced by the deceleration of a charged particle when deflected by another charged particle, typically an electron by an atomic nucleus. The moving particle loses kinetic energy, which is converted into radiation (i.e., a photon), thus satisfying the law of conservation of energy. The term is also used to refer to the process of producing the radiation. Bremsstrahlung has a continuous spectrum, which becomes more intense and whose peak intensity shifts toward higher frequencies as the change of the energy of the decelerated particles increases. Broadly speaking, bremsstrahlung or braking radiation is any radiation produced due to the deceleration (negative acceleration) of a charged particle, which includes synchrotron radiation (i.e. photon emission by a relativistic particle), cyclotron radiation (i.e. photon emission by a non-relativistic particle), and the emission of electrons and positrons during beta decay. However, the term is frequently used in the more narrow sense of radiation from electrons (from whatever source) slowing in matter. Bremsstrahlung emitted from plasma is sometimes referred to as free-free radiation. This refers to the fact that the radiation in this case is created by charged particles that are free; i.e., not part of an ion, atom or molecule, both before and after the deflection (acceleration) that caused the emission.},
  annotation = {Page Version ID: 986544278},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\82INXLDF\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@article{broidoScalefreeNetworksAre2019,
  title = {Scale-Free Networks Are Rare},
  author = {Broido, Anna D. and Clauset, Aaron},
  year = {2019},
  month = mar,
  volume = {10},
  pages = {1017},
  publisher = {{Nature Publishing Group}},
  issn = {2041-1723},
  doi = {10.1038/s41467-019-08746-5},
  abstract = {Real-world networks are often claimed to be scale free,~meaning that the fraction of nodes with degree k follows a power law k-{$\alpha$}, a pattern with broad implications for the structure and dynamics of complex systems. However, the universality of scale-free networks remains controversial. Here, we organize different definitions of scale-free networks and construct a severe test of their empirical prevalence using state-of-the-art statistical tools applied to nearly 1000 social, biological, technological, transportation, and information networks. Across these networks, we find robust evidence that strongly scale-free structure is empirically rare, while for most networks, log-normal distributions fit the data as well or better than power laws. Furthermore, social networks are at best weakly scale free, while a handful of technological and biological networks appear strongly scale free. These findings highlight the structural diversity of real-world networks and the need for new theoretical explanations of these non-scale-free patterns.},
  copyright = {2019 The Author(s)},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\N9ZATMHR\\Broido and Clauset - 2019 - Scale-free networks are rare.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\GDWPCIM3\\s41467-019-08746-5.html},
  journal = {Nature Communications},
  language = {en},
  number = {1}
}

@misc{C3physPdf,
  title = {C3-Phys.Pdf},
  howpublished = {https://web.eecs.umich.edu/\textasciitilde fessler/course/516/l/c3-phys.pdf}
}

@misc{C4sourcePdf,
  title = {C4-Source.Pdf},
  howpublished = {https://web.eecs.umich.edu/\textasciitilde fessler/course/516/l/c4-source.pdf}
}

@misc{C5recordPdf,
  title = {C5-Record.Pdf},
  howpublished = {https://web.eecs.umich.edu/\textasciitilde fessler/course/516/l/c5-record.pdf}
}

@misc{C6noisePdf,
  title = {C6-Noise.Pdf},
  howpublished = {https://web.eecs.umich.edu/\textasciitilde fessler/course/516/l/c6-noise.pdf}
}

@inproceedings{canzianiEvaluationNeuralNetwork2017,
  title = {Evaluation of Neural Network Architectures for Embedded Systems},
  booktitle = {2017 {{IEEE International Symposium}} on {{Circuits}} and {{Systems}} ({{ISCAS}})},
  author = {Canziani, Alfredo and Culurciello, Eugenio and Paszke, Adam},
  year = {2017},
  month = may,
  pages = {1--4},
  issn = {2379-447X},
  doi = {10.1109/ISCAS.2017.8050276},
  abstract = {Since the emergence of Deep Neural Networks (DNNs) as a prominent technique in the field of computer vision, the ImageNet classification challenge has played a major role in advancing the state-of-the-art. While accuracy figures have steadily increased, the resource utilization of winning models has not been properly taken into account. In this work, we present a comprehensive analysis of important metrics in practical applications: accuracy, memory footprint, parameters, operations count, inference time and power consumption. Key findings are: (1) power consumption is independent of batch size and architecture; (2) accuracy and inference time are in a hyperbolic relationship; (3) energy constraint is an upper bound on the maximum achievable accuracy and model complexity; (4) the number of operations is a reliable estimate of the inference time. We believe our analysis provides a compelling set of information that helps design and engineer efficient DNNs.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\32Y7W9RU\\8050276.html},
  keywords = {Computational modeling,computer vision,deep neural networks,DNN,embedded systems,energy constraint,image classification,ImageNet classification challenge,Memory management,neural net architecture,Neural networks,power consumption,Power demand,Resource management,resource utilization,Upper bound}
}

@book{cardinalOptimizationCalibrationDualEnergy1997,
  title = {Optimization and {{Calibration}} of {{Dual}}-{{Energy X}}-Ray {{Imaging Systems}}},
  author = {Cardinal, H. Neale},
  year = {1997},
  publisher = {{National Library of Canada}},
  abstract = {The diagnostic utility of radiography rests with its ability to detect and classify lesions of the intemal organs caused by patient pathology. However, this ability is hindered by image clutter, due to overiapping projections of other body structures, which can obscure or camouflage the lesion image. Dual-energy radiography can be used to alleviate this situation somewhat, by combining the two basis images so as to cancel the contrast of any two tissue types, e.g. improving the visibility of low-contrast lung lesions, or micro-calcifications in breast turnon. Thus, it is an attempt to maximize the diagnostic information content of the image. We propose a general optimization strategy of minimizing the maximum pixel variances in the two basis images at constant dose, and apply it to the optimization of a split septaless xenon ionization detector for chest radiography, by assuming a mediastinum object and estimating dose as that to an 1 1 -cm water laye?. For any source spectrurn, both basis images are virtually optimized by a single detector configuration, so that the optimization is imaging-task independent. Moreover, one source spectrum is nearly optimal for imaging both basis materials. We also propose the use of conic or cubic surfaces to approximate the log-signal functions (f, g) of a dual-energy system, considered as functions of the basis-material component thicknesses (x, y) of the object, and especially their inverses. We show that using the simplest of these, the conic rational function, to approximate the inverse functions yields a virtually ideal decomposition algorithm. The Taylor series expansions of f (x, y) and the sirnilarly defined log-variance function f '(x, y) are derived, shown to converge absolutely everywhere, and al1 their coefficients pararnetrized via central spectral moments of the basis-material attenuation coefficients. The lower-order moments are then used to construct conic rational approximations to f (x, y), g (x, y), x(f, g). and y(( g), as an aid to theoretical ly predicting or interpreting the coefficient values fitted numerically. Finally, a double-blind experimental / theoretical study of iodine, gadolinium, and holmium contrast agents is used to validate the simple detector model used for al1 our earlier theoretical work. The results show that the model predicts radiographie contrast (and hence log-signal values) with relative errors of mean magnitudes 6\% and 8\% for two representative x-ray imaging systems.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZTL4F4TL\\nq21282.pdf},
  keywords = {dual-energy}
}

@article{carionEndtoEndObjectDetection2020,
  title = {End-to-{{End Object Detection}} with {{Transformers}}},
  author = {Carion, Nicolas and Massa, Francisco and Synnaeve, Gabriel and Usunier, Nicolas and Kirillov, Alexander and Zagoruyko, Sergey},
  year = {2020},
  month = may,
  abstract = {We present a new method that views object detection as a direct set prediction problem. Our approach streamlines the detection pipeline, effectively removing the need for many hand-designed components like a non-maximum suppression procedure or anchor generation that explicitly encode our prior knowledge about the task. The main ingredients of the new framework, called DEtection TRansformer or DETR, are a set-based global loss that forces unique predictions via bipartite matching, and a transformer encoder-decoder architecture. Given a fixed small set of learned object queries, DETR reasons about the relations of the objects and the global image context to directly output the final set of predictions in parallel. The new model is conceptually simple and does not require a specialized library, unlike many other modern detectors. DETR demonstrates accuracy and run-time performance on par with the well-established and highly-optimized Faster RCNN baseline on the challenging COCO object detection dataset. Moreover, DETR can be easily generalized to produce panoptic segmentation in a unified manner. We show that it significantly outperforms competitive baselines. Training code and pretrained models are available at https://github.com/facebookresearch/detr.},
  archivePrefix = {arXiv},
  eprint = {2005.12872},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\QBEQF8G3\\Carion et al. - 2020 - End-to-End Object Detection with Transformers.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\XM45TCN6\\2005.html},
  journal = {arXiv:2005.12872 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-next},
  primaryClass = {cs}
}

@article{caronUnsupervisedLearningVisual2020,
  title = {Unsupervised {{Learning}} of {{Visual Features}} by {{Contrasting Cluster Assignments}}},
  author = {Caron, Mathilde and Misra, Ishan and Mairal, Julien and Goyal, Priya and Bojanowski, Piotr and Joulin, Armand},
  year = {2020},
  month = jul,
  abstract = {Unsupervised image representations have significantly reduced the gap with supervised pretraining, notably with the recent achievements of contrastive learning methods. These contrastive methods typically work online and rely on a large number of explicit pairwise feature comparisons, which is computationally challenging. In this paper, we propose an online algorithm, SwAV, that takes advantage of contrastive methods without requiring to compute pairwise comparisons. Specifically, our method simultaneously clusters the data while enforcing consistency between cluster assignments produced for different augmentations (or views) of the same image, instead of comparing features directly as in contrastive learning. Simply put, we use a swapped prediction mechanism where we predict the cluster assignment of a view from the representation of another view. Our method can be trained with large and small batches and can scale to unlimited amounts of data. Compared to previous contrastive methods, our method is more memory efficient since it does not require a large memory bank or a special momentum network. In addition, we also propose a new data augmentation strategy, multi-crop, that uses a mix of views with different resolutions in place of two full-resolution views, without increasing the memory or compute requirements much. We validate our findings by achieving 75.3\% top-1 accuracy on ImageNet with ResNet-50, as well as surpassing supervised pretraining on all the considered transfer tasks.},
  archivePrefix = {arXiv},
  eprint = {2006.09882},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\QVYLEPA7\\Caron et al. - 2020 - Unsupervised Learning of Visual Features by Contra.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\QJVUG8VV\\2006.html},
  journal = {arXiv:2006.09882 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-maybe,unsurpervised},
  primaryClass = {cs}
}

@article{castelvecchiParadoxHeartMathematics,
  title = {Paradox at the Heart of Mathematics Makes Physics Problem Unanswerable},
  author = {Castelvecchi, Davide},
  doi = {10.1038/nature.2015.18983},
  abstract = {G\"odel's incompleteness theorems are connected to unsolvable calculations in quantum physics.},
  chapter = {News},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TJYLBZ8I\\paradox-at-the-heart-of-mathematics-makes-physics-problem-unanswerable-1.html},
  journal = {Nature News},
  language = {en}
}

@misc{centerWhyRichestFreest2020,
  title = {Why the {{Richest}}, {{Freest Economies Belong}} to {{Countries With Large Powerful States}}},
  author = {Center, Brink Lindsey Brink Lindsey is a vice president at the Niskanen and Growth, Where His Research Focuses on Policy Responses to Slow and has written on a wide range of {topics}, high inequality He and Policy, Including Trade and {globalization} and {social}, American and History, Cultural and Capital, The Nature of Human},
  year = {2020},
  month = jul,
  abstract = {Flourishing market economy requires, not small government, but high-quality government.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JEKQCYE8\\why-the-richest-freest-most-advanced-economies-belong-to-countries-with-large-powerful-states.html},
  journal = {Evonomics},
  language = {en-US}
}

@misc{ChallengeReclaimingCommons,
  title = {The Challenge of Reclaiming the Commons from Capitalism \textendash{} {{Dirk Philipsen}} | {{Aeon Essays}}},
  abstract = {Against the capitalist creeds of scarcity and self-interest, a plan for humanity's shared flourishing is finally coming into view},
  chapter = {Society},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\U5QF34SH\\the-challenge-of-reclaiming-the-commons-from-capitalism.html},
  howpublished = {https://aeon.co/essays/the-challenge-of-reclaiming-the-commons-from-capitalism},
  journal = {Aeon},
  language = {en}
}

@article{chaudhariStochasticGradientDescent2018,
  title = {Stochastic Gradient Descent Performs Variational Inference, Converges to Limit Cycles for Deep Networks},
  author = {Chaudhari, Pratik and Soatto, Stefano},
  year = {2018},
  month = jan,
  abstract = {Stochastic gradient descent (SGD) is widely believed to perform implicit regularization when used to train deep neural networks, but the precise manner in which this occurs has thus far been elusive. We prove that SGD minimizes an average potential over the posterior distribution of weights along with an entropic regularization term. This potential is however not the original loss function in general. So SGD does perform variational inference, but for a different loss than the one used to compute the gradients. Even more surprisingly, SGD does not even converge in the classical sense: we show that the most likely trajectories of SGD for deep networks do not behave like Brownian motion around critical points. Instead, they resemble closed loops with deterministic components. We prove that such ``out-of-equilibrium'' behavior is a consequence of highly non-isotropic gradient noise in SGD; the covariance matrix of mini-batch gradients for deep networks has a rank as small as 1\% of its dimension. We provide extensive empirical validation of these claims, proven in the appendix.},
  archivePrefix = {arXiv},
  eprint = {1710.11029},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JSVDF6G5\\Chaudhari and Soatto - 2018 - Stochastic gradient descent performs variational i.pdf},
  journal = {arXiv:1710.11029 [cond-mat, stat]},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,lr,Mathematics - Optimization and Control,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cond-mat, stat}
}

@misc{chaudharyAmitnessToolbox2020,
  title = {Amitness/Toolbox},
  author = {Chaudhary, Amit},
  year = {2020},
  month = may,
  abstract = {Curated list of libraries for a faster machine learning workflow},
  copyright = {MIT}
}

@article{cheemaGeometricLookDouble2020,
  title = {A {{Geometric Look}} at {{Double Descent Risk}}: {{Volumes}}, {{Singularities}}, and {{Distinguishabilities}}},
  shorttitle = {A {{Geometric Look}} at {{Double Descent Risk}}},
  author = {Cheema, Prasad and Sugiyama, Mahito},
  year = {2020},
  month = jun,
  abstract = {The appearance of the double-descent risk phenomenon has received growing interest in the machine learning and statistics community, as it challenges well-understood notions behind the U-shaped train-test curves. Motivated through Rissanen's minimum description length (MDL), Balasubramanian's Occam's Razor, and Amari's information geometry, we investigate how the logarithm of the model volume: \$\textbackslash log V\$, works to extend intuition behind the AIC and BIC model selection criteria. We find that for the particular model classes of isotropic linear regression, statistical lattices, and the stochastic perceptron unit, the \$\textbackslash log V\$ term may be decomposed into a sum of distinct components. These components work to extend the idea of model complexity inherent in AIC and BIC, and are driven by new, albeit intuitive notions of (i) Model richness, and (ii) Model distinguishability. Our theoretical analysis assists in the understanding of how the double descent phenomenon may manifest, as well as why generalization error does not necessarily continue to grow with increasing model dimensionality.},
  archivePrefix = {arXiv},
  eprint = {2006.04366},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\288ELRTJ\\Cheema and Sugiyama - 2020 - A Geometric Look at Double Descent Risk Volumes, .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\ZFNBVPQ2\\2006.html},
  journal = {arXiv:2006.04366 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 8 main pages, 3 references pages, 12 supplementary pages},
  primaryClass = {cs, stat}
}

@article{chenComprehensiveModularizedStatistical2020,
  title = {A {{Comprehensive}} and {{Modularized Statistical Framework}} for {{Gradient Norm Equality}} in {{Deep Neural Networks}}},
  author = {Chen, Zhaodong and Deng, Lei and Wang, Bangyan and Li, Guoqi and Xie, Yuan},
  year = {2020},
  month = jan,
  abstract = {The rapid development of deep neural networks (DNNs) in recent years can be attributed to the various techniques that address gradient explosion and vanishing. In order to understand the principle behind these techniques and develop new methods, plenty of metrics have been proposed to identify networks that are free of gradient explosion and vanishing. However, due to the diversity of network components and complex serial-parallel hybrid connections in modern DNNs, the evaluation of existing metrics usually requires strong assumptions, complex statistical analysis, or has limited application fields, which constraints their spread in the community. In this paper, inspired by the Gradient Norm Equality and dynamical isometry, we first propose a novel metric called Block Dynamical Isometry, which measures the change of gradient norm in individual block. Because our Block Dynamical Isometry is norm-based, its evaluation needs weaker assumptions compared with the original dynamical isometry. To mitigate the challenging derivation, we propose a highly modularized statistical framework based on free probability. Our framework includes several key theorems to handle complex serial-parallel hybrid connections and a library to cover the diversity of network components. Besides, several sufficient prerequisites are provided. Powered by our metric and framework, we analyze extensive initialization, normalization, and network structures. We find that Gradient Norm Equality is a universal philosophy behind them. Then, we improve some existing methods based on our analysis, including an activation function selection strategy for initialization techniques, a new configuration for weight normalization, and a depth-aware way to derive coefficients in SeLU. Moreover, we propose a novel normalization technique named second moment normalization, which is theoretically 30\% faster than batch normalization without accuracy loss. Last but not least, our conclusions and methods are evidenced by extensive experiments on multiple models over CIFAR10 and ImageNet.},
  archivePrefix = {arXiv},
  eprint = {2001.00254},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RNEJQBTS\\2001.00254v1.pdf},
  journal = {arXiv:2001.00254 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  language = {en},
  note = {Comment: Under review as a regular paper in TPAMI},
  primaryClass = {cs, stat}
}

@article{chenModularMetaLearningShrinkage2020,
  title = {Modular {{Meta}}-{{Learning}} with {{Shrinkage}}},
  author = {Chen, Yutian and Friesen, Abram L. and Behbahani, Feryal and Doucet, Arnaud and Budden, David and Hoffman, Matthew W. and {de Freitas}, Nando},
  year = {2020},
  month = jun,
  abstract = {The modular nature of deep networks allows some components to learn general features, while others learn more task-specific features. When a deep model is then fine-tuned on a new task, each component adapts differently. For example, the input layers of an image classification convnet typically adapt very little, while the output layers may change significantly. However, standard meta-learning approaches ignore this variability and either adapt all modules equally or hand-pick a subset to adapt. This can result in overfitting and wasted computation during adaptation. In this work, we develop techniques based on Bayesian shrinkage to meta-learn how task-independent each module is and to regularize it accordingly. We show that various recent meta-learning algorithms, such as MAML and Reptile, are special cases of our formulation in the limit of no regularization. Empirically, our approach discovers a small subset of modules to adapt, and improves performance. Notably, our method finds that the final layer is not always the best layer to adapt, contradicting standard practices in the literature.},
  archivePrefix = {arXiv},
  eprint = {1909.05557},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SBRUGXWI\\Chen et al. - 2020 - Modular Meta-Learning with Shrinkage.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\TQVTLSZF\\1909.html},
  journal = {arXiv:1909.05557 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,meta learning,read-soon,Statistics - Machine Learning},
  note = {Comment: 33 pages (12 main, 21 supplement), under review},
  primaryClass = {cs, stat}
}

@article{chenMultipleDescentDesign2020,
  title = {Multiple {{Descent}}: {{Design Your Own Generalization Curve}}},
  shorttitle = {Multiple {{Descent}}},
  author = {Chen, Lin and Min, Yifei and Belkin, Mikhail and Karbasi, Amin},
  year = {2020},
  month = aug,
  abstract = {This paper explores the generalization loss of linear regression in variably parameterized families of models, both under-parameterized and over-parameterized. We show that the generalization curve can have an arbitrary number of peaks, and moreover, locations of those peaks can be explicitly controlled. Our results highlight the fact that both classical U-shaped generalization curve and the recently observed double descent curve are not intrinsic properties of the model family. Instead, their emergence is due to the interaction between the properties of the data and the inductive biases of learning algorithms.},
  archivePrefix = {arXiv},
  eprint = {2008.01036},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\8RW3UMZZ\\Chen et al. - 2020 - Multiple Descent Design Your Own Generalization C.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\XFBQC7N4\\2008.html},
  journal = {arXiv:2008.01036 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Statistics Theory,read-next,Statistics - Machine Learning},
  note = {Comment: Fixed minor typos},
  primaryClass = {cs, math, stat}
}

@incollection{chenNeuralOrdinaryDifferential2018,
  title = {Neural {{Ordinary Differential Equations}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Chen, Ricky T. Q. and Rubanova, Yulia and Bettencourt, Jesse and Duvenaud, David K},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {6571--6583},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\QVI7HTH5\\Chen et al. - 2018 - Neural Ordinary Differential Equations.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\RU79J682\\7892-neural-ordinary-differential-equations.html},
  keywords = {NODE}
}

@article{chenPadamClosingGeneralization2018,
  title = {Padam: {{Closing}} the {{Generalization Gap}} of {{Adaptive Gradient Methods}} in {{Training Deep Neural Networks}}},
  shorttitle = {Padam},
  author = {Chen, Jinghui and Gu, Quanquan},
  year = {2018},
  month = sep,
  abstract = {Adaptive gradient methods, which adopt historical gradient information to automatically adjust the learning rate, despite the nice property of fast convergence, have been observed to generalize...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2GKQK6FV\\Chen and Gu - 2018 - Padam Closing the Generalization Gap of Adaptive .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\NE3H52SH\\forum.html},
  keywords = {lr,padam,read}
}

@article{chenSimpleFrameworkContrastive2020,
  title = {A {{Simple Framework}} for {{Contrastive Learning}} of {{Visual Representations}}},
  author = {Chen, Ting and Kornblith, Simon and Norouzi, Mohammad and Hinton, Geoffrey},
  year = {2020},
  month = mar,
  abstract = {This paper presents SimCLR: a simple framework for contrastive learning of visual representations. We simplify recently proposed contrastive self-supervised learning algorithms without requiring specialized architectures or a memory bank. In order to understand what enables the contrastive prediction tasks to learn useful representations, we systematically study the major components of our framework. We show that (1) composition of data augmentations plays a critical role in defining effective predictive tasks, (2) introducing a learnable nonlinear transformation between the representation and the contrastive loss substantially improves the quality of the learned representations, and (3) contrastive learning benefits from larger batch sizes and more training steps compared to supervised learning. By combining these findings, we are able to considerably outperform previous methods for self-supervised and semi-supervised learning on ImageNet. A linear classifier trained on self-supervised representations learned by SimCLR achieves 76.5\% top-1 accuracy, which is a 7\% relative improvement over previous state-of-the-art, matching the performance of a supervised ResNet-50. When fine-tuned on only 1\% of the labels, we achieve 85.8\% top-5 accuracy, outperforming AlexNet with 100X fewer labels.},
  archivePrefix = {arXiv},
  eprint = {2002.05709},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\AC2MXHKI\\Chen et al. - 2020 - A Simple Framework for Contrastive Learning of Vis.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\S3RBEPH7\\2002.html},
  journal = {arXiv:2002.05709 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  note = {Comment: code and pretrained models at https://github.com/google-research/simclr},
  primaryClass = {cs, stat}
}

@article{cholletMeasureIntelligence2019,
  title = {On the {{Measure}} of {{Intelligence}}},
  author = {Chollet, Fran{\c c}ois},
  year = {2019},
  month = nov,
  abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks, such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to ``buy'' arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience, as critical pieces to be accounted for in characterizing intelligent systems. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a new benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
  archivePrefix = {arXiv},
  eprint = {1911.01547},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\L67CSQPU\\Chollet - 2019 - On the Measure of Intelligence.pdf},
  journal = {arXiv:1911.01547 [cs]},
  keywords = {Computer Science - Artificial Intelligence},
  language = {en},
  primaryClass = {cs}
}

@article{choromanskiRethinkingAttentionPerformers2020,
  title = {Rethinking {{Attention}} with {{Performers}}},
  author = {Choromanski, Krzysztof and Likhosherstov, Valerii and Dohan, David and Song, Xingyou and Gane, Andreea and Sarlos, Tamas and Hawkins, Peter and Davis, Jared and Mohiuddin, Afroz and Kaiser, Lukasz and Belanger, David and Colwell, Lucy and Weller, Adrian},
  year = {2020},
  month = sep,
  abstract = {We introduce Performers, Transformer architectures which can estimate regular (softmax) full-rank-attention Transformers with provable accuracy, but using only linear (as opposed to quadratic) space and time complexity, without relying on any priors such as sparsity or low-rankness. To approximate softmax attention-kernels, Performers use a novel Fast Attention Via positive Orthogonal Random features approach (FAVOR+), which may be of independent interest for scalable kernel methods. FAVOR+ can be also used to efficiently model kernelizable attention mechanisms beyond softmax. This representational power is crucial to accurately compare softmax with other kernels for the first time on large-scale tasks, beyond the reach of regular Transformers, and investigate optimal attention-kernels. Performers are linear architectures fully compatible with regular Transformers and with strong theoretical guarantees: unbiased or nearly-unbiased estimation of the attention matrix, uniform convergence and low estimation variance. We tested Performers on a rich set of tasks stretching from pixel-prediction through text models to protein sequence modeling. We demonstrate competitive results with other examined efficient sparse and dense attention methods, showcasing effectiveness of the novel attention-learning paradigm leveraged by Performers.},
  archivePrefix = {arXiv},
  eprint = {2009.14794},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6CSIQ2BP\\Choromanski et al. - 2020 - Rethinking Attention with Performers.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\VSEJWAK6\\2009.html},
  journal = {arXiv:2009.14794 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,read-half,Statistics - Machine Learning},
  note = {Comment: 36 pages. This is an updated version of a previous submission which can be found at arXiv:2006.03555. See https://github.com/google-research/google-research/tree/master/protein\_lm for protein language model code, and https://github.com/google-research/google-research/tree/master/performer for Performer code},
  primaryClass = {cs, stat}
}

@article{cichonBernoulliSumsBernstein,
  title = {On {{Bernoulli Sums}} and {{Bernstein Polynomials}}},
  author = {Cicho{\'n}, Jacek and Gol{\k{e}}biewski, Zbigniew},
  pages = {13},
  abstract = {In the paper we discuss a technology based on Bernstein polynomials of asymptotic analysis of a class of binomial sums that arise in information theory. Our method gives a quick derivation of required sums and can be generalized to multinomial distributions. As an example we derive a formula for the entropy of multinomial distributions. Our method simplifies previous work of Jacquet, Szpankowski and Flajolet from 1999.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TUSXRFE9\\Cichoń and Golębiewski - On Bernoulli Sums and Bernstein Polynomials.pdf},
  language = {en}
}

@article{cluneAIGAsAIgeneratingAlgorithms2020,
  title = {{{AI}}-{{GAs}}: {{AI}}-Generating Algorithms, an Alternate Paradigm for Producing General Artificial Intelligence},
  shorttitle = {{{AI}}-{{GAs}}},
  author = {Clune, Jeff},
  year = {2020},
  month = jan,
  abstract = {Perhaps the most ambitious scientific quest in human history is the creation of general artificial intelligence, which roughly means AI that is as smart or smarter than humans. The dominant approach in the machine learning community is to attempt to discover each of the pieces that might be required for intelligence, with the implicit assumption that at some point in the future some group will complete the Herculean task of figuring out how to combine all of those pieces into an extremely complex machine. I call this the ``manual AI approach.'' This paper describes another exciting path that ultimately may be more successful at producing general AI. It is based on the clear trend from the history of machine learning that hand-designed solutions eventually are replaced by more effective, learned solutions. The idea is to create an AI-generating algorithm (AI-GA), which itself automatically learns how to produce general AI. Three Pillars are essential for the approach: (1) meta-learning architectures, (2) meta-learning the learning algorithms themselves, and (3) generating effective learning environments. While work has begun on the first two pillars, little has been done on the third. Here I argue that either the manual or AI-GA approach could be the first to lead to general AI, and that both are worthwhile scientific endeavors irrespective of which is the fastest path. Because both approaches are roughly equally promising, and because the machine learning community is mostly committed to the engineered AI approach currently, I argue that our community should shift a substantial amount of its research investment to the AI-GA approach. To encourage such research, I describe promising work in each of the Three Pillars. I also discuss the safety and ethical considerations unique to the AI-GA approach. Because it may be the fastest path to general AI and because it is inherently scientifically interesting to understand the conditions in which a simple algorithm can produce general intelligence (as happened on Earth where Darwinian evolution produced human intelligence), I argue that the pursuit of AI-GAs should be considered a new grand challenge of computer science research.},
  archivePrefix = {arXiv},
  eprint = {1905.10985},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YHLLFN3C\\Clune - 2020 - AI-GAs AI-generating algorithms, an alternate par.pdf},
  journal = {arXiv:1905.10985 [cs]},
  keywords = {Computer Science - Artificial Intelligence,read},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{cohenGroupEquivariantConvolutional2016,
  title = {Group Equivariant Convolutional Networks},
  booktitle = {International Conference on Machine Learning},
  author = {Cohen, Taco and Welling, Max},
  year = {2016},
  pages = {2990--2999},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZIWVFF2E\\Cohen and Welling - 2016 - Group equivariant convolutional networks.pdf}
}

@article{cookInitialDataNumerical2000,
  title = {Initial {{Data}} for {{Numerical Relativity}}},
  author = {Cook, Gregory B.},
  year = {2000},
  month = dec,
  volume = {3},
  pages = {5},
  issn = {2367-3613, 1433-8351},
  doi = {10.12942/lrr-2000-5},
  abstract = {Initial data are the starting point for any numerical simulation. In the case of numerical relativity, Einstein's equations constrain our choices of these initial data. We will examine several of the formalisms used for specifying Cauchy initial data in the 3 + 1 decomposition of Einstein's equations. We will then explore how these formalisms have been used in constructing initial data for spacetimes containing black holes and neutron stars. In the topics discussed, emphasis is placed on those issues that are important for obtaining astrophysically realistic initial data for compact binary coalescence.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RYL2A4FS\\Cook - 2000 - Initial Data for Numerical Relativity.pdf},
  journal = {Living Reviews in Relativity},
  language = {en},
  number = {1}
}

@article{cremerVeryMinimalIntroduction,
  title = {A Very Minimal Introduction to {{Tik Z}}},
  author = {Cremer, Jacques},
  pages = {24},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\WHFCBMQ5\\Cremer - A very minimal introduction to Tik Z.pdf},
  keywords = {latex,tikz},
  language = {en}
}

@misc{CS231nConvolutionalNeural,
  title = {{{CS231n Convolutional Neural Networks}} for {{Visual Recognition}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HQ2J4M65\\neural-networks-3.html},
  howpublished = {https://cs231n.github.io/neural-networks-3/}
}

@article{cubittUndecidabilitySpectralGap2015,
  title = {Undecidability of the {{Spectral Gap}} (Full Version)},
  author = {Cubitt, Toby and {Perez-Garcia}, David and Wolf, Michael M.},
  year = {2015},
  month = dec,
  volume = {528},
  pages = {207--211},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature16059},
  abstract = {We show that the spectral gap problem is undecidable. Specifically, we construct families of translationally-invariant, nearest-neighbour Hamiltonians on a 2D square lattice of d-level quantum systems (d constant), for which determining whether the system is gapped or gapless is an undecidable problem. This is true even with the promise that each Hamiltonian is either gapped or gapless in the strongest sense: it is promised to either have continuous spectrum above the ground state in the thermodynamic limit, or its spectral gap is lower-bounded by a constant in the thermodynamic limit. Moreover, this constant can be taken equal to the local interaction strength of the Hamiltonian.},
  archivePrefix = {arXiv},
  eprint = {1502.04573},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\648VTJH7\\Cubitt et al. - 2015 - Undecidability of the Spectral Gap (full version).pdf;C\:\\Users\\ext1150\\Zotero\\storage\\WTAPMCKT\\1502.html},
  journal = {Nature},
  keywords = {Condensed Matter - Other Condensed Matter,High Energy Physics - Theory,Mathematical Physics,Quantum Physics},
  note = {Comment: v1: 146 pages, 56 theorems etc., 15 figures. See shorter companion paper arXiv:1502.04135 (same title and authors) for a short version omitting technical details. v2: Small but important fix to wording of abstract. v3: Simplified and shortened some parts of the proof; minor fixes to other parts. Now only 127 pages, 55 theorems etc., 10 figures. v4: Minor updates to introduction},
  number = {7581}
}

@inproceedings{cubukAutoAugmentLearningAugmentation2019,
  title = {{{AutoAugment}}: {{Learning Augmentation Strategies From Data}}},
  shorttitle = {{{AutoAugment}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Cubuk, Ekin D. and Zoph, Barret and Mane, Dandelion and Vasudevan, Vijay and Le, Quoc V.},
  year = {2019},
  pages = {113--123},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TQBP44LX\\Cubuk et al. - 2019 - AutoAugment Learning Augmentation Strategies From.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\XALDJ5WL\\Cubuk_AutoAugment_Learning_Augmentation_Strategies_From_Data_CVPR_2019_paper.html},
  keywords = {read-soon}
}

@article{cubukRandAugmentPracticalAutomated2019,
  title = {{{RandAugment}}: {{Practical}} Automated Data Augmentation with a Reduced Search Space},
  shorttitle = {{{RandAugment}}},
  author = {Cubuk, Ekin D. and Zoph, Barret and Shlens, Jonathon and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0\% accuracy, a 0.6\% increase over the previous state-of-the-art and 1.0\% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3\% improvement over baseline augmentation, and is within 0.3\% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.},
  archivePrefix = {arXiv},
  eprint = {1909.13719},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\N4FB4HED\\Cubuk et al. - 2019 - RandAugment Practical automated data augmentation.pdf},
  journal = {arXiv:1909.13719 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-next},
  language = {en},
  note = {Comment: Added ablation experiments},
  primaryClass = {cs}
}

@article{cuiClassBalancedLossBased,
  title = {Class-{{Balanced Loss Based}} on {{Effective Number}} of {{Samples}}},
  author = {Cui, Yin and Jia, Menglin and Lin, Tsung-Yi and Song, Yang and Belongie, Serge},
  pages = {10},
  abstract = {With the rapid increase of large-scale, real-world datasets, it becomes critical to address the problem of longtailed data distribution (i.e., a few classes account for most of the data, while most classes are under-represented). Existing solutions typically adopt class re-balancing strategies such as re-sampling and re-weighting based on the number of observations for each class. In this work, we argue that as the number of samples increases, the additional benefit of a newly added data point will diminish. We introduce a novel theoretical framework to measure data overlap by associating with each sample a small neighboring region rather than a single point. The effective number of samples is defined as the volume of samples and can be calculated by a simple formula (1-{$\beta$}n)/(1-{$\beta$}), where n is the number of samples and {$\beta$} {$\in$} [0, 1) is a hyperparameter. We design a re-weighting scheme that uses the effective number of samples for each class to re-balance the loss, thereby yielding a class-balanced loss. Comprehensive experiments are conducted on artificially induced long-tailed CIFAR datasets and large-scale datasets including ImageNet and iNaturalist. Our results show that when trained with the proposed class-balanced loss, the network is able to achieve significant performance gains on long-tailed datasets.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\3YL4SIIV\\Cui et al. - Class-Balanced Loss Based on Effective Number of S.pdf},
  keywords = {read-soon},
  language = {en}
}

@article{daiDeformableConvolutionalNetworks2017,
  title = {Deformable {{Convolutional Networks}}},
  author = {Dai, Jifeng and Qi, Haozhi and Xiong, Yuwen and Li, Yi and Zhang, Guodong and Hu, Han and Wei, Yichen},
  year = {2017},
  month = jun,
  abstract = {Convolutional neural networks (CNNs) are inherently limited to model geometric transformations due to the fixed geometric structures in their building modules. In this work, we introduce two new modules to enhance the transformation modeling capability of CNNs, namely, deformable convolution and deformable RoI pooling. Both are based on the idea of augmenting the spatial sampling locations in the modules with additional offsets and learning the offsets from the target tasks, without additional supervision. The new modules can readily replace their plain counterparts in existing CNNs and can be easily trained end-to-end by standard back-propagation, giving rise to deformable convolutional networks. Extensive experiments validate the performance of our approach. For the first time, we show that learning dense spatial transformation in deep CNNs is effective for sophisticated vision tasks such as object detection and semantic segmentation. The code is released at https://github.com/ msracver/Deformable-ConvNets.},
  archivePrefix = {arXiv},
  eprint = {1703.06211},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PT4MSFBM\\Dai et al. - 2017 - Deformable Convolutional Networks.pdf},
  journal = {arXiv:1703.06211 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{dalalHistogramsOrientedGradients2005,
  title = {Histograms of Oriented Gradients for Human Detection},
  booktitle = {2005 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}}'05)},
  author = {Dalal, N. and Triggs, B.},
  year = {2005},
  month = jun,
  volume = {1},
  pages = {886-893 vol. 1},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2005.177},
  abstract = {We study the question of feature sets for robust visual object recognition; adopting linear SVM based human detection as a test case. After reviewing existing edge and gradient based descriptors, we show experimentally that grids of histograms of oriented gradient (HOG) descriptors significantly outperform existing feature sets for human detection. We study the influence of each stage of the computation on performance, concluding that fine-scale gradients, fine orientation binning, relatively coarse spatial binning, and high-quality local contrast normalization in overlapping descriptor blocks are all important for good results. The new approach gives near-perfect separation on the original MIT pedestrian database, so we introduce a more challenging dataset containing over 1800 annotated human images with a large range of pose variations and backgrounds.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\S8FX6DI7\\Dalal and Triggs - 2005 - Histograms of oriented gradients for human detecti.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\L332V6VN\\1467360.html},
  keywords = {coarse spatial binning,contrast normalization,edge based descriptors,feature extraction,fine orientation binning,fine-scale gradients,gradient based descriptors,gradient methods,High performance computing,Histograms,histograms of oriented gradients,human detection,Humans,Image databases,Image edge detection,linear SVM,object detection,Object detection,object recognition,Object recognition,overlapping descriptor,pedestrian database,read-maybe,robust visual object recognition,Robustness,sliding-window,support vector machines,Support vector machines,Testing}
}

@article{damourUnderspecificationPresentsChallenges2020,
  title = {Underspecification {{Presents Challenges}} for {{Credibility}} in {{Modern Machine Learning}}},
  author = {D'Amour, Alexander and Heller, Katherine and Moldovan, Dan and Adlam, Ben and Alipanahi, Babak and Beutel, Alex and Chen, Christina and Deaton, Jonathan and Eisenstein, Jacob and Hoffman, Matthew D. and Hormozdiari, Farhad and Houlsby, Neil and Hou, Shaobo and Jerfel, Ghassen and Karthikesalingam, Alan and Lucic, Mario and Ma, Yian and McLean, Cory and Mincu, Diana and Mitani, Akinori and Montanari, Andrea and Nado, Zachary and Natarajan, Vivek and Nielson, Christopher and Osborne, Thomas F. and Raman, Rajiv and Ramasamy, Kim and Sayres, Rory and Schrouff, Jessica and Seneviratne, Martin and Sequeira, Shannon and Suresh, Harini and Veitch, Victor and Vladymyrov, Max and Wang, Xuezhi and Webster, Kellie and Yadlowsky, Steve and Yun, Taedong and Zhai, Xiaohua and Sculley, D.},
  year = {2020},
  month = nov,
  abstract = {ML models often exhibit unexpectedly poor behavior when they are deployed in real-world domains. We identify underspecification as a key reason for these failures. An ML pipeline is underspecified when it can return many predictors with equivalently strong held-out performance in the training domain. Underspecification is common in modern ML pipelines, such as those based on deep learning. Predictors returned by underspecified pipelines are often treated as equivalent based on their training domain performance, but we show here that such predictors can behave very differently in deployment domains. This ambiguity can lead to instability and poor model behavior in practice, and is a distinct failure mode from previously identified issues arising from structural mismatch between training and deployment domains. We show that this problem appears in a wide variety of practical ML pipelines, using examples from computer vision, medical imaging, natural language processing, clinical risk prediction based on electronic health records, and medical genomics. Our results show the need to explicitly account for underspecification in modeling pipelines that are intended for real-world deployment in any domain.},
  archivePrefix = {arXiv},
  eprint = {2011.03395},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\X2F9MWTS\\D'Amour et al. - 2020 - Underspecification Presents Challenges for Credibi.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\7KRNP32J\\2011.html},
  journal = {arXiv:2011.03395 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{darasSMYRFEfficientAttention2020,
  title = {{{SMYRF}}: {{Efficient Attention}} Using {{Asymmetric Clustering}}},
  shorttitle = {{{SMYRF}}},
  author = {Daras, Giannis and Kitaev, Nikita and Odena, Augustus and Dimakis, Alexandros G.},
  year = {2020},
  month = oct,
  abstract = {We propose a novel type of balanced clustering algorithm to approximate attention. Attention complexity is reduced from \$O(N\^2)\$ to \$O(N \textbackslash log N)\$, where \$N\$ is the sequence length. Our algorithm, SMYRF, uses Locality Sensitive Hashing (LSH) in a novel way by defining new Asymmetric transformations and an adaptive scheme that produces balanced clusters. The biggest advantage of SMYRF is that it can be used as a drop-in replacement for dense attention layers without any retraining. On the contrary, prior fast attention methods impose constraints (e.g. queries and keys share the same vector representations) and require re-training from scratch. We apply our method to pre-trained state-of-the-art Natural Language Processing and Computer Vision models and we report significant memory and speed benefits. Notably, SMYRF-BERT outperforms (slightly) BERT on GLUE, while using \$50\textbackslash\%\$ less memory. We also show that SMYRF can be used interchangeably with dense attention before and after training. Finally, we use SMYRF to train GANs with attention in high resolutions. Using a single TPU, we were able to scale attention to 128x128=16k and 256x256=65k tokens on BigGAN on CelebA-HQ.},
  archivePrefix = {arXiv},
  eprint = {2010.05315},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VPDIQG44\\Daras et al. - 2020 - SMYRF Efficient Attention using Asymmetric Cluste.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\7EDK7DDG\\2010.html},
  journal = {arXiv:2010.05315 [cs]},
  keywords = {attention,Computer Science - Machine Learning},
  note = {Comment: 30 pages, 10 figures},
  primaryClass = {cs}
}

@misc{darrenlTzutalinLabelImg2020,
  title = {Tzutalin/{{labelImg}}},
  author = {{darrenl}},
  year = {2020},
  month = oct,
  abstract = {🖍️ LabelImg is a graphical image annotation tool and label object bounding boxes in images},
  copyright = {MIT License         ,                 MIT License},
  keywords = {annotations,deep-learning,detection,image-classification,imagenet,label,python2,python3,recognition,tools}
}

@article{dauphinIdentifyingAttackingSaddle2014,
  title = {Identifying and Attacking the Saddle Point Problem in High-Dimensional Non-Convex Optimization},
  author = {Dauphin, Yann and Pascanu, Razvan and Gulcehre, Caglar and Cho, Kyunghyun and Ganguli, Surya and Bengio, Yoshua},
  year = {2014},
  month = jun,
  abstract = {A central challenge to many fields of science and engineering involves minimizing non-convex error functions over continuous, high dimensional spaces. Gradient descent or quasi-Newton methods are almost ubiquitously used to perform such minimizations, and it is often thought that a main source of difficulty for these local methods to find the global minimum is the proliferation of local minima with much higher error than the global minimum. Here we argue, based on results from statistical physics, random matrix theory, neural network theory, and empirical evidence, that a deeper and more profound difficulty originates from the proliferation of saddle points, not local minima, especially in high dimensional problems of practical interest. Such saddle points are surrounded by high error plateaus that can dramatically slow down learning, and give the illusory impression of the existence of a local minimum. Motivated by these arguments, we propose a new approach to second-order optimization, the saddle-free Newton method, that can rapidly escape high dimensional saddle points, unlike gradient descent and quasi-Newton methods. We apply this algorithm to deep or recurrent neural network training, and provide numerical evidence for its superior optimization performance.},
  archivePrefix = {arXiv},
  eprint = {1406.2572},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LU3IHJ4E\\Dauphin et al. - 2014 - Identifying and attacking the saddle point problem.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\HR765JZ9\\1406.html},
  journal = {arXiv:1406.2572 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,read-next,Statistics - Machine Learning},
  note = {Comment: The theoretical review and analysis in this article draw heavily from arXiv:1405.4604 [cs.LG]},
  primaryClass = {cs, math, stat}
}

@misc{DeepLearningNature,
  title = {Deep Learning | {{Nature}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\QLB9E9R5\\nature14539.html},
  howpublished = {https://www.nature.com/articles/nature14539}
}

@misc{DeepReinforcementLearning2020,
  title = {Deep Reinforcement Learning for Supply Chain and Price Optimization},
  year = {2020},
  month = feb,
  abstract = {A hands-on tutorial that describes how to develop reinforcement learning optimizers using PyTorch and RLlib for supply chain and price management.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TQKFG9Y5\\deep-reinforcement-learning-for-supply-chain-and-price-optimization.html},
  howpublished = {https://blog.griddynamics.com/deep-reinforcement-learning-for-supply-chain-and-price-optimization/},
  journal = {Grid Dynamics Blog},
  language = {en}
}

@misc{DeepSpeedExtremescaleModel2020,
  title = {{{DeepSpeed}}: {{Extreme}}-Scale Model Training for Everyone},
  shorttitle = {{{DeepSpeed}}},
  year = {2020},
  month = sep,
  abstract = {DeepSpeed continues to innovate, making its tools more powerful while broadening its reach. Learn how it now powers 10x bigger model training on one GPU, 10x longer input sequences, 5x less communication volume, \& scales to train trillion-parameter models.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\789FZVXD\\deepspeed-extreme-scale-model-training-for-everyone.html},
  journal = {Microsoft Research},
  language = {en-US}
}

@article{deisenrothMathematicsMachineLearning,
  title = {Mathematics for {{Machine Learning}}},
  author = {Deisenroth, Marc Peter and Faisal, A Aldo and Ong, Cheng Soon},
  pages = {417},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TLDTRV57\\Deisenroth et al. - Mathematics for Machine Learning.pdf},
  language = {en}
}

@article{dewynterOptimalSubarchitectureExtraction2020,
  title = {Optimal {{Subarchitecture Extraction For BERT}}},
  author = {{de Wynter}, Adrian and Perry, Daniel J.},
  year = {2020},
  month = oct,
  abstract = {We extract an optimal subset of architectural parameters for the BERT architecture from Devlin et al. (2018) by applying recent breakthroughs in algorithms for neural architecture search. This optimal subset, which we refer to as "Bort", is demonstrably smaller, having an effective (that is, not counting the embedding layer) size of \$5.5\textbackslash\%\$ the original BERT-large architecture, and \$16\textbackslash\%\$ of the net size. Bort is also able to be pretrained in \$288\$ GPU hours, which is \$1.2\textbackslash\%\$ of the time required to pretrain the highest-performing BERT parametric architectural variant, RoBERTa-large (Liu et al., 2019), and about \$33\textbackslash\%\$ of that of the world-record, in GPU hours, required to train BERT-large on the same hardware. It is also \$7.9\$x faster on a CPU, as well as being better performing than other compressed variants of the architecture, and some of the non-compressed variants: it obtains performance improvements of between \$0.3\textbackslash\%\$ and \$31\textbackslash\%\$, absolute, with respect to BERT-large, on multiple public natural language understanding (NLU) benchmarks.},
  archivePrefix = {arXiv},
  eprint = {2010.10499},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\69JGRQW7\\2010.10499.pdf},
  journal = {arXiv:2010.10499 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,read-soon},
  language = {en},
  note = {Comment: Preprint. Under review},
  primaryClass = {cs}
}

@misc{DifferentiableMonteCarlo,
  title = {Differentiable {{Monte Carlo Ray Tracing}} through {{Edge Sampling}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4AKHGP83\\diffrt.html},
  howpublished = {https://people.csail.mit.edu/tzumao/diffrt/}
}

@article{dinhSharpMinimaCan2017,
  title = {Sharp {{Minima Can Generalize For Deep Nets}}},
  author = {Dinh, Laurent and Pascanu, Razvan and Bengio, Samy and Bengio, Yoshua},
  year = {2017},
  month = may,
  abstract = {Despite their overwhelming capacity to overfit, deep learning architectures tend to generalize relatively well to unseen data, allowing them to be deployed in practice. However, explaining why this is the case is still an open area of research. One standing hypothesis that is gaining popularity, e.g. Hochreiter \& Schmidhuber (1997); Keskar et al. (2017), is that the flatness of minima of the loss function found by stochastic gradient based methods results in good generalization. This paper argues that most notions of flatness are problematic for deep models and can not be directly applied to explain generalization. Specifically, when focusing on deep networks with rectifier units, we can exploit the particular geometry of parameter space induced by the inherent symmetries that these architectures exhibit to build equivalent models corresponding to arbitrarily sharper minima. Furthermore, if we allow to reparametrize a function, the geometry of its parameters can change drastically without affecting its generalization properties.},
  archivePrefix = {arXiv},
  eprint = {1703.04933},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LSEMMD5F\\Dinh et al. - 2017 - Sharp Minima Can Generalize For Deep Nets.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\J3LTHF4Y\\1703.html},
  journal = {arXiv:1703.04933 [cs]},
  keywords = {Computer Science - Machine Learning,read-soon},
  note = {Comment: 8.5 pages of main content, 2.5 of bibliography and 1 page of appendix},
  primaryClass = {cs}
}

@inproceedings{dollarIntegralChannelFeatures2009,
  title = {Integral {{Channel Features}}},
  booktitle = {Procedings of the {{British Machine Vision Conference}} 2009},
  author = {Dollar, Piotr and Tu, Zhuowen and Perona, Pietro and Belongie, Serge},
  year = {2009},
  pages = {91.1-91.11},
  publisher = {{British Machine Vision Association}},
  address = {{London}},
  doi = {10.5244/C.23.91},
  abstract = {We study the performance of `integral channel features' for image classification tasks, focusing in particular on pedestrian detection. The general idea behind integral channel features is that multiple registered image channels are computed using linear and non-linear transformations of the input image, and then features such as local sums, histograms, and Haar features and their various generalizations are efficiently computed using integral images. Such features have been used in recent literature for a variety of tasks \textendash{} indeed, variations appear to have been invented independently multiple times. Although integral channel features have proven effective, little effort has been devoted to analyzing or optimizing the features themselves. In this work we present a unified view of the relevant work in this area and perform a detailed experimental evaluation. We demonstrate that when designed properly, integral channel features not only outperform other features including histogram of oriented gradient (HOG), they also (1) naturally integrate heterogeneous sources of information, (2) have few parameters and are insensitive to exact parameter settings, (3) allow for more accurate spatial localization during detection, and (4) result in fast detectors when coupled with cascade classifiers.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\5DWAMVD9\\Dollar et al. - 2009 - Integral Channel Features.pdf},
  isbn = {978-1-901725-39-1},
  keywords = {read-maybe,sliding-window},
  language = {en}
}

@article{DomainAdaptation2020,
  title = {Domain Adaptation},
  year = {2020},
  month = nov,
  abstract = {Domain adaptation is a field associated with machine learning and transfer learning. This scenario arises when we aim at learning from a source data distribution a well performing model on a different (but related) target data distribution. For instance, one of the tasks of the common spam filtering problem consists in adapting a model from one user (the source distribution) to a new user who receives significantly different emails (the target distribution). Domain adaptation has also been shown to be beneficial for learning unrelated sources. Note that, when more than one source distribution is available the problem is referred to as multi-source domain adaptation.},
  annotation = {Page Version ID: 986607771},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PEQ3U46X\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@misc{dominiccummingsComplexityPredictionVI2019,
  title = {Complexity and Prediction {{VI}}: A Model Predicts the Frequency and Severity of Interstate Wars, `a Profound Mystery for Which We Have No Explanation'},
  shorttitle = {Complexity and Prediction {{VI}}},
  author = {{dominiccummings}},
  year = {2019},
  month = mar,
  abstract = {I spend a lot of time these days reading papers on prediction from different fields looking for connections between methods. This is an interesting paper:~On the frequency and severity of interstat\ldots},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\58G9MCQA\\complexity-and-prediction-vi-a-model-predicts-the-frequency-and-severity-of-interstate-wars-a-p.html},
  journal = {Dominic Cummings's Blog},
  language = {en}
}

@misc{dominiccummingsStateoftheartAICausality2018,
  title = {State-of-the-Art in {{AI}} \#1: Causality, Hypotheticals, and Robots with Free Will \& Capacity for Evil ({{UPDATED}})},
  shorttitle = {State-of-the-Art in {{AI}} \#1},
  author = {{dominiccummings}},
  year = {2018},
  month = may,
  abstract = {Judea Pearl is one of the most important scholars in the field of causal reasoning. His book Causality is the leading textbook in the field. This blog has two short parts \textemdash{} a paper he wrote a\ldots},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\5ZFESH79\\technology-the-state-of-the-art-in-ai-causality-and-hypotheticals.html},
  journal = {Dominic Cummings's Blog},
  language = {en}
}

@article{douglasReportStatusYangMills,
  title = {Report on the {{Status}} of the {{Yang}}-{{Mills Millenium Prize Problem}}},
  author = {Douglas, Michael R},
  pages = {4},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\H7XN3XQ4\\Douglas - Report on the Status of the Yang-Mills Millenium P.pdf},
  language = {en}
}

@article{dozatIncorporatingNesterovMomentum,
  title = {Incorporating {{Nesterov Momentum}} into {{Adam}}},
  author = {Dozat, Timothy},
  pages = {6},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\W6FIWUHZ\\Dozat - Incorporating Nesterov Momentum into Adam.pdf},
  language = {en}
}

@article{dozatINCORPORATINGNESTEROVMOMENTUM2016,
  title = {{{INCORPORATING NESTEROV MOMENTUM INTO ADAM}}},
  author = {Dozat, Timothy},
  year = {2016},
  pages = {4},
  abstract = {This work aims to improve upon the recently proposed and rapidly popularized optimization algorithm Adam (Kingma \& Ba, 2014). Adam has two main components\textemdash a momentum component and an adaptive learning rate component. However, regular momentum can be shown conceptually and empirically to be inferior to a similar algorithm known as Nesterov's accelerated gradient (NAG). We show how to modify Adam's momentum component to take advantage of insights from NAG, and then we present preliminary evidence suggesting that making this substitution improves the speed of convergence and the quality of the learned models.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RPRDM2YT\\Dozat - 2016 - INCORPORATING NESTEROV MOMENTUM INTO ADAM.pdf},
  keywords = {lr,read},
  language = {en}
}

@misc{dreierSpectralCorrectionApplication,
  title = {Spectral {{Correction}} and {{Application}} of {{Single}}-{{Photon Imaging}} with {{Direct Conversion X}}-Ray {{Detectors}}},
  author = {Dreier, Erik Schou},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LUFHBZC5\\DcYSfKVCck.pdf}
}

@article{dubeyDiffGradOptimizationMethod2020,
  title = {{{diffGrad}}: {{An Optimization Method}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{diffGrad}}},
  author = {Dubey, Shiv Ram and Chakraborty, Soumendu and Roy, Swalpa Kumar and Mukherjee, Snehasis and Singh, Satish Kumar and Chaudhuri, Bidyut Baran},
  year = {2020},
  month = mar,
  abstract = {Stochastic Gradient Decent (SGD) is one of the core techniques behind the success of deep neural networks. The gradient provides information on the direction in which a function has the steepest rate of change. The main problem with basic SGD is to change by equal sized steps for all parameters, irrespective of gradient behavior. Hence, an efficient way of deep network optimization is to make adaptive step sizes for each parameter. Recently, several attempts have been made to improve gradient descent methods such as AdaGrad, AdaDelta, RMSProp and Adam. These methods rely on the square roots of exponential moving averages of squared past gradients. Thus, these methods do not take advantage of local change in gradients. In this paper, a novel optimizer is proposed based on the difference between the present and the immediate past gradient (i.e., diffGrad). In the proposed diffGrad optimization technique, the step size is adjusted for each parameter in such a way that it should have a larger step size for faster gradient changing parameters and a lower step size for lower gradient changing parameters. The convergence analysis is done using the regret bound approach of online learning framework. Rigorous analysis is made in this paper over three synthetic complex non-convex functions. The image categorization experiments are also conducted over the CIFAR10 and CIFAR100 datasets to observe the performance of diffGrad with respect to the state-of-the-art optimizers such as SGDM, AdaGrad, AdaDelta, RMSProp, AMSGrad, and Adam. The residual unit (ResNet) based Convolutional Neural Networks (CNN) architecture is used in the experiments. The experiments show that diffGrad outperforms other optimizers. Also, we show that diffGrad performs uniformly well for training CNN using different activation functions. The source code is made publicly available at https://github.com/shivram1987/diffGrad.},
  archivePrefix = {arXiv},
  eprint = {1909.11015},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\XADLL8J7\\Dubey et al. - 2020 - diffGrad An Optimization Method for Convolutional.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\WC3GAUK3\\1909.html},
  journal = {arXiv:1909.11015 [cs, math]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,lr,Mathematics - Optimization and Control},
  primaryClass = {cs, math}
}

@inproceedings{dwibediCutPasteLearn2017,
  title = {Cut, {{Paste}} and {{Learn}}: {{Surprisingly Easy Synthesis}} for {{Instance Detection}}},
  shorttitle = {Cut, {{Paste}} and {{Learn}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Dwibedi, Debidatta and Misra, Ishan and Hebert, Martial},
  year = {2017},
  month = oct,
  pages = {1310--1319},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.146},
  abstract = {A major impediment in rapidly deploying object detection models for instance detection is the lack of large annotated datasets. For example, finding a large labeled dataset containing instances in a particular kitchen is unlikely. Each new environment with new instances requires expensive data collection and annotation. In this paper, we propose a simple approach to generate large annotated instance datasets with minimal effort. Our key insight is that ensuring only patch-level realism provides enough training signal for current object detector models. We automatically `cut' object instances and `paste' them on random backgrounds. A naive way to do this results in pixel artifacts which result in poor performance for trained models. We show how to make detectors ignore these artifacts during training and generate data that gives competitive performance on real data. Our method outperforms existing synthesis approaches and when combined with real images improves relative performance by more than 21\% on benchmark datasets. In a cross-domain setting, our synthetic data combined with just 10\% real data outperforms models trained on all real data.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VESYHC3E\\Dwibedi et al. - 2017 - Cut, Paste and Learn Surprisingly Easy Synthesis .pdf},
  isbn = {978-1-5386-1032-9},
  keywords = {read},
  language = {en}
}

@article{dyerAsymptoticsWideNetworks2019,
  title = {Asymptotics of {{Wide Networks}} from {{Feynman Diagrams}}},
  author = {Dyer, Ethan and {Gur-Ari}, Guy},
  year = {2019},
  month = sep,
  abstract = {Understanding the asymptotic behavior of wide networks is of considerable interest. In this work, we present a general method for analyzing this large width behavior. The method is an adaptation of Feynman diagrams, a standard tool for computing multivariate Gaussian integrals. We apply our method to study training dynamics, improving existing bounds and deriving new results on wide network evolution during stochastic gradient descent. Going beyond the strict large width limit, we present closed-form expressions for higher-order terms governing wide network training, and test these predictions empirically.},
  archivePrefix = {arXiv},
  eprint = {1909.11304},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RM5YQ3AH\\1909.11304.pdf},
  journal = {arXiv:1909.11304 [hep-th, stat]},
  keywords = {Computer Science - Machine Learning,High Energy Physics - Theory,read-someday,Statistics - Machine Learning},
  language = {en},
  note = {Comment: 10 pages, 3 figures, 1 Table + Appendices},
  primaryClass = {hep-th, stat}
}

@misc{Dynamicland,
  title = {Dynamicland},
  abstract = {incubating a humane dynamic medium},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2DI4BVUF\\dynamicland.org.html},
  howpublished = {https://dynamicland.org/}
}

@misc{EasytointerpretNeuronsMay,
  title = {{Easy-to-interpret neurons may hinder learning in deep neural networks}},
  abstract = {What does an AI model ``understand'' and why? A long-held belief is there are easy-to-interpret neurons -- or ``class selective'' neurons. For instance, finding neurons that},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GDUMT7R7\\easy-to-interpret-neurons-may-hinder-learning-in-deep-neural-networks.html},
  howpublished = {https://ai.facebook.com/blog/easy-to-interpret-neurons-may-hinder-learning-in-deep-neural-networks/},
  language = {da}
}

@book{EECS516Lecture,
  title = {{{EECS}} 516 {{Lecture Notes}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6G9MNQGC\\c3-phys.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\EIJMWU55\\c5-record.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\FSICF3UN\\c2-linsys.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\I7MI52UV\\c4-source.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\L82CU648\\c6-noise.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\WKFFFMP3\\c1-intro.pdf}
}

@misc{ElegantFiguresSubtleties2013,
  title = {Elegant {{Figures}} - {{Subtleties}} of {{Color}} ({{Part}} 1 of 6)},
  year = {2013},
  month = aug,
  publisher = {{NASA Earth Observatory}},
  abstract = {climate change, global climate change, global warming, natural hazards, Earth, environment, remote sensing, atmosphere, land processes, oceans, volcanoes, land cover, Earth science data, NASA, environmental processes, Blue Marble, global maps},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\I4HJVC7B\\subtleties-of-color-part-1-of-6.html},
  howpublished = {https://earthobservatory.nasa.gov/blogs/elegantfigures/2013/08/05/subtleties-of-color-part-1-of-6/},
  keywords = {coloring},
  language = {en},
  type = {Text.{{Article}}}
}

@article{engstromExploringLandscapeSpatial2019,
  title = {Exploring the {{Landscape}} of {{Spatial Robustness}}},
  author = {Engstrom, Logan and Tran, Brandon and Tsipras, Dimitris and Schmidt, Ludwig and Madry, Aleksander},
  year = {2019},
  month = sep,
  abstract = {The study of adversarial robustness has so far largely focused on perturbations bound in p-norms. However, state-of-the-art models turn out to be also vulnerable to other, more natural classes of perturbations such as translations and rotations. In this work, we thoroughly investigate the vulnerability of neural network--based classifiers to rotations and translations. While data augmentation offers relatively small robustness, we use ideas from robust optimization and test-time input aggregation to significantly improve robustness. Finally we find that, in contrast to the p-norm case, first-order methods cannot reliably find worst-case perturbations. This highlights spatial robustness as a fundamentally different setting requiring additional study. Code available at https://github.com/MadryLab/adversarial\_spatial and https://github.com/MadryLab/spatial-pytorch.},
  archivePrefix = {arXiv},
  eprint = {1712.02779},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\DNLE7JRM\\Engstrom et al. - 2019 - Exploring the Landscape of Spatial Robustness.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\GJJMLAIK\\Engstrom et al. - 2019 - Exploring the Landscape of Spatial Robustness.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\4V9UBK4K\\1712.html},
  journal = {arXiv:1712.02779 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-soon,Statistics - Machine Learning},
  language = {en},
  note = {Comment: ICML 2019. Presented in NIPS 2017 Workshop on Machine Learning and Computer Security as "A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations."
\par
Comment: ICML 2019. Presented in NIPS 2017 Workshop on Machine Learning and Computer Security as "A Rotation and a Translation Suffice: Fooling CNNs with Simple Transformations."},
  primaryClass = {cs, stat}
}

@article{evciRiggingLotteryMaking2020,
  title = {Rigging the {{Lottery}}: {{Making All Tickets Winners}}},
  shorttitle = {Rigging the {{Lottery}}},
  author = {Evci, Utku and Gale, Trevor and Menick, Jacob and Castro, Pablo Samuel and Elsen, Erich},
  year = {2020},
  month = jul,
  abstract = {Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-tosparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static*.},
  archivePrefix = {arXiv},
  eprint = {1911.11134},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PJWGC2XU\\Evci et al. - 2020 - Rigging the Lottery Making All Tickets Winners.pdf},
  journal = {arXiv:1911.11134 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-next,Statistics - Machine Learning},
  language = {en},
  note = {Comment: Published in Proceedings of the 37th International Conference on Machine Learning. Code can be found in github.com/google-research/rigl},
  primaryClass = {cs, stat}
}

@article{everittGravityProbeFinal2011,
  title = {Gravity {{Probe B}}: {{Final Results}} of a {{Space Experiment}} to {{Test General Relativity}}},
  shorttitle = {Gravity {{Probe B}}},
  author = {Everitt, C. W. F. and DeBra, D. B. and Parkinson, B. W. and Turneaure, J. P. and Conklin, J. W. and Heifetz, M. I. and Keiser, G. M. and Silbergleit, A. S. and Holmes, T. and Kolodziejczak, J. and {Al-Meshari}, M. and Mester, J. C. and Muhlfelder, B. and Solomonik, V. G. and Stahl, K. and Worden, P. W. and Bencze, W. and Buchman, S. and Clarke, B. and {Al-Jadaan}, A. and {Al-Jibreen}, H. and Li, J. and Lipa, J. A. and Lockhart, J. M. and {Al-Suwaidan}, B. and Taber, M. and Wang, S.},
  year = {2011},
  month = may,
  volume = {106},
  pages = {221101},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.106.221101},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\QMHMKNEY\\Everitt et al. - 2011 - Gravity Probe B Final Results of a Space Experime.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {22}
}

@article{fayeEnergyReconstructionElectrons,
  title = {Energy Reconstruction of Electrons and Photons Using Convolutional Neural Networks},
  author = {Faye, Frederik G},
  pages = {191},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CY3T5V8K\\Faye - Energy reconstruction of electrons and photons usi.pdf},
  language = {en}
}

@inproceedings{felzenszwalbCascadeObjectDetection2010,
  title = {Cascade Object Detection with Deformable Part Models},
  booktitle = {2010 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Felzenszwalb, Pedro F. and Girshick, Ross B. and McAllester, David},
  year = {2010},
  month = jun,
  pages = {2241--2248},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2010.5539906},
  abstract = {We describe a general method for building cascade classifiers from part-based deformable models such as pictorial structures. We focus primarily on the case of star-structured models and show how a simple algorithm based on partial hypothesis pruning can speed up object detection by more than one order of magnitude without sacrificing detection accuracy. In our algorithm, partial hypotheses are pruned with a sequence of thresholds. In analogy to probably approximately correct (PAC) learning, we introduce the notion of probably approximately admissible (PAA) thresholds. Such thresholds provide theoretical guarantees on the performance of the cascade method and can be computed from a small sample of positive examples. Finally, we outline a cascade detection algorithm for a general class of models defined by a grammar formalism. This class includes not only tree-structured pictorial structures but also richer models that can represent each part recursively as a mixture of other parts.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\59EAQZWY\\Felzenszwalb et al. - 2010 - Cascade object detection with deformable part mode.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\GEG2EXC3\\5539906.html},
  keywords = {Buildings,cascade detection algorithm,cascade object detection,Deformable models,Detection algorithms,Dynamic programming,grammar formalism,hypothesis pruning,image classification,image motion analysis,image segmentation,image sequences,object detection,Object detection,partial hypotheses,pictorial structures,probably approximately admissible,probably approximately correct learning,read-someday,sliding-window,star structured model,Statistical analysis,Statistical distributions,Testing,Training data,tree structured pictorial structures,Visualization}
}

@misc{Feynman687Interviews,
  title = {Feynman687/{{Interviews}}},
  abstract = {Contribute to Feynman687/Interviews development by creating an account on GitHub.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JU9TZERC\\StatML.html},
  howpublished = {https://github.com/Feynman687/Interviews},
  journal = {GitHub},
  language = {en}
}

@misc{FigurePerformanceSERLU,
  title = {Figure 2: {{Performance}} of {{SERLU}} and {{SELU}} for a {{FNN}} over {{MNIST}}.},
  shorttitle = {Figure 2},
  abstract = {Download scientific diagram | Performance of SERLU and SELU for a FNN over MNIST. from publication: Effectiveness of Scaled Exponentially-Regularized Linear Units (SERLUs) | Recently, self-normalizing neural networks (SNNs) have been proposed with the intention to avoid batch or weight normalization. The key step in SNNs is to properly scale the exponential linear unit (referred to as SELU) to inherently incorporate normalization based on central... | Weights and Measures | ResearchGate, the professional network for scientists.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HTWDJNH3\\Performance-of-SERLU-and-SELU-for-a-FNN-over-MNIST_fig2_326646583.html},
  howpublished = {https://www.researchgate.net/figure/Performance-of-SERLU-and-SELU-for-a-FNN-over-MNIST\_fig2\_326646583},
  journal = {ResearchGate},
  language = {en}
}

@article{finnLearningLearnGradients,
  title = {Learning to {{Learn}} with {{Gradients}}},
  author = {Finn, Chelsea},
  pages = {198},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GR4Q9ENX\\dissertation.pdf},
  language = {en}
}

@misc{fisherParadeBankersNew2018,
  title = {The {{Parade}} of {{Bankers}}' {{New Clothes Continues}}: 34 {{Flawed Claims Debunked}}},
  shorttitle = {The {{Parade}} of {{Bankers}}' {{New Clothes Continues}}},
  author = {Fisher, Blake},
  year = {2018},
  month = aug,
  abstract = {The debate on banking regulation has been dominated by flawed and misleading claims.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6EU977W5\\parade-bankers-new-clothes-continues-34-flawed-claims-debunked.html},
  howpublished = {https://admati.people.stanford.edu/publications/parade-bankers-new-clothes-continues-34-flawed-claims-debunked},
  language = {en},
  type = {Text}
}

@article{fortDeepEnsemblesLoss2020,
  title = {Deep {{Ensembles}}: {{A Loss Landscape Perspective}}},
  shorttitle = {Deep {{Ensembles}}},
  author = {Fort, Stanislav and Hu, Huiyi and Lakshminarayanan, Balaji},
  year = {2020},
  month = jun,
  abstract = {Deep ensembles have been empirically shown to be a promising approach for improving accuracy, uncertainty and out-of-distribution robustness of deep learning models. While deep ensembles were theoretically motivated by the bootstrap, non-bootstrap ensembles trained with just random initialization also perform well in practice, which suggests that there could be other explanations for why deep ensembles work well. Bayesian neural networks, which learn distributions over the parameters of the network, are theoretically well-motivated by Bayesian principles, but do not perform as well as deep ensembles in practice, particularly under dataset shift. One possible explanation for this gap between theory and practice is that popular scalable variational Bayesian methods tend to focus on a single mode, whereas deep ensembles tend to explore diverse modes in function space. We investigate this hypothesis by building on recent work on understanding the loss landscape of neural networks and adding our own exploration to measure the similarity of functions in the space of predictions. Our results show that random initializations explore entirely different modes, while functions along an optimization trajectory or sampled from the subspace thereof cluster within a single mode predictions-wise, while often deviating significantly in the weight space. Developing the concept of the diversity--accuracy plane, we show that the decorrelation power of random initializations is unmatched by popular subspace sampling methods. Finally, we evaluate the relative effects of ensembling, subspace based methods and ensembles of subspace based methods, and the experimental results validate our hypothesis.},
  archivePrefix = {arXiv},
  eprint = {1912.02757},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\A4V7MYUZ\\Fort et al. - 2020 - Deep Ensembles A Loss Landscape Perspective.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\QNAKJWZC\\1912.html},
  journal = {arXiv:1912.02757 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-maybe,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@misc{fossMeatMasterIIOne,
  title = {{{MeatMaster}}\_{{II}}\_{{One}}\_pager\_{{GB}}},
  author = {FOSS},
  publisher = {{FOSS}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HB4SGP9F\\MeatMaster_II_One_pager_GB.pdf}
}

@misc{FOSSMissionSustainable,
  title = {The {{FOSS}} Mission: {{Sustainable}} Use of Agricultural Resources},
  shorttitle = {The {{FOSS}} Mission},
  abstract = {The FOSS mission focuses on enabling more sustainable use of agricultural resources by improving quality and optimising production.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NQ4JAGCJ\\a-company-on-a-mission.html},
  howpublished = {https://www.fossanalytics.com/en/about-foss/a-company-on-a-mission},
  language = {en}
}

@article{Framedragging2020,
  title = {Frame-Dragging},
  year = {2020},
  month = apr,
  abstract = {Frame-dragging is an effect on spacetime, predicted by Albert Einstein's general theory of relativity, that is due to non-static stationary distributions of mass\textendash energy. A stationary field is one that is in a steady state, but the masses causing that field may be non-static{$\mkern1mu$}\nolinebreak\textemdash{$\mkern1mu$}rotating, for instance. More generally, the subject that deals with the effects caused by mass\textendash energy currents is known as gravitomagnetism, which is analogous to classical electromagnetism. The first frame-dragging effect was derived in 1918, in the framework of general relativity, by the Austrian physicists Josef Lense and Hans Thirring, and is also known as the Lense\textendash Thirring effect. They predicted that the rotation of a massive object would distort the spacetime metric, making the orbit of a nearby test particle precess. This does not happen in Newtonian mechanics for which the gravitational field of a body depends only on its mass, not on its rotation. The Lense\textendash Thirring effect is very small\textemdash about one part in a few trillion. To detect it, it is necessary to examine a very massive object, or build an instrument that is very sensitive. In 2015, new general-relativistic extensions of Newtonian rotation laws were formulated to describe geometric dragging of frames which incorporates a newly discovered antidragging effect.},
  annotation = {Page Version ID: 953603088},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\R55FDPIY\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@article{frankleLotteryTicketHypothesis2019,
  title = {The {{Lottery Ticket Hypothesis}}: {{Finding Sparse}}, {{Trainable Neural Networks}}},
  shorttitle = {The {{Lottery Ticket Hypothesis}}},
  author = {Frankle, Jonathan and Carbin, Michael},
  year = {2019},
  month = mar,
  abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance.},
  archivePrefix = {arXiv},
  eprint = {1803.03635},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\IKG8HYXE\\Frankle and Carbin - 2019 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf},
  journal = {arXiv:1803.03635 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-soon},
  language = {en},
  note = {Comment: ICLR camera ready},
  primaryClass = {cs}
}

@article{frankleTrainingBatchNormOnly2020,
  title = {Training {{BatchNorm}} and {{Only BatchNorm}}: {{On}} the {{Expressive Power}} of {{Random Features}} in {{CNNs}}},
  shorttitle = {Training {{BatchNorm}} and {{Only BatchNorm}}},
  author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
  year = {2020},
  month = feb,
  abstract = {Batch normalization (BatchNorm) has become an indispensable tool for training deep neural networks, yet it is still poorly understood. Although previous work has typically focused on its normalization component, BatchNorm also adds two per-feature trainable parameters: a coefficient and a bias. However, the role and expressive power of these parameters remains unclear. To study this question, we investigate the performance achieved when training only these parameters and freezing all others at their random initializations. We find that doing so leads to surprisingly high performance. For example, a sufficiently deep ResNet reaches 83\% accuracy on CIFAR-10 in this configuration. Interestingly, BatchNorm achieves this performance in part by naturally learning to disable around a third of the random features without any changes to the training objective. Not only do these results highlight the under-appreciated role of the affine parameters in BatchNorm, but - in a broader sense - they characterize the expressive power of neural networks constructed simply by shifting and rescaling random features.},
  archivePrefix = {arXiv},
  eprint = {2003.00152},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7H9HJBFM\\Frankle et al. - 2020 - Training BatchNorm and Only BatchNorm On the Expr.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\PA2CJHJI\\2003.html},
  journal = {arXiv:2003.00152 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{frankleTrainingBatchNormOnly2020a,
  title = {Training {{BatchNorm}} and {{Only BatchNorm}}: {{On}} the {{Expressive Power}} of {{Random Features}} in {{CNNs}}},
  shorttitle = {Training {{BatchNorm}} and {{Only BatchNorm}}},
  author = {Frankle, Jonathan and Schwab, David J. and Morcos, Ari S.},
  year = {2020},
  month = jun,
  abstract = {Batch normalization (BatchNorm) has become an indispensable tool for training deep neural networks, yet it is still poorly understood. Although previous work has typically focused on its normalization component, BatchNorm also adds two per-feature trainable parameters - a coefficient and a bias - whose role and expressive power remain unclear. To study this question, we investigate the performance achieved when training only these parameters and freezing all others at their random initializations. We find that doing so leads to surprisingly high performance. For example, sufficiently deep ResNets reach 82\% (CIFAR-10) and 32\% (ImageNet, top-5) accuracy in this configuration, far higher than when training an equivalent number of randomly chosen parameters from elsewhere in the network. BatchNorm achieves this performance in part by naturally learning to disable around a third of the random features. Not only do these results highlight the under-appreciated role of the affine parameters in BatchNorm, but - in a broader sense - they characterize the expressive power of neural networks constructed simply by shifting and rescaling random features.},
  archivePrefix = {arXiv},
  eprint = {2003.00152},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\8WSW6K9F\\Frankle et al. - 2020 - Training BatchNorm and Only BatchNorm On the Expr.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\TLRETE9L\\2003.html},
  journal = {arXiv:2003.00152 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: NeurIPS submission},
  primaryClass = {cs, stat}
}

@misc{FullStackDeep,
  title = {Full {{Stack Deep Learning}}},
  abstract = {Full Stack Deep Learning helps you bridge the gap from training machine learning models to deploying AI systems in the real world.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NWICBKU4\\course.fullstackdeeplearning.com.html},
  howpublished = {https://course.fullstackdeeplearning.com/}
}

@incollection{gaierWeightAgnosticNeural2019,
  title = {Weight {{Agnostic Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Gaier, Adam and Ha, David},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {5364--5378},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\DLVC9F86\\Gaier and Ha - 2019 - Weight Agnostic Neural Networks.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\W5NV9YYI\\8777-weight-agnostic-neural-networks.html}
}

@article{galDropoutBayesianApproximation,
  title = {Dropout as a {{Bayesian Approximation}}:  {{Representing Model Uncertainty}} in {{Deep Learning}}},
  author = {Gal, Yarin and Ghahramani, Zoubin},
  pages = {10},
  abstract = {Deep learning tools have gained tremendous attention in applied machine learning. However such tools for regression and classification do not capture model uncertainty. In comparison, Bayesian models offer a mathematically grounded framework to reason about model uncertainty, but usually come with a prohibitive computational cost. In this paper we develop a new theoretical framework casting dropout training in deep neural networks (NNs) as approximate Bayesian inference in deep Gaussian processes. A direct result of this theory gives us tools to model uncertainty with dropout NNs \textendash extracting information from existing models that has been thrown away so far. This mitigates the problem of representing uncertainty in deep learning without sacrificing either computational complexity or test accuracy. We perform an extensive study of the properties of dropout's uncertainty. Various network architectures and nonlinearities are assessed on tasks of regression and classification, using MNIST as an example. We show a considerable improvement in predictive log-likelihood and RMSE compared to existing state-of-the-art methods, and finish by using dropout's uncertainty in deep reinforcement learning.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\REC8YUP9\\Gal and Ghahramani - Dropout as a Bayesian Approximation  Representing.pdf},
  language = {en}
}

@article{galeStateSparsityDeep2019,
  title = {The {{State}} of {{Sparsity}} in {{Deep Neural Networks}}},
  author = {Gale, Trevor and Elsen, Erich and Hooker, Sara},
  year = {2019},
  month = feb,
  abstract = {We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle \& Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification.},
  archivePrefix = {arXiv},
  eprint = {1902.09574},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YRIJ3XJY\\Gale et al. - 2019 - The State of Sparsity in Deep Neural Networks.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\9BNDP43V\\1902.html},
  journal = {arXiv:1902.09574 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{garneloNeuralProcesses2018,
  title = {Neural {{Processes}}},
  author = {Garnelo, Marta and Schwarz, Jonathan and Rosenbaum, Dan and Viola, Fabio and Rezende, Danilo J. and Eslami, S. M. Ali and Teh, Yee Whye},
  year = {2018},
  month = jul,
  abstract = {A neural network (NN) is a parameterised function that can be tuned via gradient descent to approximate a labelled collection of data with high precision. A Gaussian process (GP), on the other hand, is a probabilistic model that defines a distribution over possible functions, and is updated in light of data via the rules of probabilistic inference. GPs are probabilistic, data-efficient and flexible, however they are also computationally intensive and thus limited in their applicability. We introduce a class of neural latent variable models which we call Neural Processes (NPs), combining the best of both worlds. Like GPs, NPs define distributions over functions, are capable of rapid adaptation to new observations, and can estimate the uncertainty in their predictions. Like NNs, NPs are computationally efficient during training and evaluation but also learn to adapt their priors to data. We demonstrate the performance of NPs on a range of learning tasks, including regression and optimisation, and compare and contrast with related models in the literature.},
  archivePrefix = {arXiv},
  eprint = {1807.01622},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SIC9SNX8\\Garnelo et al. - 2018 - Neural Processes.pdf},
  journal = {arXiv:1807.01622 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{geigerScalingDescriptionGeneralization2020,
  title = {Scaling Description of Generalization with Number of Parameters in Deep Learning},
  author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and {d'Ascoli}, St{\'e}phane and Biroli, Giulio and Hongler, Cl{\'e}ment and Wyart, Matthieu},
  year = {2020},
  month = feb,
  volume = {2020},
  pages = {023401},
  publisher = {{IOP Publishing}},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/ab633c},
  abstract = {Supervised deep learning involves the training of neural networks with a large number N of parameters. For large enough N, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as N grows past a certain threshold N *. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with N. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations of the neural net output function f N around its expectation . These affect the generalization error for classification: under natural assumptions, it decays to a plateau value in a power-law fashion {$\sim$}N -1/2. This description breaks down at a so-called jamming transition N = N *. At this threshold, we argue that diverges. This result leads to a plausible explanation for the cusp in test error known to occur at N *. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond N *, and averaging their outputs.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\WLERE5KX\\Geiger et al. - 2020 - Scaling description of generalization with number .pdf},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  language = {en},
  number = {2}
}

@article{geigerScalingDescriptionGeneralization2020a,
  title = {Scaling Description of Generalization with Number of Parameters in Deep Learning},
  author = {Geiger, Mario and Jacot, Arthur and Spigler, Stefano and Gabriel, Franck and Sagun, Levent and {d'Ascoli}, St{\'e}phane and Biroli, Giulio and Hongler, Cl{\'e}ment and Wyart, Matthieu},
  year = {2020},
  month = feb,
  volume = {2020},
  pages = {023401},
  issn = {1742-5468},
  doi = {10.1088/1742-5468/ab633c},
  abstract = {Supervised deep learning involves the training of neural networks with a large number \$N\$ of parameters. For large enough \$N\$, in the so-called over-parametrized regime, one can essentially fit the training data points. Sparsity-based arguments would suggest that the generalization error increases as \$N\$ grows past a certain threshold \$N\^\{*\}\$. Instead, empirical studies have shown that in the over-parametrized regime, generalization error keeps decreasing with \$N\$. We resolve this paradox through a new framework. We rely on the so-called Neural Tangent Kernel, which connects large neural nets to kernel methods, to show that the initialization causes finite-size random fluctuations \$\textbackslash |f\_\{N\}-\textbackslash bar\{f\}\_\{N\}\textbackslash |\textbackslash sim N\^\{-1/4\}\$ of the neural net output function \$f\_\{N\}\$ around its expectation \$\textbackslash bar\{f\}\_\{N\}\$. These affect the generalization error \$\textbackslash epsilon\_\{N\}\$ for classification: under natural assumptions, it decays to a plateau value \$\textbackslash epsilon\_\{\textbackslash infty\}\$ in a power-law fashion \$\textbackslash sim N\^\{-1/2\}\$. This description breaks down at a so-called jamming transition \$N=N\^\{*\}\$. At this threshold, we argue that \$\textbackslash |f\_\{N\}\textbackslash |\$ diverges. This result leads to a plausible explanation for the cusp in test error known to occur at \$N\^\{*\}\$. Our results are confirmed by extensive empirical observations on the MNIST and CIFAR image datasets. Our analysis finally suggests that, given a computational envelope, the smallest generalization error is obtained using several networks of intermediate sizes, just beyond \$N\^\{*\}\$, and averaging their outputs.},
  archivePrefix = {arXiv},
  eprint = {1901.01608},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\MU2PZNI9\\Geiger et al. - 2020 - Scaling description of generalization with number .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\WLJFWZRS\\1901.html},
  journal = {Journal of Statistical Mechanics: Theory and Experiment},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Disordered Systems and Neural Networks},
  note = {Comment: The clarity of the text has been improved: the section "Related works" has been updated and the section "3.1 Regression task" has been added},
  number = {2}
}

@inproceedings{georgakisMultiviewRGBDDataset2016,
  title = {Multiview {{RGB}}-{{D Dataset}} for {{Object Instance Detection}}},
  booktitle = {2016 {{Fourth International Conference}} on {{3D Vision}} ({{3DV}})},
  author = {Georgakis, Georgios and Reza, Md Alimoor and Mousavian, Arsalan and Le, Phi-Hung and Kosecka, Jana},
  year = {2016},
  month = oct,
  pages = {426--434},
  publisher = {{IEEE}},
  address = {{Stanford, CA, USA}},
  doi = {10.1109/3DV.2016.52},
  abstract = {This paper presents a new multi-view RGB-D dataset of nine kitchen scenes, each containing several objects in realistic cluttered environments including a subset of objects from the BigBird dataset [26]. The viewpoints of the scenes are densely sampled and objects in the scenes are annotated with bounding boxes and in the 3D point cloud. Also, an approach for detection and recognition is presented, which is comprised of two parts: i) a new multiview 3D proposal generation method and ii) the development of several recognition baselines using AlexNet [14] to score our proposals, which is trained either on crops of the dataset or on synthetically composited training images. Finally, we compare the performance of the object proposals and a detection baseline to the Washington RGBD Scenes (WRGB-D) dataset [15] and demonstrate that our Kitchen scenes dataset is more challenging for object detection and recognition. The dataset is available at: http: //cs.gmu.edu/\texttildelow robot/gmu-kitchens.html.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\XK34DMBY\\Georgakis et al. - 2016 - Multiview RGB-D Dataset for Object Instance Detect.pdf},
  isbn = {978-1-5090-5407-7},
  keywords = {GMU dataset,read-maybe},
  language = {en}
}

@article{georgakisSynthesizingTrainingData2017,
  title = {Synthesizing {{Training Data}} for {{Object Detection}} in {{Indoor Scenes}}},
  author = {Georgakis, Georgios and Mousavian, Arsalan and Berg, Alexander C. and Kosecka, Jana},
  year = {2017},
  doi = {10.15607/RSS.2017.XIII.043},
  abstract = {Detection of objects in cluttered indoor environments is one of the key enabling functionalities for service robots. The best performing object detection approaches in computer vision exploit deep Convolutional Neural Networks (CNN) to simultaneously detect and categorize the objects of interest in cluttered scenes. Training of such models typically requires large amounts of annotated training data which is time consuming and costly to obtain. In this work we explore the ability of using synthetically generated composite images for training state-of-the-art object detectors, especially for object instance detection. We superimpose 2D images of textured object models into images of real environments at variety of locations and scales. Our experiments evaluate different superimposition strategies ranging from purely image-based blending all the way to depth and semantics informed positioning of the object models into real scenes. We demonstrate the effectiveness of these object detector training strategies on two publicly available datasets, the GMU-Kitchens and the Washington RGB-D Scenes v2. As one observation, augmenting some hand-labeled training data with synthetic examples carefully composed onto scenes yields object detectors with comparable performance to using much more hand-labeled data. Broadly, this work charts new opportunities for training detectors for new objects by exploiting existing object model repositories in either a purely automatic fashion or with only a very small number of human-annotated examples.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\63KJHFJQ\\Georgakis et al. - 2017 - Synthesizing Training Data for Object Detection in.pdf},
  journal = {Robotics: Science and Systems},
  keywords = {read-next}
}

@article{ghiasiDropBlockRegularizationMethod,
  title = {{{DropBlock}}: {{A}} Regularization Method for Convolutional Networks},
  author = {Ghiasi, Golnaz and Lin, Tsung-Yi and Le, Quoc V},
  pages = {11},
  abstract = {Deep neural networks often work well when they are over-parameterized and trained with a massive amount of noise and regularization, such as weight decay and dropout. Although dropout is widely used as a regularization technique for fully connected layers, it is often less effective for convolutional layers. This lack of success of dropout for convolutional layers is perhaps due to the fact that activation units in convolutional layers are spatially correlated so information can still flow through convolutional networks despite dropout. Thus a structured form of dropout is needed to regularize convolutional networks. In this paper, we introduce DropBlock, a form of structured dropout, where units in a contiguous region of a feature map are dropped together. We found that applying DropbBlock in skip connections in addition to the convolution layers increases the accuracy. Also, gradually increasing number of dropped units during training leads to better accuracy and more robust to hyperparameter choices. Extensive experiments show that DropBlock works better than dropout in regularizing convolutional networks. On ImageNet classification, ResNet-50 architecture with DropBlock achieves 78.13\% accuracy, which is more than 1.6\% improvement on the baseline. On COCO detection, DropBlock improves Average Precision of RetinaNet from 36.8\% to 38.4\%.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6E7XVPSD\\Ghiasi et al. - DropBlock A regularization method for convolution.pdf},
  keywords = {read},
  language = {en}
}

@article{ginsburgStochasticGradientMethods2020,
  title = {Stochastic {{Gradient Methods}} with {{Layer}}-Wise {{Adaptive Moments}} for {{Training}} of {{Deep Networks}}},
  author = {Ginsburg, Boris and Castonguay, Patrice and Hrinchuk, Oleksii and Kuchaiev, Oleksii and Lavrukhin, Vitaly and Leary, Ryan and Li, Jason and Nguyen, Huyen and Zhang, Yang and Cohen, Jonathan M.},
  year = {2020},
  month = feb,
  abstract = {We propose NovoGrad, an adaptive stochastic gradient descent method with layer-wise gradient normalization and decoupled weight decay. In our experiments on neural networks for image classification, speech recognition, machine translation, and language modeling, it performs on par or better than well tuned SGD with momentum and Adam or AdamW. Additionally, NovoGrad (1) is robust to the choice of learning rate and weight initialization, (2) works well in a large batch setting, and (3) has two times smaller memory footprint than Adam.},
  archivePrefix = {arXiv},
  eprint = {1905.11286},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TYV7SDQL\\Ginsburg et al. - 2020 - Stochastic Gradient Methods with Layer-wise Adapti.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\ZXLNUBNH\\1905.html},
  journal = {arXiv:1905.11286 [cs, stat]},
  keywords = {Computer Science - Machine Learning,lr,Statistics - Machine Learning},
  note = {Comment: Preprint, under review},
  primaryClass = {cs, stat}
}

@article{gisinClassicalIntuitionisticMathematical2020,
  title = {Classical and Intuitionistic Mathematical Languages Shape Our Understanding of Time in Physics},
  author = {Gisin, Nicolas},
  year = {2020},
  month = feb,
  volume = {16},
  pages = {114--116},
  issn = {1745-2473, 1745-2481},
  doi = {10.1038/s41567-019-0748-5},
  abstract = {Physics is formulated in terms of timeless classical mathematics. A formulation on the basis of intuitionist mathematics, built on time-evolving processes, would offer a perspective that is closer to our experience of physical reality.},
  archivePrefix = {arXiv},
  eprint = {2002.01653},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\AQPXPTYE\\Gisin - 2020 - Classical and intuitionistic mathematical language.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\4T7SEPV7\\2002.html},
  journal = {Nature Physics},
  keywords = {Physics - History and Philosophy of Physics,Quantum Physics},
  note = {Comment: Submitted version of a comment to Nature Physics that appeared on line in January 2020},
  number = {2}
}

@article{glorotDeepSparseRectifier2011,
  title = {Deep {{Sparse Rectifier Neural Networks}}},
  author = {Glorot, Xavier and Bordes, Antoine and Bengio, Yoshua},
  year = {2011},
  month = jun,
  volume = {15},
  pages = {9},
  abstract = {While logistic sigmoid neurons are more biologically plausible than hyperbolic tangent neurons, the latter work better for training multi-layer neural networks. This paper shows that rectifying neurons are an even better model of biological neurons and yield equal or better performance than hyperbolic tangent networks in spite of the hard non-linearity and non-differentiability at zero, creating sparse representations with true zeros, which seem remarkably suitable for naturally sparse data. Even though they can take advantage of semi-supervised setups with extra-unlabeled data, deep rectifier networks can reach their best performance without requiring any unsupervised pre-training on purely supervised tasks with large labeled datasets. Hence, these results can be seen as a new milestone in the attempts at understanding the difficulty in training deep but purely supervised neural networks, and closing the performance gap between neural networks learnt with and without unsupervised pre-training.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7SKZX4UT\\Glorot et al. - Deep Sparse Rectiﬁer Neural Networks.pdf},
  journal = {The Journal of Machine Learning Research},
  keywords = {read-next,relu},
  language = {en}
}

@book{goodfellowDeepLearning2016,
  title = {Deep {{Learning}}},
  author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
  year = {2016},
  publisher = {{MIT Press}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\BZIQN5Z7\\www.deeplearningbook.org.html}
}

@article{goodfellowExplainingHarnessingAdversarial2015,
  title = {Explaining and {{Harnessing Adversarial Examples}}},
  author = {Goodfellow, Ian J. and Shlens, Jonathon and Szegedy, Christian},
  year = {2015},
  month = mar,
  abstract = {Several machine learning models, including neural networks, consistently misclassify adversarial examples\textemdash inputs formed by applying small but intentionally worst-case perturbations to examples from the dataset, such that the perturbed input results in the model outputting an incorrect answer with high confidence. Early attempts at explaining this phenomenon focused on nonlinearity and overfitting. We argue instead that the primary cause of neural networks' vulnerability to adversarial perturbation is their linear nature. This explanation is supported by new quantitative results while giving the first explanation of the most intriguing fact about them: their generalization across architectures and training sets. Moreover, this view yields a simple and fast method of generating adversarial examples. Using this approach to provide examples for adversarial training, we reduce the test set error of a maxout network on the MNIST dataset.},
  archivePrefix = {arXiv},
  eprint = {1412.6572},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JKYYXPVF\\Goodfellow et al. - 2015 - Explaining and Harnessing Adversarial Examples.pdf},
  journal = {arXiv:1412.6572 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-maybe,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{goodfellowGenerativeAdversarialNets,
  title = {Generative {{Adversarial Nets}}},
  author = {Goodfellow, Ian and {Pouget-Abadie}, Jean and Mirza, Mehdi and Xu, Bing and {Warde-Farley}, David and Ozair, Sherjil and Courville, Aaron and Bengio, Yoshua},
  pages = {9},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\UE7BVBEW\\Goodfellow et al. - Generative Adversarial Nets.pdf},
  keywords = {read-soon},
  language = {en}
}

@article{gopalakrishnanEarlyWarningSignals2016,
  title = {Early Warning Signals for Critical Transitions in a Thermoacoustic System},
  author = {Gopalakrishnan, E. A. and Sharma, Yogita and John, Tony and Dutta, Partha Sharathi and Sujith, R. I.},
  year = {2016},
  month = oct,
  volume = {6},
  pages = {35310},
  publisher = {{Nature Publishing Group}},
  issn = {2045-2322},
  doi = {10.1038/srep35310},
  abstract = {Dynamical systems can undergo critical transitions where the system suddenly shifts from one stable state to another at a critical threshold called the tipping point. The decrease in recovery rate to equilibrium (critical slowing down) as the system approaches the tipping point can be used to identify the proximity to a critical transition. Several measures have been adopted to provide early indications of critical transitions that happen in a variety of complex systems. In this study, we use early warning indicators to predict subcritical Hopf bifurcation occurring in a thermoacoustic system by analyzing the observables from experiments and from a theoretical model. We find that the early warning measures perform as robust indicators in the presence and absence of external noise. Thus, we illustrate the applicability of these indicators in an engineering system depicting critical transitions.},
  copyright = {2016 The Author(s)},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2D3X59TC\\Gopalakrishnan et al. - 2016 - Early warning signals for critical transitions in .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\YLGVZN3H\\srep35310.html},
  journal = {Scientific Reports},
  language = {en},
  number = {1}
}

@article{gotmareCLOSERLOOKDEEP2019,
  title = {A {{CLOSER LOOK AT DEEP LEARNING HEURISTICS}}: {{LEARNING RATE RESTARTS}}, {{WARMUP AND DISTILLA}}-},
  author = {Gotmare, Akhilesh and Keskar, Nitish Shirish and Xiong, Caiming and Socher, Richard},
  year = {2019},
  pages = {16},
  abstract = {The convergence rate and final performance of common deep learning models have significantly benefited from heuristics such as learning rate schedules, knowledge distillation, skip connections, and normalization layers. In the absence of theoretical underpinnings, controlled experiments aimed at explaining these strategies can aid our understanding of deep learning landscapes and the training dynamics. Existing approaches for empirical analysis rely on tools of linear interpolation and visualizations with dimensionality reduction, each with their limitations. Instead, we revisit such analysis of heuristics through the lens of recently proposed methods for loss surface and representation analysis, viz., mode connectivity and canonical correlation analysis (CCA), and hypothesize reasons for the success of the heuristics. In particular, we explore knowledge distillation and learning rate heuristics of (cosine) restarts and warmup using mode connectivity and CCA. Our empirical analysis suggests that: (a) the reasons often quoted for the success of cosine annealing are not evidenced in practice; (b) that the effect of learning rate warmup is to prevent the deeper layers from creating training instability; and (c) that the latent knowledge shared by the teacher is primarily disbursed to the deeper layers.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\N6CANTRT\\Gotmare et al. - 2019 - A CLOSER LOOK AT DEEP LEARNING HEURISTICS LEARNIN.pdf},
  keywords = {read-soon},
  language = {en}
}

@article{goyalAccurateLargeMinibatch2018,
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  year = {2018},
  month = apr,
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  archivePrefix = {arXiv},
  eprint = {1706.02677},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\9DRLIB3P\\Goyal et al. - 2018 - Accurate, Large Minibatch SGD Training ImageNet i.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\JH5HAPW3\\1706.html},
  journal = {arXiv:1706.02677 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,read-soon},
  note = {Comment: Tech report (v2: correct typos)},
  primaryClass = {cs}
}

@article{goyalAccurateLargeMinibatch2018a,
  title = {Accurate, {{Large Minibatch SGD}}: {{Training ImageNet}} in 1 {{Hour}}},
  shorttitle = {Accurate, {{Large Minibatch SGD}}},
  author = {Goyal, Priya and Doll{\'a}r, Piotr and Girshick, Ross and Noordhuis, Pieter and Wesolowski, Lukasz and Kyrola, Aapo and Tulloch, Andrew and Jia, Yangqing and He, Kaiming},
  year = {2018},
  month = apr,
  abstract = {Deep learning thrives with large neural networks and large datasets. However, larger networks and larger datasets result in longer training times that impede research and development progress. Distributed synchronous SGD offers a potential solution to this problem by dividing SGD minibatches over a pool of parallel workers. Yet to make this scheme efficient, the per-worker workload must be large, which implies nontrivial growth in the SGD minibatch size. In this paper, we empirically show that on the ImageNet dataset large minibatches cause optimization difficulties, but when these are addressed the trained networks exhibit good generalization. Specifically, we show no loss of accuracy when training with large minibatch sizes up to 8192 images. To achieve this result, we adopt a hyper-parameter-free linear scaling rule for adjusting learning rates as a function of minibatch size and develop a new warmup scheme that overcomes optimization challenges early in training. With these simple techniques, our Caffe2-based system trains ResNet-50 with a minibatch size of 8192 on 256 GPUs in one hour, while matching small minibatch accuracy. Using commodity hardware, our implementation achieves \textasciitilde 90\% scaling efficiency when moving from 8 to 256 GPUs. Our findings enable training visual recognition models on internet-scale data with high efficiency.},
  archivePrefix = {arXiv},
  eprint = {1706.02677},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SFX5F6DJ\\Goyal et al. - 2018 - Accurate, Large Minibatch SGD Training ImageNet i.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\YYS45SIE\\1706.html},
  journal = {arXiv:1706.02677 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,read-next},
  note = {Comment: Tech report (v2: correct typos)},
  primaryClass = {cs}
}

@article{grantTooMuchGood2011,
  title = {Too {{Much}} of a {{Good Thing}}: {{The Challenge}} and {{Opportunity}} of the {{Inverted U}}},
  shorttitle = {Too {{Much}} of a {{Good Thing}}},
  author = {Grant, Adam M. and Schwartz, Barry},
  year = {2011},
  month = jan,
  volume = {6},
  pages = {61--76},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691610393523},
  abstract = {Aristotle proposed that to achieve happiness and success, people should cultivate virtues at mean or intermediate levels between deficiencies and excesses. In stark contrast to this assertion that virtues have costs at high levels, a wealth of psychological research has focused on demonstrating the well-being and performance benefits of positive traits, states, and experiences. This focus has obscured the prevalence and importance of nonmonotonic inverted-U-shaped effects, whereby positive phenomena reach inflection points at which their effects turn negative. We trace the evidence for nonmonotonic effects in psychology and provide recommendations for conceptual and empirical progress. We conclude that for psychology in general and positive psychology in particular, Aristotle's idea of the mean may serve as a useful guide for developing both a descriptive and a prescriptive account of happiness and success.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CVVW4MGY\\Grant and Schwartz - 2011 - Too Much of a Good Thing The Challenge and Opport.pdf},
  journal = {Perspectives on Psychological Science},
  language = {en},
  number = {1}
}

@article{Gravitoelectromagnetism2020,
  title = {Gravitoelectromagnetism},
  year = {2020},
  month = jun,
  abstract = {Gravitoelectromagnetism, abbreviated GEM, refers to a set of formal analogies between the equations for electromagnetism and relativistic gravitation; specifically: between Maxwell's field equations and an approximation, valid under certain conditions, to the Einstein field equations for general relativity. Gravitomagnetism is a widely used term referring specifically to the kinetic effects of gravity, in analogy to the magnetic effects of moving electric charge. The most common version of GEM is valid only far from isolated sources, and for slowly moving test particles. The analogy and equations differing only by some small factors were first published in 1893, before general relativity, by Oliver Heaviside as a separate theory expanding Newton's law.},
  annotation = {Page Version ID: 962528070},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZD3GFIFA\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@article{greffLSTMSearchSpace2017,
  title = {{{LSTM}}: {{A Search Space Odyssey}}},
  shorttitle = {{{LSTM}}},
  author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutn{\'i}k, Jan and Steunebrink, Bas R. and Schmidhuber, J{\"u}rgen},
  year = {2017},
  month = oct,
  volume = {28},
  pages = {2222--2232},
  issn = {2162-237X, 2162-2388},
  doi = {10.1109/TNNLS.2016.2582924},
  abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs ({$\approx$} 15 years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
  archivePrefix = {arXiv},
  eprint = {1503.04069},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SAEKNMF2\\Greff et al. - 2017 - LSTM A Search Space Odyssey.pdf},
  journal = {IEEE Transactions on Neural Networks and Learning Systems},
  keywords = {68T10,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,H.5.5,I.2.6,I.2.7,I.5.1,lstm,read-maybe},
  language = {en},
  note = {Comment: 12 pages, 6 figures},
  number = {10}
}

@article{greseleRelativeGradientOptimization2020,
  title = {Relative Gradient Optimization of the {{Jacobian}} Term in Unsupervised Deep Learning},
  author = {Gresele, Luigi and Fissore, Giancarlo and Javaloy, Adri{\'a}n and Sch{\"o}lkopf, Bernhard and Hyv{\"a}rinen, Aapo},
  year = {2020},
  month = jun,
  abstract = {Learning expressive probabilistic models correctly describing the data is a ubiquitous problem in machine learning. A popular approach for solving it is mapping the observations into a representation space with a simple joint distribution, which can typically be written as a product of its marginals -- thus drawing a connection with the field of nonlinear independent component analysis. Deep density models have been widely used for this task, but their likelihood-based training requires estimating the log-determinant of the Jacobian and is computationally expensive, thus imposing a trade-off between computation and expressive power. In this work, we propose a new approach for exact likelihood-based training of such neural networks. Based on relative gradients, we exploit the matrix structure of neural network parameters to compute updates efficiently even in high-dimensional spaces; the computational cost of the training is quadratic in the input size, in contrast with the cubic scaling of the naive approaches. This allows fast training with objective functions involving the log-determinant of the Jacobian without imposing constraints on its structure, in stark contrast to normalizing flows. An implementation of our method can be found at https://github.com/fissoreg/relative-gradient-jacobian},
  archivePrefix = {arXiv},
  eprint = {2006.15090},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\BIWHXGUR\\Gresele et al. - 2020 - Relative gradient optimization of the Jacobian ter.pdf},
  journal = {arXiv:2006.15090 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{guenAugmentingPhysicalModels2020,
  title = {Augmenting {{Physical Models}} with {{Deep Networks}} for {{Complex Dynamics Forecasting}}},
  author = {Guen, Vincent Le and Yin, Yuan and Dona, J{\'e}r{\'e}mie and Ayed, Ibrahim and {de B{\'e}zenac}, Emmanuel and Thome, Nicolas and Gallinari, Patrick},
  year = {2020},
  month = oct,
  abstract = {Forecasting complex dynamical phenomena in settings where only partial knowledge of their dynamics is available is a prevalent problem across various scientific fields. While purely data-driven approaches are arguably insufficient in this context, standard physical modeling based approaches tend to be over-simplistic, inducing non-negligible errors. In this work, we introduce the APHYNITY framework, a principled approach for augmenting incomplete physical dynamics described by differential equations with deep data-driven models. It consists in decomposing the dynamics into two components: a physical component accounting for the dynamics for which we have some prior knowledge, and a data-driven component accounting for errors of the physical model. The learning problem is carefully formulated such that the physical model explains as much of the data as possible, while the data-driven component only describes information that cannot be captured by the physical model, no more, no less. This not only provides the existence and uniqueness for this decomposition, but also ensures interpretability and benefits generalization. Experiments made on three important use cases, each representative of a different family of phenomena, i.e. reaction-diffusion equations, wave equations and the non-linear damped pendulum, show that APHYNITY can efficiently leverage approximate physical models to accurately forecast the evolution of the system and correctly identify relevant physical parameters.},
  archivePrefix = {arXiv},
  eprint = {2010.04456},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\G6NBH4NY\\Guen et al. - 2020 - Augmenting Physical Models with Deep Networks for .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\5IARBY7V\\2010.html},
  journal = {arXiv:2010.04456 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{guilbeaultComplexContagionsDecade2018,
  title = {Complex {{Contagions}}: {{A Decade}} in {{Review}}},
  shorttitle = {Complex {{Contagions}}},
  booktitle = {Complex {{Spreading Phenomena}} in {{Social Systems}}},
  author = {Guilbeault, Douglas and Becker, Joshua and Centola, Damon},
  editor = {Lehmann, Sune and Ahn, Yong-Yeol},
  year = {2018},
  pages = {3--25},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-319-77332-2_1},
  abstract = {Since the publication of ``Complex Contagions and the Weakness of Long Ties'' in 2007, complex contagions have been studied across an enormous variety of social domains. In reviewing this decade of research, we discuss recent advancements in applied studies of complex contagions, particularly in the domains of health, innovation diffusion, social media, and politics. We also discuss how these empirical studies have spurred complementary advancements in the theoretical modeling of contagions, which concern the effects of network topology on diffusion, as well as the effects of individual-level attributes and thresholds. In synthesizing these developments, we suggest three main directions for future research. The first concerns the study of how multiple contagions interact within the same network and across networks, in what may be called an ecology of contagions. The second concerns the study of how the structure of thresholds and their behavioral consequences can vary by individual and social context. The third area concerns the roles of diversity and homophily in the dynamics of complex contagion, including both diversity of demographic profiles among local peers, and the broader notion of structural diversity within a network. Throughout this discussion, we make an effort to highlight the theoretical and empirical opportunities that lie ahead.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\QHLU2W4V\\Guilbeault et al. - 2018 - Complex Contagions A Decade in Review.pdf},
  isbn = {978-3-319-77331-5 978-3-319-77332-2},
  language = {en}
}

@article{haffXrayDetectionDefects2008,
  title = {X-Ray Detection of Defects and Contaminants in the Food Industry},
  author = {Haff, Ronald P. and Toyofuku, Natsuko},
  year = {2008},
  month = dec,
  volume = {2},
  pages = {262--273},
  issn = {1932-7587, 1932-9954},
  doi = {10.1007/s11694-008-9059-8},
  abstract = {The ability of X-rays to traverse through matter and reveal hidden contaminants or defects has led to their extensive use in manufacturing industries for quality control inspection. The difficulties inherent in the detection of defects and contaminants in food products have kept the use of X-ray in that industry limited mainly to the packaged foods sector. Nevertheless, the need for non-destructive internal product inspection has motivated a considerable research effort in this field spanning many decades. Improvements in technology, especially more compact and affordable high voltage power sources, high speed computing, and high resolution detector arrays, have made many X-ray detection tasks possible today that were previously unfeasible. These improvements can be expected to continue into the future. The purpose of this article is to give a review of research activity related to the use of X-ray imaging for the detection of defects and contaminants in agricultural commodities and discuss improvements in technology required to improve these detection capabilities.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2STB77XQ\\Haff and Toyofuku - 2008 - X-ray detection of defects and contaminants in the.pdf},
  journal = {Sensing and Instrumentation for Food Quality and Safety},
  keywords = {introduction},
  language = {en},
  number = {4}
}

@article{hafnerMasterEsisHas,
  title = {Is {{Master}}'s Esis Has Been Carried out at},
  author = {H{\"a}fner, Dion},
  pages = {108},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\WDB468RY\\Häfner - is Master’s esis has been carried out at.pdf},
  language = {en}
}

@article{hanDeepCompressionCompressing2016,
  title = {Deep {{Compression}}: {{Compressing Deep Neural Networks}} with {{Pruning}}, {{Trained Quantization}} and {{Huffman Coding}}},
  shorttitle = {Deep {{Compression}}},
  author = {Han, Song and Mao, Huizi and Dally, William J.},
  year = {2016},
  month = feb,
  abstract = {Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce ``deep compression'', a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35\texttimes{} to 49\texttimes{} without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9\texttimes{} to 13\texttimes; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35\texttimes, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49\texttimes{} from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3\texttimes{} to 4\texttimes{} layerwise speedup and 3\texttimes{} to 7\texttimes{} better energy efficiency.},
  archivePrefix = {arXiv},
  eprint = {1510.00149},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\B5VJ7JB3\\Han et al. - 2016 - Deep Compression Compressing Deep Neural Networks.pdf},
  journal = {arXiv:1510.00149 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing,pruning},
  language = {en},
  note = {Comment: Published as a conference paper at ICLR 2016 (oral)},
  primaryClass = {cs}
}

@inproceedings{heBagTricksImage2019,
  title = {Bag of {{Tricks}} for {{Image Classification}} with {{Convolutional Neural Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  year = {2019},
  month = jun,
  pages = {558--567},
  issn = {2575-7075},
  doi = {10.1109/CVPR.2019.00065},
  abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TMRPMBHB\\He et al. - 2019 - Bag of Tricks for Image Classification with Convol.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\RUMQPBWZ\\8954382.html},
  keywords = {Categorization,CNN models,convolutional neural nets,convolutional neural networks,data augmentations,Deep Learning,image classification,image classification research,learning (artificial intelligence),object detection,optimization methods,read,Recognition: Detection,ResNet-50 top-1 validation accuracy,Retrieval,semantic segmentation,transfer learning performance}
}

@inproceedings{heBagTricksImage2019a,
  title = {Bag of {{Tricks}} for {{Image Classification}} with {{Convolutional Neural Networks}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {He, Tong and Zhang, Zhi and Zhang, Hang and Zhang, Zhongyue and Xie, Junyuan and Li, Mu},
  year = {2019},
  month = jun,
  pages = {558--567},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00065},
  abstract = {Much of the recent progress made in image classification research can be credited to training procedure refinements, such as changes in data augmentations and optimization methods. In the literature, however, most refinements are either briefly mentioned as implementation details or only visible in source code. In this paper, we will examine a collection of such refinements and empirically evaluate their impact on the final model accuracy through ablation study. We will show that, by combining these refinements together, we are able to improve various CNN models significantly. For example, we raise ResNet-50's top-1 validation accuracy from 75.3\% to 79.29\% on ImageNet. We will also demonstrate that improvement on image classification accuracy leads to better transfer learning performance in other application domains such as object detection and semantic segmentation.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\MMC3VRQX\\He et al. - 2019 - Bag of Tricks for Image Classification with Convol.pdf},
  isbn = {978-1-72813-293-8},
  keywords = {read-next},
  language = {en}
}

@article{heDeepResidualLearning2015,
  title = {Deep {{Residual Learning}} for {{Image Recognition}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = dec,
  abstract = {Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers---8x deeper than VGG nets but still having lower complexity. An ensemble of these residual nets achieves 3.57\% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28\% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC \& COCO 2015 competitions, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation.},
  archivePrefix = {arXiv},
  eprint = {1512.03385},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Z26S5PQ4\\He et al. - 2015 - Deep Residual Learning for Image Recognition.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\7DHAVJXJ\\1512.html},
  journal = {arXiv:1512.03385 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read},
  note = {Comment: Tech report},
  primaryClass = {cs}
}

@article{heDelvingDeepRectifiers2015,
  ids = {heDelvingDeepRectifiers2015a},
  title = {Delving {{Deep}} into {{Rectifiers}}: {{Surpassing Human}}-{{Level Performance}} on {{ImageNet Classification}}},
  shorttitle = {Delving {{Deep}} into {{Rectifiers}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2015},
  month = feb,
  abstract = {Rectified activation units (rectifiers) are essential for state-of-the-art neural networks. In this work, we study rectifier neural networks for image classification from two aspects. First, we propose a Parametric Rectified Linear Unit (PReLU) that generalizes the traditional rectified unit. PReLU improves model fitting with nearly zero extra computational cost and little overfitting risk. Second, we derive a robust initialization method that particularly considers the rectifier nonlinearities. This method enables us to train extremely deep rectified models directly from scratch and to investigate deeper or wider network architectures. Based on our PReLU networks (PReLU-nets), we achieve 4.94\% top-5 test error on the ImageNet 2012 classification dataset. This is a 26\% relative improvement over the ILSVRC 2014 winner (GoogLeNet, 6.66\% [29]). To our knowledge, our result is the first to surpass human-level performance (5.1\%, [22]) on this visual recognition challenge.},
  archivePrefix = {arXiv},
  eprint = {1502.01852},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\53K8DPS5\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\ECW7T7VA\\He et al. - 2015 - Delving Deep into Rectifiers Surpassing Human-Lev.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\26DLLRSV\\1502.html},
  journal = {arXiv:1502.01852 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{heIdentityMappingsDeep2016,
  title = {Identity {{Mappings}} in {{Deep Residual Networks}}},
  author = {He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  year = {2016},
  month = jul,
  abstract = {Deep residual networks [1] have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62\% error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/ resnet-1k-layers.},
  archivePrefix = {arXiv},
  eprint = {1603.05027},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\B8J86NSR\\He et al. - 2016 - Identity Mappings in Deep Residual Networks.pdf},
  journal = {arXiv:1603.05027 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  note = {Comment: ECCV 2016 camera-ready},
  primaryClass = {cs}
}

@article{heinTikz3dplotPackage,
  title = {The Tikz-3dplot {{Package}}},
  author = {Hein, Jeff},
  pages = {44},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TJ3JHNA4\\Hein - The tikz-3dplot Package.pdf},
  keywords = {latex,tikz},
  language = {en}
}

@article{henaffDataEfficientImageRecognition2019,
  title = {Data-{{Efficient Image Recognition}} with {{Contrastive Predictive Coding}}},
  author = {H{\'e}naff, Olivier J. and Srinivas, Aravind and De Fauw, Jeffrey and Razavi, Ali and Doersch, Carl and Eslami, S. M. Ali and van den Oord, Aaron},
  year = {2019},
  month = dec,
  abstract = {Human observers can learn to recognize new categories of images from a handful of examples, yet doing so with machine perception remains an open challenge. We hypothesize that data-efficient recognition is enabled by representations which make the variability in natural signals more predictable. We therefore revisit and improve Contrastive Predictive Coding, an unsupervised objective for learning such representations. This new implementation produces features which support state-of-the-art linear classification accuracy on the ImageNet dataset. When used as input for non-linear classification with deep neural networks, this representation allows us to use 2-5x less labels than classifiers trained directly on image pixels. Finally, this unsupervised representation substantially improves transfer learning to object detection on PASCAL VOC-2007, surpassing fully supervised pre-trained ImageNet classifiers.},
  archivePrefix = {arXiv},
  eprint = {1905.09272},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\9K4VY237\\Hénaff et al. - 2019 - Data-Efficient Image Recognition with Contrastive .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\YG52AAVR\\1905.html},
  journal = {arXiv:1905.09272 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{heoSlowingWeightNorm2020,
  title = {Slowing {{Down}} the {{Weight Norm Increase}} in {{Momentum}}-Based {{Optimizers}}},
  author = {Heo, Byeongho and Chun, Sanghyuk and Oh, Seong Joon and Han, Dongyoon and Yun, Sangdoo and Uh, Youngjung and Ha, Jung-Woo},
  year = {2020},
  month = jun,
  abstract = {Normalization techniques, such as batch normalization (BN), have led to significant improvements in deep neural network performances. Prior studies have analyzed the benefits of the resulting scale invariance of the weights for the gradient descent (GD) optimizers: it leads to a stabilized training due to the auto-tuning of step sizes. However, we show that, combined with the momentum-based algorithms, the scale invariance tends to induce an excessive growth of the weight norms. This in turn overly suppresses the effective step sizes during training, potentially leading to sub-optimal performances in deep neural networks. We analyze this phenomenon both theoretically and empirically. We propose a simple and effective solution: at each iteration of momentum-based GD optimizers (e.g. SGD or Adam) applied on scale-invariant weights (e.g. Conv weights preceding a BN layer), we remove the radial component (i.e. parallel to the weight vector) from the update vector. Intuitively, this operation prevents the unnecessary update along the radial direction that only increases the weight norm without contributing to the loss minimization. We verify that the modified optimizers SGDP and AdamP successfully regularize the norm growth and improve the performance of a broad set of models. Our experiments cover tasks including image classification and retrieval, object detection, robustness benchmarks, and audio classification. Source code is available at https://github.com/clovaai/AdamP.},
  archivePrefix = {arXiv},
  eprint = {2006.08217},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2SR7I4T8\\Heo et al. - 2020 - Slowing Down the Weight Norm Increase in Momentum-.pdf},
  journal = {arXiv:2006.08217 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,lr,Statistics - Machine Learning},
  language = {en},
  note = {Comment: 16 pages, 9 figures. First two authors contributed equally},
  primaryClass = {cs, stat}
}

@article{hernandez-garciaDataAugmentationInstead2019,
  title = {Data Augmentation Instead of Explicit Regularization},
  author = {{Hern{\'a}ndez-Garc{\'i}a}, Alex and K{\"o}nig, Peter},
  year = {2019},
  month = jun,
  abstract = {Modern deep artificial neural networks have achieved impressive results through models with orders of magnitude more parameters than training examples which control overfitting with the help of regularization. Regularization can be implicit, as is the case of stochastic gradient descent and parameter sharing in convolutional layers, or explicit. Explicit regularization techniques, most common forms are weight decay and dropout, have proven successful in terms of improved generalization, but they blindly reduce the effective capacity of the model, introduce sensitive hyper-parameters and require deeper and wider architectures to compensate for the reduced capacity. In contrast, data augmentation techniques exploit domain knowledge to increase the number of training examples and improve generalization without reducing the effective capacity and without introducing model-dependent parameters, since it is applied on the training data. In this paper we systematically contrast data augmentation and explicit regularization on three popular architectures and three data sets. Our results demonstrate that data augmentation alone can achieve the same performance or higher as regularized models and exhibits much higher adaptability to changes in the architecture and the amount of training data.},
  archivePrefix = {arXiv},
  eprint = {1806.03852},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PCSQBM4U\\Hernández-García and König - 2019 - Data augmentation instead of explicit regularizati.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\QLKC3FAG\\1806.html},
  journal = {arXiv:1806.03852 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,dropout},
  note = {Comment: Major changes: 1. New section (3. Theoretical insights), with theoretical insights from statistical learning theory 2. Supplementary material is restructured; new section with a discussion about regularization taxonomy 3. The overall text has been revised, hopefully improved},
  primaryClass = {cs}
}

@article{hesselRainbowCombiningImprovements2017,
  title = {Rainbow: {{Combining Improvements}} in {{Deep Reinforcement Learning}}},
  shorttitle = {Rainbow},
  author = {Hessel, Matteo and Modayil, Joseph and {van Hasselt}, Hado and Schaul, Tom and Ostrovski, Georg and Dabney, Will and Horgan, Dan and Piot, Bilal and Azar, Mohammad and Silver, David},
  year = {2017},
  month = oct,
  abstract = {The deep reinforcement learning community has made several independent improvements to the DQN algorithm. However, it is unclear which of these extensions are complementary and can be fruitfully combined. This paper examines six extensions to the DQN algorithm and empirically studies their combination. Our experiments show that the combination provides state-of-the-art performance on the Atari 2600 benchmark, both in terms of data efficiency and final performance. We also provide results from a detailed ablation study that shows the contribution of each component to overall performance.},
  archivePrefix = {arXiv},
  eprint = {1710.02298},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\9GUWALCQ\\Hessel et al. - 2017 - Rainbow Combining Improvements in Deep Reinforcem.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\YRZ5KLWG\\1710.html},
  journal = {arXiv:1710.02298 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Reinforcement learning},
  note = {Comment: Under review as a conference paper at AAAI 2018},
  primaryClass = {cs}
}

@article{hintonImprovingNeuralNetworks2012,
  title = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
  author = {Hinton, Geoffrey E. and Srivastava, Nitish and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan R.},
  year = {2012},
  month = jul,
  abstract = {When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This "overfitting" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random "dropout" gives big improvements on many benchmark tasks and sets new records for speech and object recognition.},
  archivePrefix = {arXiv},
  eprint = {1207.0580},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\AD29FQ23\\Hinton et al. - 2012 - Improving neural networks by preventing co-adaptat.pdf},
  journal = {arXiv:1207.0580 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-someday},
  language = {en},
  primaryClass = {cs}
}

@article{hofferTrainLongerGeneralize,
  title = {Train Longer, Generalize Better: Closing the Generalization Gap in Large Batch Training of Neural Networks},
  author = {Hoffer, Elad and Hubara, Itay and Soudry, Daniel},
  pages = {11},
  abstract = {Background: Deep learning models are typically trained using stochastic gradient descent or one of its variants. These methods update the weights using their gradient, estimated from a small fraction of the training data. It has been observed that when using large batch sizes there is a persistent degradation in generalization performance - known as the "generalization gap" phenomenon. Identifying the origin of this gap and closing it had remained an open problem.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7BNEUB6T\\Hoffer et al. - Train longer, generalize better closing the gener.pdf},
  language = {en}
}

@article{hoffmanAcmeResearchFramework2020,
  title = {Acme: {{A Research Framework}} for {{Distributed Reinforcement Learning}}},
  shorttitle = {Acme},
  author = {Hoffman, Matt and Shahriari, Bobak and Aslanides, John and {Barth-Maron}, Gabriel and Behbahani, Feryal and Norman, Tamara and Abdolmaleki, Abbas and Cassirer, Albin and Yang, Fan and Baumli, Kate and Henderson, Sarah and Novikov, Alex and Colmenarejo, Sergio G{\'o}mez and Cabi, Serkan and Gulcehre, Caglar and Paine, Tom Le and Cowie, Andrew and Wang, Ziyu and Piot, Bilal and {de Freitas}, Nando},
  year = {2020},
  month = jun,
  abstract = {Deep reinforcement learning has led to many recent-and groundbreaking-advancements. However, these advances have often come at the cost of both the scale and complexity of the underlying RL algorithms. Increases in complexity have in turn made it more difficult for researchers to reproduce published RL algorithms or rapidly prototype ideas. To address this, we introduce Acme, a tool to simplify the development of novel RL algorithms that is specifically designed to enable simple agent implementations that can be run at various scales of execution. Our aim is also to make the results of various RL algorithms developed in academia and industrial labs easier to reproduce and extend. To this end we are releasing baseline implementations of various algorithms, created using our framework. In this work we introduce the major design decisions behind Acme and show how these are used to construct these baselines. We also experiment with these agents at different scales of both complexity and computation-including distributed versions. Ultimately, we show that the design decisions behind Acme lead to agents that can be scaled both up and down and that, for the most part, greater levels of parallelization result in agents with equivalent performance, just faster.},
  archivePrefix = {arXiv},
  eprint = {2006.00979},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\KQPXMBMZ\\Hoffman et al. - 2020 - Acme A Research Framework for Distributed Reinfor.pdf},
  journal = {arXiv:2006.00979 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@misc{HomeCookiecutterData,
  title = {Home - {{Cookiecutter Data Science}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NNY2HLEB\\cookiecutter-data-science.html},
  howpublished = {https://drivendata.github.io/cookiecutter-data-science/}
}

@article{howardMobileNetsEfficientConvolutional2017,
  title = {{{MobileNets}}: {{Efficient Convolutional Neural Networks}} for {{Mobile Vision Applications}}},
  shorttitle = {{{MobileNets}}},
  author = {Howard, Andrew G. and Zhu, Menglong and Chen, Bo and Kalenichenko, Dmitry and Wang, Weijun and Weyand, Tobias and Andreetto, Marco and Adam, Hartwig},
  year = {2017},
  month = apr,
  abstract = {We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depthwise separable convolutions to build light weight deep neural networks. We introduce two simple global hyperparameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization.},
  archivePrefix = {arXiv},
  eprint = {1704.04861},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZHG7D2TF\\Howard et al. - 2017 - MobileNets Efficient Convolutional Neural Network.pdf},
  journal = {arXiv:1704.04861 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-soon},
  language = {en},
  primaryClass = {cs}
}

@misc{HttpsCoursesCs,
  title = {{{https://courses.cs.washington.edu/courses/cse522/11wi/scribes.html}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FZNLEPBC\\scribes.html},
  howpublished = {https://courses.cs.washington.edu/courses/cse522/11wi/scribes.html},
  keywords = {generalization}
}

@inproceedings{huangArbitraryStyleTransfer2017,
  title = {Arbitrary {{Style Transfer}} in {{Real}}-{{Time}} with {{Adaptive Instance Normalization}}},
  booktitle = {2017 {{IEEE International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Huang, Xun and Belongie, Serge},
  year = {2017},
  month = oct,
  pages = {1510--1519},
  publisher = {{IEEE}},
  address = {{Venice}},
  doi = {10.1109/ICCV.2017.167},
  abstract = {Gatys et al. recently introduced a neural algorithm that renders a content image in the style of another image, achieving so-called style transfer. However, their framework requires a slow iterative optimization process, which limits its practical application. Fast approximations with feed-forward neural networks have been proposed to speed up neural style transfer. Unfortunately, the speed improvement comes at a cost: the network is usually tied to a fixed set of styles and cannot adapt to arbitrary new styles. In this paper, we present a simple yet effective approach that for the first time enables arbitrary style transfer in real-time. At the heart of our method is a novel adaptive instance normalization (AdaIN) layer that aligns the mean and variance of the content features with those of the style features. Our method achieves speed comparable to the fastest existing approach, without the restriction to a pre-defined set of styles. In addition, our approach allows flexible user controls such as content-style trade-off, style interpolation, color \& spatial controls, all using a single feed-forward neural network.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TB6AUUXL\\Huang and Belongie - 2017 - Arbitrary Style Transfer in Real-Time with Adaptiv.pdf},
  isbn = {978-1-5386-1032-9},
  language = {en}
}

@inproceedings{huangDenselyConnectedConvolutional2017,
  title = {Densely {{Connected Convolutional Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Huang, Gao and Liu, Zhuang and van der Maaten, Laurens and Weinberger, Kilian Q.},
  year = {2017},
  month = jul,
  pages = {2261--2269},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.243},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\254HHXM2\\Huang et al. - 2017 - Densely Connected Convolutional Networks.pdf},
  isbn = {978-1-5386-0457-1},
  keywords = {read-soon},
  language = {en}
}

@article{huangDenselyConnectedConvolutional2018,
  title = {Densely {{Connected Convolutional Networks}}},
  author = {Huang, Gao and Liu, Zhuang and {van der Maaten}, Laurens and Weinberger, Kilian Q.},
  year = {2018},
  month = jan,
  abstract = {Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections - one between each layer and its subsequent layer - our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet .},
  archivePrefix = {arXiv},
  eprint = {1608.06993},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\EAVEKTJB\\Huang et al. - 2018 - Densely Connected Convolutional Networks.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\259CPN99\\1608.html},
  journal = {arXiv:1608.06993 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-next},
  note = {Comment: CVPR 2017},
  primaryClass = {cs}
}

@article{huangSNDCNNSelfnormalizingDeep2020,
  title = {{{SNDCNN}}: {{Self}}-Normalizing Deep {{CNNs}} with Scaled Exponential Linear Units for Speech Recognition},
  shorttitle = {{{SNDCNN}}},
  author = {Huang, Zhen and Ng, Tim and Liu, Leo and Mason, Henry and Zhuang, Xiaodan and Liu, Daben},
  year = {2020},
  month = mar,
  abstract = {Very deep CNNs achieve state-of-the-art results in both computer vision and speech recognition, but are difficult to train. The most popular way to train very deep CNNs is to use shortcut connections (SC) together with batch normalization (BN). Inspired by Self- Normalizing Neural Networks, we propose the self-normalizing deep CNN (SNDCNN) based acoustic model topology, by removing the SC/BN and replacing the typical RELU activations with scaled exponential linear unit (SELU) in ResNet-50. SELU activations make the network self-normalizing and remove the need for both shortcut connections and batch normalization. Compared to ResNet- 50, we can achieve the same or lower (up to 4.5\% relative) word error rate (WER) while boosting both training and inference speed by 60\%-80\%. We also explore other model inference optimization schemes to further reduce latency for production use.},
  archivePrefix = {arXiv},
  eprint = {1910.01992},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\3WN5L826\\Huang et al. - 2020 - SNDCNN Self-normalizing deep CNNs with scaled exp.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\9YRMU5FX\\1910.html},
  journal = {arXiv:1910.01992 [cs, eess, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Sound,Electrical Engineering and Systems Science - Audio and Speech Processing,read-soon,Statistics - Machine Learning},
  primaryClass = {cs, eess, stat}
}

@article{huangStyleDistributionFeatures2020,
  title = {Style Is a {{Distribution}} of {{Features}}},
  author = {Huang, Eddie and Gupta, Sahil},
  year = {2020},
  month = jul,
  abstract = {Neural style transfer (NST) is a powerful image generation technique that uses a convolutional neural network (CNN) to merge the content of one image with the style of another. Contemporary methods of NST use first or second order statistics of the CNN's features to achieve transfers with relatively little computational cost. However, these methods cannot fully extract the style from the CNN's features. We present a new algorithm for style transfer that fully extracts the style from the features by redefining the style loss as the Wasserstein distance between the distribution of features. Thus, we set a new standard in style transfer quality. In addition, we state two important interpretations of NST. The first is a re-emphasis from Li et al. [2017a], which states that style is simply the distribution of features. The second states that NST is a type of generative adversarial network (GAN) problem.},
  archivePrefix = {arXiv},
  eprint = {2007.13010},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YVIUXQ9J\\Huang and Gupta - 2020 - Style is a Distribution of Features.pdf},
  journal = {arXiv:2007.13010 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  language = {en},
  primaryClass = {cs, eess}
}

@article{huSqueezeandExcitationNetworks2019,
  title = {Squeeze-and-{{Excitation Networks}}},
  author = {Hu, Jie and Shen, Li and Albanie, Samuel and Sun, Gang and Wu, Enhua},
  year = {2019},
  month = may,
  abstract = {The central building block of convolutional neural networks (CNNs) is the convolution operator, which enables networks to construct informative features by fusing both spatial and channel-wise information within local receptive fields at each layer. A broad range of prior research has investigated the spatial component of this relationship, seeking to strengthen the representational power of a CNN by enhancing the quality of spatial encodings throughout its feature hierarchy. In this work, we focus instead on the channel relationship and propose a novel architectural unit, which we term the "Squeeze-and-Excitation" (SE) block, that adaptively recalibrates channel-wise feature responses by explicitly modelling interdependencies between channels. We show that these blocks can be stacked together to form SENet architectures that generalise extremely effectively across different datasets. We further demonstrate that SE blocks bring significant improvements in performance for existing state-of-the-art CNNs at slight additional computational cost. Squeeze-and-Excitation Networks formed the foundation of our ILSVRC 2017 classification submission which won first place and reduced the top-5 error to 2.251\%, surpassing the winning entry of 2016 by a relative improvement of \textasciitilde 25\%. Models and code are available at https://github.com/hujie-frank/SENet.},
  archivePrefix = {arXiv},
  eprint = {1709.01507},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\65GVMMID\\Hu et al. - 2019 - Squeeze-and-Excitation Networks.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\D5VV2GB8\\1709.html},
  journal = {arXiv:1709.01507 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-next},
  note = {Comment: journal version of the CVPR 2018 paper, accepted by TPAMI},
  primaryClass = {cs}
}

@misc{ICCV2017Open,
  title = {{{ICCV}} 2017 {{Open Access Repository}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7MP2N8YS\\Huang_Arbitrary_Style_Transfer_ICCV_2017_paper.html},
  howpublished = {http://openaccess.thecvf.com/content\_iccv\_2017/html/Huang\_Arbitrary\_Style\_Transfer\_ICCV\_2017\_paper.html}
}

@misc{iljaIlmoiMMLBook2020,
  title = {Ilmoi/{{MML}}-{{Book}}},
  author = {{ilja}},
  year = {2020},
  month = sep,
  abstract = {Code / solutions for Mathematics for Machine Learning (MML Book)},
  keywords = {machine-learning,math-for-machine-learning,mml-book,self-study}
}

@misc{InstallingPythonPackages,
  title = {Installing {{Python Packages}} from a {{Jupyter Notebook}} | {{Pythonic Perambulations}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\8XF7WGVF\\installing-python-packages-from-jupyter.html},
  howpublished = {https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/}
}

@article{ioffeBatchNormalizationAccelerating2015,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = mar,
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization. It also acts as a regularizer, in some cases eliminating the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batchnormalized networks, we improve upon the best published result on ImageNet classification: reaching 4.9\% top-5 validation error (and 4.8\% test error), exceeding the accuracy of human raters.},
  archivePrefix = {arXiv},
  eprint = {1502.03167},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YPET2943\\Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf},
  journal = {arXiv:1502.03167 [cs]},
  keywords = {Computer Science - Machine Learning,read},
  language = {en},
  primaryClass = {cs}
}

@inproceedings{ioffeBatchNormalizationAccelerating2015a,
  title = {Batch {{Normalization}}: {{Accelerating Deep Network Training}} by {{Reducing Internal Covariate Shift}}},
  shorttitle = {Batch {{Normalization}}},
  booktitle = {International {{Conference}} on {{Machine Learning}}},
  author = {Ioffe, Sergey and Szegedy, Christian},
  year = {2015},
  month = jun,
  pages = {448--456},
  publisher = {{PMLR}},
  issn = {1938-7228},
  abstract = {Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the t...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\KM8UX6CC\\Ioffe and Szegedy - 2015 - Batch Normalization Accelerating Deep Network Tra.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\IRYQL5GQ\\ioffe15.html},
  keywords = {read},
  language = {en}
}

@article{istrateIncrementalTrainingDeep2018,
  ids = {istrateIncrementalTrainingDeep2018a},
  title = {Incremental {{Training}} of {{Deep Convolutional Neural Networks}}},
  author = {Istrate, Roxana and Malossi, Adelmo Cristiano Innocenza and Bekas, Costas and Nikolopoulos, Dimitrios},
  year = {2018},
  month = mar,
  abstract = {We propose an incremental training method that partitions the original network into sub-networks, which are then gradually incorporated in the running network during the training process. To allow for a smooth dynamic growth of the network, we introduce a look-ahead initialization that outperforms the random initialization. We demonstrate that our incremental approach reaches the reference network baseline accuracy. Additionally, it allows to identify smaller partitions of the original state-of-the-art network, that deliver the same final accuracy, by using only a fraction of the global number of parameters. This allows for a potential speedup of the training time of several factors. We report training results on CIFAR-10 for ResNet and VGGNet.},
  archivePrefix = {arXiv},
  eprint = {1803.10232},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GNQBIPV4\\Istrate et al. - 2018 - Incremental training of deep convolutional neural .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\KDCMNU7K\\Istrate et al. - 2018 - Incremental Training of Deep Convolutional Neural .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\KZZLZG7B\\1803.html;C\:\\Users\\ext1150\\Zotero\\storage\\XMLJUAI5\\Vol-1998.html},
  journal = {arXiv:1803.10232 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{izmailovAveragingWeightsLeads2019,
  title = {Averaging {{Weights Leads}} to {{Wider Optima}} and {{Better Generalization}}},
  author = {Izmailov, Pavel and Podoprikhin, Dmitrii and Garipov, Timur and Vetrov, Dmitry and Wilson, Andrew Gordon},
  year = {2019},
  month = feb,
  abstract = {Deep neural networks are typically trained by optimizing a loss function with an SGD variant, in conjunction with a decaying learning rate, until convergence. We show that simple averaging of multiple points along the trajectory of SGD, with a cyclical or constant learning rate, leads to better generalization than conventional training. We also show that this Stochastic Weight Averaging (SWA) procedure finds much flatter solutions than SGD, and approximates the recent Fast Geometric Ensembling (FGE) approach with a single model. Using SWA we achieve notable improvement in test accuracy over conventional SGD training on a range of state-of-the-art residual networks, PyramidNets, DenseNets, and ShakeShake networks on CIFAR-10, CIFAR-100, and ImageNet. In short, SWA is extremely easy to implement, improves generalization, and has almost no computational overhead.},
  archivePrefix = {arXiv},
  eprint = {1803.05407},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JR4U2LN9\\1803.05407.pdf},
  journal = {arXiv:1803.05407 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  language = {en},
  note = {Comment: Appears at the Conference on Uncertainty in Artificial Intelligence (UAI), 2018},
  primaryClass = {cs, stat}
}

@article{jaffeQUANTUMYANGMILLS,
  title = {{{QUANTUM YANG}}\textendash{{MILLS THEORY}}},
  author = {Jaffe, Arthur and Witten, Edward},
  pages = {14},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FZLZGQ6P\\Jaffe and Witten - QUANTUM YANG–MILLS THEORY.pdf},
  language = {en}
}

@article{jafferisSemiClassicalAnalysisString2020,
  title = {Semi-{{Classical Analysis}} of the {{String Theory Cigar}}},
  author = {Jafferis, Daniel Louis and Schneider, Elliot},
  year = {2020},
  month = apr,
  abstract = {We study the semi-classical limit of the reflection coefficient for the SL(2, R)k/U(1) CFT. For large k, the CFT describes a string in a Euclidean black hole of 2dimensional dilaton-gravity, whose target space is a cigar with an asymptotically linear dilaton. This sigma-model description is weakly coupled in the large k limit, and we investigate the saddle-point expansion of the functional integral that computes the reflection coefficient. As in the semi-classical limit of Liouville CFT studied in [1], we find that one must complexify the functional integral and sum over complex saddles to reproduce the limit of the exact reflection coefficient. Unlike Liouville, the SL(2, R)k/U(1) CFT admits bound states that manifest as poles of the reflection coefficient. To reproduce them in the semi-classical limit, we find that one must sum over configurations that hit the black hole singularity, but nevertheless contribute to the saddle-point expansion with finite action.},
  archivePrefix = {arXiv},
  eprint = {2004.05223},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NYZJLVCZ\\2004.05223.pdf},
  journal = {arXiv:2004.05223 [hep-th]},
  keywords = {High Energy Physics - Theory},
  language = {en},
  note = {Comment: 61 pages},
  primaryClass = {hep-th}
}

@article{jakubovitzGeneralizationErrorDeep2019,
  title = {Generalization {{Error}} in {{Deep Learning}}},
  author = {Jakubovitz, Daniel and Giryes, Raja and Rodrigues, Miguel R. D.},
  year = {2019},
  month = apr,
  abstract = {Deep learning models have lately shown great performance in various fields such as computer vision, speech recognition, speech translation, and natural language processing. However, alongside their state-of-the-art performance, it is still generally unclear what is the source of their generalization ability. Thus, an important question is what makes deep neural networks able to generalize well from the training set to new data. In this article, we provide an overview of the existing theory and bounds for the characterization of the generalization error of deep neural networks, combining both classical and more recent theoretical and empirical results.},
  archivePrefix = {arXiv},
  eprint = {1808.01174},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VBPXX9VF\\Jakubovitz et al. - 2019 - Generalization Error in Deep Learning.pdf},
  journal = {arXiv:1808.01174 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,generalization,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{jarrettWhatBestMultistage2009,
  title = {What Is the Best Multi-Stage Architecture for Object Recognition?},
  booktitle = {2009 {{IEEE}} 12th {{International Conference}} on {{Computer Vision}}},
  author = {Jarrett, K. and Kavukcuoglu, K. and Ranzato, M. and LeCun, Y.},
  year = {2009},
  month = sep,
  pages = {2146--2153},
  issn = {2380-7504},
  doi = {10.1109/ICCV.2009.5459469},
  abstract = {In many recent object recognition systems, feature extraction stages are generally composed of a filter bank, a non-linear transformation, and some sort of feature pooling layer. Most systems use only one stage of feature extraction in which the filters are hard-wired, or two stages where the filters in one or both stages are learned in supervised or unsupervised mode. This paper addresses three questions: 1. How does the non-linearities that follow the filter banks influence the recognition accuracy? 2. does learning the filter banks in an unsupervised or supervised manner improve the performance over random filters or hardwired filters? 3. Is there any advantage to using an architecture with two stages of feature extraction, rather than one? We show that using non-linearities that include rectification and local contrast normalization is the single most important ingredient for good accuracy on object recognition benchmarks. We show that two stages of feature extraction yield better accuracy than one. Most surprisingly, we show that a two-stage system with random filters can yield almost 63\% recognition rate on Caltech-101, provided that the proper non-linearities and pooling layers are used. Finally, we show that with supervised refinement, the system achieves state-of-the-art performance on NORB dataset (5.6\%) and unsupervised pre-training followed by supervised refinement produces good accuracy on Caltech-101 ({$>$} 65\%), and the lowest known error rate on the undistorted, unprocessed MNIST dataset (0.53\%).},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\V8HQIXDJ\\Jarrett et al. - 2009 - What is the best multi-stage architecture for obje.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\V63QYNTA\\5459469.html},
  keywords = {Brain modeling,Caltech-101,Error analysis,feature extraction,Feature extraction,feature pooling layer,feature rectification,filter bank,Filter bank,Gabor filters,Histograms,Image edge detection,Learning systems,local contrast normalization,multistage architecture,nonlinear transformation,NORB dataset,object recognition,Object recognition,Refining,supervised learning,unprocessed MNIST dataset,unsupervised learning}
}

@article{jastrzebskiThreeFactorsInfluencing2018,
  title = {Three {{Factors Influencing Minima}} in {{SGD}}},
  author = {Jastrz{\k{e}}bski, Stanis{\l}aw and Kenton, Zachary and Arpit, Devansh and Ballas, Nicolas and Fischer, Asja and Bengio, Yoshua and Storkey, Amos},
  year = {2018},
  month = sep,
  abstract = {We investigate the dynamical and convergent properties of stochastic gradient descent (SGD) applied to Deep Neural Networks (DNNs). Characterizing the relation between learning rate, batch size and the properties of the final minima, such as width or generalization, remains an open question. In order to tackle this problem we investigate the previously proposed approximation of SGD by a stochastic differential equation (SDE). We theoretically argue that three factors - learning rate, batch size and gradient covariance - influence the minima found by SGD. In particular we find that the ratio of learning rate to batch size is a key determinant of SGD dynamics and of the width of the final minima, and that higher values of the ratio lead to wider minima and often better generalization. We confirm these findings experimentally. Further, we include experiments which show that learning rate schedules can be replaced with batch size schedules and that the ratio of learning rate to batch size is an important factor influencing the memorization process.},
  archivePrefix = {arXiv},
  eprint = {1711.04623},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\I69GYYCE\\Jastrzębski et al. - 2018 - Three Factors Influencing Minima in SGD.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\B3XFHLSY\\1711.html},
  journal = {arXiv:1711.04623 [cs, stat]},
  keywords = {batch_size,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,lr,read,Statistics - Machine Learning},
  note = {Comment: First two authors contributed equally. Short version accepted into ICLR workshop. Accepted to Artificial Neural Networks and Machine Learning, ICANN 2018},
  primaryClass = {cs, stat}
}

@article{jiaCaffeConvolutionalArchitecture2014,
  title = {Caffe: {{Convolutional Architecture}} for {{Fast Feature Embedding}}},
  shorttitle = {Caffe},
  author = {Jia, Yangqing and Shelhamer, Evan and Donahue, Jeff and Karayev, Sergey and Long, Jonathan and Girshick, Ross and Guadarrama, Sergio and Darrell, Trevor},
  year = {2014},
  month = jun,
  abstract = {Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (\$\textbackslash approx\$ 2.5 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia.},
  archivePrefix = {arXiv},
  eprint = {1408.5093},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PA7RTB3J\\Jia et al. - 2014 - Caffe Convolutional Architecture for Fast Feature.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\NFU62WG9\\1408.html},
  journal = {arXiv:1408.5093 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: Tech report for the Caffe software at http://github.com/BVLC/Caffe/},
  primaryClass = {cs}
}

@misc{johannessonDetectionBoneFractures,
  title = {Detection of Bone Fractures in Chicken Legs.Pdf},
  author = {Johannesson, Rasmus Skov},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\5B3NLDWI\\projekt_2018.pdf}
}

@misc{johannessonGeneratingRealisticArtificial,
  title = {Generating Realistic Artificial Xray Images of Highly Homogeneous Food Items},
  author = {Johannesson, Rasmus Skov},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CI965BBG\\Rasmus Img Gen.pdf}
}

@article{jonesHowWriteGreat,
  title = {How to Write a Great Research Paper {{Seven}} Simple Suggestions},
  author = {Jones, Simon Peyton},
  pages = {52},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\98NNG8LS\\Jones - How to write a great research paper Seven simple s.pdf},
  keywords = {read},
  language = {en}
}

@misc{kabirEnemy2020,
  title = {N {{Is The Enemy}}},
  author = {Kabir, Naim},
  year = {2020},
  month = jul,
  abstract = {Big Population + Big Data = Critical Failure},
  howpublished = {https://towardsdatascience.com/n-is-the-enemy-c72cc1ba683b},
  journal = {Medium},
  language = {en}
}

@article{katharopoulosTransformersAreRNNs2020,
  title = {Transformers Are {{RNNs}}: {{Fast Autoregressive Transformers}} with {{Linear Attention}}},
  shorttitle = {Transformers Are {{RNNs}}},
  author = {Katharopoulos, Angelos and Vyas, Apoorv and Pappas, Nikolaos and Fleuret, Fran{\c c}ois},
  year = {2020},
  month = aug,
  abstract = {Transformers achieve remarkable performance in several tasks but due to their quadratic complexity, with respect to the input's length, they are prohibitively slow for very long sequences. To address this limitation, we express the self-attention as a linear dot-product of kernel feature maps and make use of the associativity property of matrix products to reduce the complexity from \$\textbackslash mathcal\{O\}\textbackslash left(N\^2\textbackslash right)\$ to \$\textbackslash mathcal\{O\}\textbackslash left(N\textbackslash right)\$, where \$N\$ is the sequence length. We show that this formulation permits an iterative implementation that dramatically accelerates autoregressive transformers and reveals their relationship to recurrent neural networks. Our linear transformers achieve similar performance to vanilla transformers and they are up to 4000x faster on autoregressive prediction of very long sequences.},
  archivePrefix = {arXiv},
  eprint = {2006.16236},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6H23UAY9\\Katharopoulos et al. - 2020 - Transformers are RNNs Fast Autoregressive Transfo.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\N4IGE5DA\\2006.html},
  journal = {arXiv:2006.16236 [cs, stat]},
  keywords = {attention,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: ICML 2020, project at https://linear-transformers.com/},
  primaryClass = {cs, stat}
}

@article{kendallBayesianSegNetModel2016,
  title = {Bayesian {{SegNet}}: {{Model Uncertainty}} in {{Deep Convolutional Encoder}}-{{Decoder Architectures}} for {{Scene Understanding}}},
  shorttitle = {Bayesian {{SegNet}}},
  author = {Kendall, Alex and Badrinarayanan, Vijay and Cipolla, Roberto},
  year = {2016},
  month = oct,
  abstract = {We present a deep learning framework for probabilistic pixel-wise semantic segmentation, which we term Bayesian SegNet. Semantic segmentation is an important tool for visual scene understanding and a meaningful measure of uncertainty is essential for decision making. Our contribution is a practical system which is able to predict pixelwise class labels with a measure of model uncertainty. We achieve this by Monte Carlo sampling with dropout at test time to generate a posterior distribution of pixel class labels. In addition, we show that modelling uncertainty improves segmentation performance by 2-3\% across a number of state of the art architectures such as SegNet, FCN and Dilation Network, with no additional parametrisation. We also observe a significant improvement in performance for smaller datasets where modelling uncertainty is more effective. We benchmark Bayesian SegNet on the indoor SUN Scene Understanding and outdoor CamVid driving scenes datasets.},
  archivePrefix = {arXiv},
  eprint = {1511.02680},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\DXTG8FGF\\Kendall et al. - 2016 - Bayesian SegNet Model Uncertainty in Deep Convolu.pdf},
  journal = {arXiv:1511.02680 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{kendallWhatUncertaintiesWe2017,
  title = {What {{Uncertainties Do We Need}} in {{Bayesian Deep Learning}} for {{Computer Vision}}?},
  author = {Kendall, Alex and Gal, Yarin},
  year = {2017},
  month = oct,
  abstract = {There are two major types of uncertainty one can model. Aleatoric uncertainty captures noise inherent in the observations. On the other hand, epistemic uncertainty accounts for uncertainty in the model -- uncertainty which can be explained away given enough data. Traditionally it has been difficult to model epistemic uncertainty in computer vision, but with new Bayesian deep learning tools this is now possible. We study the benefits of modeling epistemic vs. aleatoric uncertainty in Bayesian deep learning models for vision tasks. For this we present a Bayesian deep learning framework combining input-dependent aleatoric uncertainty together with epistemic uncertainty. We study models under the framework with per-pixel semantic segmentation and depth regression tasks. Further, our explicit uncertainty formulation leads to new loss functions for these tasks, which can be interpreted as learned attenuation. This makes the loss more robust to noisy data, also giving new state-of-the-art results on segmentation and depth regression benchmarks.},
  archivePrefix = {arXiv},
  eprint = {1703.04977},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CYDYRCL8\\Kendall and Gal - 2017 - What Uncertainties Do We Need in Bayesian Deep Lea.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\2KRKTLTJ\\1703.html},
  journal = {arXiv:1703.04977 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-someday},
  note = {Comment: NIPS 2017},
  primaryClass = {cs}
}

@misc{keramitasScalingTransformersReform2020,
  title = {Scaling {{Transformers}}: {{Reform}} Your Ways},
  shorttitle = {Scaling {{Transformers}}},
  author = {Keramitas, Romain},
  year = {2020},
  month = nov,
  abstract = {After showing you my Python setup in the previous post, I wanted to showcase it in a second post with a project. However it got out of hand: I chose to write a post on scaling Transformers, implemented ideas from two NLP papers, and \ldots{} ended up with a super long post 😅 Since the topic is close to my heart, I decided to start a series of articles on it, focusing on one paper at a time. In all cases, let's dive in !},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\DGWEVF5T\\scaling-transformers-reform-your-ways.html},
  howpublished = {http://keramitas.io/2020/11/24/scaling-transformers-reform-your-ways.html},
  journal = {keramitas},
  language = {en}
}

@article{khoslaSupervisedContrastiveLearning2020,
  title = {Supervised {{Contrastive Learning}}},
  author = {Khosla, Prannay and Teterwak, Piotr and Wang, Chen and Sarna, Aaron and Tian, Yonglong and Isola, Phillip and Maschinot, Aaron and Liu, Ce and Krishnan, Dilip},
  year = {2020},
  month = apr,
  abstract = {Cross entropy is the most widely used loss function for supervised training of image classification models. In this paper, we propose a novel training methodology that consistently outperforms cross entropy on supervised learning tasks across different architectures and data augmentations. We modify the batch contrastive loss, which has recently been shown to be very effective at learning powerful representations in the self-supervised setting. We are thus able to leverage label information more effectively than cross entropy. Clusters of points belonging to the same class are pulled together in embedding space, while simultaneously pushing apart clusters of samples from different classes. In addition to this, we leverage key ingredients such as large batch sizes and normalized embeddings, which have been shown to benefit self-supervised learning. On both ResNet-50 and ResNet-200, we outperform cross entropy by over 1\%, setting a new state of the art number of 78.8\% among methods that use AutoAugment data augmentation. The loss also shows clear benefits for robustness to natural corruptions on standard benchmarks on both calibration and accuracy. Compared to cross entropy, our supervised contrastive loss is more stable to hyperparameter settings such as optimizers or data augmentations.},
  archivePrefix = {arXiv},
  eprint = {2004.11362},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\T87LAB9I\\Khosla et al. - 2020 - Supervised Contrastive Learning.pdf},
  journal = {arXiv:2004.11362 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{kidgerNeuralControlledDifferential2020,
  ids = {kidgerNeuralControlledDifferential2020a},
  title = {Neural {{Controlled Differential Equations}} for {{Irregular Time Series}}},
  author = {Kidger, Patrick and Morrill, James and Foster, James and Lyons, Terry},
  year = {2020},
  month = may,
  abstract = {Neural ordinary differential equations are an attractive option for modelling temporal dynamics. However, a fundamental issue is that the solution to an ordinary differential equation is determined by its initial condition, and there is no mechanism for adjusting the trajectory based on subsequent observations. Here, we demonstrate how this may be resolved through the well-understood mathematics of \textbackslash emph\{controlled differential equations\}. The resulting \textbackslash emph\{neural controlled differential equation\} model is directly applicable to the general setting of partially-observed irregularly-sampled multivariate time series, and (unlike previous work on this problem) it may utilise memory-efficient adjoint-based backpropagation even across observations. We demonstrate that our model achieves state-of-the-art performance against similar (ODE or RNN based) models in empirical studies on a range of datasets. Finally we provide theoretical results demonstrating universal approximation, and that our model subsumes alternative ODE models.},
  archivePrefix = {arXiv},
  eprint = {2005.08926},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4IFXHA6Y\\Kidger et al. - 2020 - Neural Controlled Differential Equations for Irreg.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\KS2UBVXZ\\2005.08926v1.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\ADV23Z4F\\2005.html},
  journal = {arXiv:2005.08926 [cs, stat]},
  keywords = {Computer Science - Machine Learning,NODE,Statistics - Machine Learning},
  note = {Comment: 25 pages
\par
Comment: 25 pages},
  primaryClass = {cs, stat}
}

@article{kieferStochasticEstimationMaximum1952,
  title = {Stochastic {{Estimation}} of the {{Maximum}} of a {{Regression Function}}},
  author = {Kiefer, J. and Wolfowitz, J.},
  year = {1952},
  month = sep,
  volume = {23},
  pages = {462--466},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729392},
  abstract = {Let M(x)M(x)M(x) be a regression function which has a maximum at the unknown point \texttheta.M(x)\texttheta.M(x)\textbackslash theta. M(x) is itself unknown to the statistician who, however, can take observations at any level xxx. This paper gives a scheme whereby, starting from an arbitrary point x1x1x\_1, one obtains successively x2,x3,{$\cdots$}x2,x3,{$\cdots$}x\_2, x\_3, \textbackslash cdots such that xnxnx\_n converges to \texttheta\texttheta\textbackslash theta in probability as n\textrightarrow{$\infty$}n\textrightarrow{$\infty$}n \textbackslash rightarrow \textbackslash infty.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\5ICBEGCG\\Kiefer and Wolfowitz - 1952 - Stochastic Estimation of the Maximum of a Regressi.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\XD8XGL6G\\1177729392.html},
  journal = {Annals of Mathematical Statistics},
  keywords = {lr},
  language = {EN},
  mrnumber = {MR50243},
  number = {3},
  zmnumber = {0049.36601}
}

@article{kingmaAdamMethodStochastic2017,
  title = {Adam: {{A Method}} for {{Stochastic Optimization}}},
  shorttitle = {Adam},
  author = {Kingma, Diederik P. and Ba, Jimmy},
  year = {2017},
  month = jan,
  abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
  archivePrefix = {arXiv},
  eprint = {1412.6980},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NDPP4L9T\\Kingma and Ba - 2017 - Adam A Method for Stochastic Optimization.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\7YEIVX4V\\1412.html},
  journal = {arXiv:1412.6980 [cs]},
  keywords = {adam,Computer Science - Machine Learning,lr,read},
  note = {Comment: Published as a conference paper at the 3rd International Conference for Learning Representations, San Diego, 2015},
  primaryClass = {cs}
}

@article{kingmaAutoEncodingVariationalBayes2013,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2013},
  month = dec,
  abstract = {Can we efficiently learn the parameters of directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions? We introduce an unsupervised...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JBZF9BSL\\forum.html},
  language = {en}
}

@article{kingmaAutoEncodingVariationalBayes2014,
  title = {Auto-{{Encoding Variational Bayes}}},
  author = {Kingma, Diederik P. and Welling, Max},
  year = {2014},
  month = may,
  abstract = {How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results.},
  archivePrefix = {arXiv},
  eprint = {1312.6114},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GY627ZN5\\Kingma and Welling - 2014 - Auto-Encoding Variational Bayes.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\DNBS9HBY\\1312.html},
  journal = {arXiv:1312.6114 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{kitaevReformerEfficientTransformer2020,
  title = {Reformer: {{The Efficient Transformer}}},
  shorttitle = {Reformer},
  author = {Kitaev, Nikita and Kaiser, {\L}ukasz and Levskaya, Anselm},
  year = {2020},
  month = feb,
  abstract = {Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O(L2) to O(L log L), where L is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of N times, where N is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences.},
  archivePrefix = {arXiv},
  eprint = {2001.04451},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\8GRPIFLQ\\Kitaev et al. - 2020 - Reformer The Efficient Transformer.pdf},
  journal = {arXiv:2001.04451 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  note = {Comment: ICLR 2020},
  primaryClass = {cs, stat}
}

@article{klambauerSelfNormalizingNeuralNetworks2017,
  title = {Self-{{Normalizing Neural Networks}}},
  author = {Klambauer, G{\"u}nter and Unterthiner, Thomas and Mayr, Andreas and Hochreiter, Sepp},
  year = {2017},
  month = sep,
  abstract = {Deep Learning has revolutionized vision via convolutional neural networks (CNNs) and natural language processing via recurrent neural networks (RNNs). However, success stories of Deep Learning with standard feed-forward neural networks (FNNs) are rare. FNNs that perform well are typically shallow and, therefore cannot exploit many levels of abstract representations. We introduce self-normalizing neural networks (SNNs) to enable high-level abstract representations. While batch normalization requires explicit normalization, neuron activations of SNNs automatically converge towards zero mean and unit variance. The activation function of SNNs are ``scaled exponential linear units'' (SELUs), which induce self-normalizing properties. Using the Banach fixed-point theorem, we prove that activations close to zero mean and unit variance that are propagated through many network layers will converge towards zero mean and unit variance \textemdash{} even under the presence of noise and perturbations. This convergence property of SNNs allows to (1) train deep networks with many layers, (2) employ strong regularization schemes, and (3) to make learning highly robust. Furthermore, for activations not close to unit variance, we prove an upper and lower bound on the variance, thus, vanishing and exploding gradients are impossible. We compared SNNs on (a) 121 tasks from the UCI machine learning repository, on (b) drug discovery benchmarks, and on (c) astronomy tasks with standard FNNs, and other machine learning methods such as random forests and support vector machines. For FNNs we considered (i) ReLU networks without normalization, (ii) batch normalization, (iii) layer normalization, (iv) weight normalization, (v) highway networks, and (vi) residual networks. SNNs significantly outperformed all competing FNN methods at 121 UCI tasks, outperformed all competing methods at the Tox21 dataset, and set a new record at an astronomy data set. The winning SNN architectures are often very deep. Implementations are available at: github.com/bioinf-jku/SNNs.},
  archivePrefix = {arXiv},
  eprint = {1706.02515},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\96MRDV7N\\Klambauer et al. - 2017 - Self-Normalizing Neural Networks.pdf},
  journal = {arXiv:1706.02515 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  language = {en},
  note = {Comment: 9 pages (+ 93 pages appendix)},
  primaryClass = {cs, stat}
}

@book{knollRadiationDetectionMeasurement2000,
  title = {Radiation Detection and Measurement},
  author = {Knoll, Glenn F.},
  year = {2000},
  edition = {3rd ed},
  publisher = {{Wiley}},
  address = {{New York}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\H7GGEPJT\\Knoll - 2000 - Radiation detection and measurement.pdf},
  isbn = {978-0-471-07338-3},
  keywords = {Measurement,Nuclear counters,Radiation},
  language = {en},
  lccn = {QC787.C6 K56 2000}
}

@misc{kochOnlineInspectionXray2017,
  title = {Online {{Inspection}} of {{X}}-Ray {{Images}}},
  author = {Koch, Thorbj{\o}rn Louring},
  year = {2017},
  month = nov,
  publisher = {{KU master thesis}},
  abstract = {This thesis develop the use of contemporary image analysis methods within online classi cation of X-ray absorption images. The fundamental interactions underlying X-rays interacting with matter is presented. The focus is placed on absorption spectroscopy as the contrast mechanism for online classi cation of foods using Xrays. X-ray tube sources and their associated spectra is discussed. Direct and indirect methods for X-ray detection are brie y introduced. The Cadmium Telluride detector is presented for its ability to detect X-rays within the 20-100keV region. Multi-bin detectors are discussed for their applications within dual energy imaging. An alternative technique for separating materials with similar total absorption is discussed. The new method is based on the Kullback-Leibler divergence from information theory. Dual energy imaging is tested theoretically using a starch/water model for potatoes. It is shown that the Laplace transform performed when detecting X-rays limit how small changes are detectable in the chemical composition using dual energy imaging. Afterwards the construction of a model setup for online X-ray absorption imaging is discussed. The setup is used to record a dataset of X-ray images potatoes with imitated defects. The types of defects hollow heart disease and needles. These are chosen due to how di cult they are to detect for other non-destructive testing methods. A similar dataset is also recorded using a Cadmium Telluride detector. The main part of this thesis is automatic classi cation of these two datasets using techniques within machine learning and computer vision. The primary focus is put on convolutional neural networks and discriminative dictionary learning. A thresholding technique based on integral images are also tested. Other thresholding techniques are also discussed, but the one working with integral images is chosen due to how fast it is compared with conventional methods. The integral image is used to design a smoothing algorithm similar to morphological operations, just without the strict structure required in morphology. Deep learning with images is introduced from the basics until the necessary knowledge required to understand convolutional neural networks have been presented. After that, a convolutional neural network is constructed with a similar structure to AlexNet, and trained using the recorded X-ray images. Discriminative dictionary learning for classi cation is introduced using the MNIST dataset of 70000 handwritten digits. Training with batched data and unsupervised subclasses is discussed. Subsequently, an implementation for classi cation of the X-ray images is discussed. Lastly, an automatic detection and classi cation scheme for  nding unwanted particles in beers is constructed. The particles, while not harmful, decrease the consumer satisfaction. The constructed method is shown to agree with the manual method applied by Carlsberg. During this part, a novel method for binning data with Gaussian or Poisson error in unequal bins is designed. Geometrical e ects from observing a 2D projection of 3D data are also discussed.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YX55XPFF\\TLK_thesis.pdf},
  keywords = {dual-energy}
}

@article{kolesnikovBigTransferBiT2020,
  title = {Big {{Transfer}} ({{BiT}}): {{General Visual Representation Learning}}},
  shorttitle = {Big {{Transfer}} ({{BiT}})},
  author = {Kolesnikov, Alexander and Beyer, Lucas and Zhai, Xiaohua and Puigcerver, Joan and Yung, Jessica and Gelly, Sylvain and Houlsby, Neil},
  year = {2020},
  month = may,
  abstract = {Transfer of pre-trained representations improves sample efficiency and simplifies hyperparameter tuning when training deep neural networks for vision. We revisit the paradigm of pre-training on large supervised datasets and fine-tuning the model on a target task. We scale up pre-training, and propose a simple recipe that we call Big Transfer (BiT). By combining a few carefully selected components, and transferring using a simple heuristic, we achieve strong performance on over 20 datasets. BiT performs well across a surprisingly wide range of data regimes \textemdash{} from 1 example per class to 1 M total examples. BiT achieves 87.5\% top-1 accuracy on ILSVRC-2012, 99.4\% on CIFAR-10, and 76.3\% on the 19 task Visual Task Adaptation Benchmark (VTAB). On small datasets, BiT attains 76.8\% on ILSVRC-2012 with 10 examples per class, and 97.0\% on CIFAR-10 with 10 examples per class. We conduct detailed analysis of the main components that lead to high transfer performance.},
  archivePrefix = {arXiv},
  eprint = {1912.11370},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\D2A8MZBX\\Kolesnikov et al. - 2020 - Big Transfer (BiT) General Visual Representation .pdf},
  journal = {arXiv:1912.11370 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read},
  language = {en},
  note = {Comment: The first three authors contributed equally. Results on ObjectNet are reported in v3},
  primaryClass = {cs}
}

@misc{koller12FactorsReproducible2020,
  title = {12 {{Factors}} of Reproducible {{Machine Learning}} in Production},
  author = {Koller, Benedikt},
  year = {2020},
  month = sep,
  abstract = {The last two decades have yielded us some great understandings about Software Development. A big part of that is due to the emergence of DevOps and it's wide adoption throughout the industry.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\IMJX9BB9\\12-factors-of-ml-in-production.html},
  howpublished = {https://blog.maiot.io/12-factors-of-ml-in-production/},
  journal = {maiot Blog},
  language = {en}
}

@inproceedings{kornblithBetterImageNetModels2019,
  title = {Do Better {{ImageNet}} Models Transfer Better?},
  author = {Kornblith, Simon and Shlens, Jon and Le, Quoc V.},
  year = {2019},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6UC8J6WC\\Kornblith et al. - 2019 - Do better ImageNet models transfer better.pdf}
}

@article{koselleckPRACTICECONCEPTUALHISTORY,
  title = {{{THE PRACTICE OF CONCEPTUAL HISTORY}}},
  author = {Koselleck, Reinhart},
  pages = {380},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NC35TZGX\\Koselleck - THE PRACTICE OF CONCEPTUAL HISTORY.pdf},
  language = {en}
}

@article{kosiorekStackedCapsuleAutoencoders,
  title = {Stacked {{Capsule Autoencoders}}},
  author = {Kosiorek, Adam and Sabour, Sara and Teh, Yee Whye and Hinton, Geoffrey E},
  pages = {11},
  abstract = {Objects are composed of a set of geometrically organized parts. We introduce an unsupervised capsule autoencoder (SCAE), which explicitly uses geometric relationships between parts to reason about objects. Since these relationships do not depend on the viewpoint, our model is robust to viewpoint changes. SCAE consists of two stages. In the first stage, the model predicts presences and poses of part templates directly from the image and tries to reconstruct the image by appropriately arranging the templates. In the second stage, SCAE predicts parameters of a few object capsules, which are then used to reconstruct part poses. Inference in this model is amortized and performed by off-the-shelf neural encoders, unlike in previous capsule networks. We find that object capsule presences are highly informative of the object class, which leads to state-of-the-art results for unsupervised classification on SVHN (55\%) and MNIST (98.7\%).},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TCZFD2N5\\Kosiorek et al. - Stacked Capsule Autoencoders.pdf},
  language = {en}
}

@article{kotwaliwaleXrayImagingMethods2014,
  title = {X-Ray Imaging Methods for Internal Quality Evaluation of Agricultural Produce},
  author = {Kotwaliwale, Nachiket and Singh, Karan and Kalne, Abhimannyu and Jha, Shyam Narayan and Seth, Neeraj and Kar, Abhijit},
  year = {2014},
  month = jan,
  volume = {51},
  pages = {1--15},
  issn = {0022-1155},
  doi = {10.1007/s13197-011-0485-y},
  abstract = {A number of non-destructive methods for internal quality evaluation have been studied by different researchers over the past eight decades. X-ray and computed tomography imaging techniques are few of them which are gaining popularity now days in various fields of agriculture and food quality evaluation. These techniques, so far predominantly used in medical applications, have also been explored for internal quality inspection of various agricultural products non-destructively, when quality features are not visible on the surface of the products. Though, safety of operators and time required for tests are of concern, the non-destructive nature of these techniques has great potential for wide applications on agricultural produce. This paper presents insight of X-ray based non-destructive techniques such as X-ray imaging and Computed Tomography (CT). The concepts, properties, equipment and their parameters, systems and applications associated with the use of X-rays and CT for agricultural produce have been elaborated.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\69ZUFRK5\\Kotwaliwale et al. - 2014 - X-ray imaging methods for internal quality evaluat.pdf},
  journal = {Journal of Food Science and Technology},
  keywords = {Agricultural produce,Computed tomography,Digital radiography,Internal quality evaluation,introduction,Non-destructive quality inspection,X-ray imaging},
  language = {eng},
  number = {1},
  pmcid = {PMC3857400},
  pmid = {24426042}
}

@incollection{krizhevskyImageNetClassificationDeep2012,
  title = {{{ImageNet Classification}} with {{Deep Convolutional Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 25},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
  editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
  year = {2012},
  pages = {1097--1105},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\9QG4SFZ4\\4824-imagenet-classification-with-deep-convolutional-neural-networ.html}
}

@article{krizhevskyImageNetClassificationDeep2017,
  title = {{{ImageNet}} Classification with Deep Convolutional Neural Networks},
  author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E.},
  year = {2017},
  month = may,
  volume = {60},
  pages = {84--90},
  issn = {00010782},
  doi = {10.1145/3065386},
  abstract = {We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5\% and 17.0\% which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully-connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully-connected layers we employed a recently-developed regularization method called ``dropout'' that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3\%, compared to 26.2\% achieved by the second-best entry.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PKXDMXMH\\Krizhevsky et al. - 2017 - ImageNet classification with deep convolutional ne.pdf},
  journal = {Communications of the ACM},
  language = {en},
  number = {6}
}

@article{krizhevskyOneWeirdTrick2014,
  title = {One Weird Trick for Parallelizing Convolutional Neural Networks},
  author = {Krizhevsky, Alex},
  year = {2014},
  month = apr,
  abstract = {I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks.},
  archivePrefix = {arXiv},
  eprint = {1404.5997},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NJYJDJCB\\Krizhevsky - 2014 - One weird trick for parallelizing convolutional ne.pdf},
  journal = {arXiv:1404.5997 [cs]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  primaryClass = {cs}
}

@article{kullbackInformationSufficiency1951,
  title = {On {{Information}} and {{Sufficiency}}},
  author = {Kullback, S. and Leibler, R. A.},
  year = {1951},
  month = mar,
  volume = {22},
  pages = {79--86},
  publisher = {{Institute of Mathematical Statistics}},
  issn = {0003-4851, 2168-8990},
  doi = {10.1214/aoms/1177729694},
  abstract = {Project Euclid - mathematics and statistics online},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\BD3DAW7X\\Kullback and Leibler - 1951 - On Information and Sufficiency.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\J66K2HTV\\1177729694.html},
  journal = {Annals of Mathematical Statistics},
  language = {EN},
  mrnumber = {MR39968},
  number = {1},
  zmnumber = {0042.38403}
}

@article{lassanceLaplacianNetworksBounding2018,
  title = {Laplacian {{Networks}}: {{Bounding Indicator Function Smoothness}} for {{Neural Network Robustness}}},
  shorttitle = {Laplacian {{Networks}}},
  author = {Lassance, Carlos Eduardo Rosar Kos and Gripon, Vincent and Ortega, Antonio},
  year = {2018},
  month = oct,
  abstract = {For the past few years, Deep Neural Network (DNN) robustness has become a question of paramount importance. As a matter of fact, in sensitive settings misclassification can lead to dramatic consequences. Such misclassifications are likely to occur when facing adversarial attacks, hardware failures or limitations, and imperfect signal acquisition. To address this question, authors have proposed different approaches aiming at increasing the robustness of DNNs, such as adding regularizers or training using noisy examples. In this paper we propose a new regularizer built upon the Laplacian of similarity graphs obtained from the representation of training data at each layer of the DNN architecture. This regularizer penalizes large changes (across consecutive layers in the architecture) in the distance between examples of different classes, and as such enforces smooth variations of the class boundaries. Since it is agnostic to the type of deformations that are expected when predicting with the DNN, the proposed regularizer can be combined with existing ad-hoc methods. We provide theoretical justification for this regularizer and demonstrate its effectiveness to improve robustness of DNNs on classical supervised learning vision datasets.},
  archivePrefix = {arXiv},
  eprint = {1805.10133},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\K2P3PM4R\\Lassance et al. - 2018 - Laplacian Networks Bounding Indicator Function Sm.pdf},
  journal = {arXiv:1805.10133 [cs, stat]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  note = {Comment: 12 pages, github link coming soon},
  primaryClass = {cs, stat}
}

@misc{LectureAcceleratedCharges,
  title = {Lecture 3 : {{Accelerated}} Charges and Bremsstrahlung},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\8RSFF4RL\\l3.html},
  howpublished = {http://www.astro.utu.fi/\textasciitilde cflynn/astroII/l3.html}
}

@article{lecunBackpropagationAppliedHandwritten1989,
  title = {Backpropagation {{Applied}} to {{Handwritten Zip Code Recognition}}},
  author = {LeCun, Y. and Boser, B. and Denker, J. S. and Henderson, D. and Howard, R. E. and Hubbard, W. and Jackel, L. D.},
  year = {1989},
  month = dec,
  volume = {1},
  pages = {541--551},
  publisher = {{MIT Press}},
  issn = {0899-7667},
  doi = {10.1162/neco.1989.1.4.541},
  abstract = {The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6PRAMAJU\\LeCun et al. - 1989 - Backpropagation Applied to Handwritten Zip Code Re.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\J9ZC6QT3\\neco.1989.1.4.html},
  journal = {Neural Computation},
  keywords = {read,sliding-window},
  number = {4}
}

@article{lecunDeepLearning2015,
  title = {Deep Learning},
  author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
  year = {2015},
  month = may,
  volume = {521},
  pages = {436--444},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/nature14539},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZRNTTYWG\\LeCun et al. - 2015 - Deep learning.pdf},
  journal = {Nature},
  language = {en},
  number = {7553}
}

@incollection{lecunEfficientBackProp2012,
  title = {Efficient {{BackProp}}},
  booktitle = {Neural {{Networks}}: {{Tricks}} of the {{Trade}}: {{Second Edition}}},
  author = {LeCun, Yann A. and Bottou, L{\'e}on and Orr, Genevieve B. and M{\"u}ller, Klaus-Robert},
  editor = {Montavon, Gr{\'e}goire and Orr, Genevi{\`e}ve B. and M{\"u}ller, Klaus-Robert},
  year = {2012},
  pages = {9--48},
  publisher = {{Springer}},
  address = {{Berlin, Heidelberg}},
  doi = {10.1007/978-3-642-35289-8_3},
  abstract = {The convergence of back-propagation learning is analyzed so as to explain common phenomenon observed by practitioners. Many undesirable behaviors of backprop can be avoided with tricks that are rarely exposed in serious technical publications. This paper gives some of those tricks, and offers explanations of why they work.Many authors have suggested that second-order optimization methods are advantageous for neural net training. It is shown that most ``classical'' second-order methods are impractical for large neural networks. A few methods are proposed that do not have these limitations.},
  isbn = {978-3-642-35289-8},
  keywords = {Conjugate Gradient,Gradient Descent,Handwritten Digit,Neural Information Processing System,Newton Algorithm,normalization,read-next},
  language = {en},
  series = {Lecture {{Notes}} in {{Computer Science}}}
}

@article{leeGradientsMeasureUncertainty2020,
  title = {Gradients as a {{Measure}} of {{Uncertainty}} in {{Neural Networks}}},
  author = {Lee, Jinsol and AlRegib, Ghassan},
  year = {2020},
  month = aug,
  abstract = {Despite tremendous success of modern neural networks, they are known to be overconfident even when the model encounters inputs with unfamiliar conditions. Detecting such inputs is vital to preventing models from making naive predictions that may jeopardize real-world applications of neural networks. In this paper, we address the challenging problem of devising a simple yet effective measure of uncertainty in deep neural networks. Specifically, we propose to utilize backpropagated gradients to quantify the uncertainty of trained models. Gradients depict the required amount of change for a model to properly represent given inputs, thus providing a valuable insight into how familiar and certain the model is regarding the inputs. We demonstrate the effectiveness of gradients as a measure of model uncertainty in applications of detecting unfamiliar inputs, including out-of-distribution and corrupted samples. We show that our gradient-based method outperforms state-of-the-art methods by up to 4.8\% of AUROC score in out-of-distribution detection and 35.7\% in corrupted input detection.},
  archivePrefix = {arXiv},
  eprint = {2008.08030},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7L8HQMAG\\Lee and AlRegib - 2020 - Gradients as a Measure of Uncertainty in Neural Ne.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\KJDSPYYH\\2008.html},
  journal = {arXiv:2008.08030 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-next},
  note = {Comment: IEEE International Conference on Image Processing (ICIP) 2020},
  primaryClass = {cs}
}

@article{leeSimpleUnifiedFramework2018,
  title = {A {{Simple Unified Framework}} for {{Detecting Out}}-of-{{Distribution Samples}} and {{Adversarial Attacks}}},
  author = {Lee, Kimin and Lee, Kibok and Lee, Honglak and Shin, Jinwoo},
  year = {2018},
  month = oct,
  abstract = {Detecting test samples drawn sufficiently far away from the training distribution statistically or adversarially is a fundamental requirement for deploying a good classifier in many real-world machine learning applications. However, deep neural networks with the softmax classifier are known to produce highly overconfident posterior distributions even for such abnormal samples. In this paper, we propose a simple yet effective method for detecting any abnormal samples, which is applicable to any pre-trained softmax neural classifier. We obtain the class conditional Gaussian distributions with respect to (low- and upper-level) features of the deep models under Gaussian discriminant analysis, which result in a confidence score based on the Mahalanobis distance. While most prior methods have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed method achieves the state-of-the-art performances for both cases in our experiments. Moreover, we found that our proposed method is more robust in harsh cases, e.g., when the training dataset has noisy labels or small number of samples. Finally, we show that the proposed method enjoys broader usage by applying it to class-incremental learning: whenever out-of-distribution samples are detected, our classification rule can incorporate new classes well without further training deep models.},
  archivePrefix = {arXiv},
  eprint = {1807.03888},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HFCF3HCD\\Lee et al. - 2018 - A Simple Unified Framework for Detecting Out-of-Di.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\325DZZBB\\1807.html},
  journal = {arXiv:1807.03888 [cs, stat]},
  keywords = {Computer Science - Cryptography and Security,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Accepted in NIPS 2018},
  primaryClass = {cs, stat}
}

@article{liAttentiveNormalization2019,
  title = {Attentive {{Normalization}}},
  author = {Li, Xilai and Sun, Wei and Wu, Tianfu},
  year = {2019},
  month = nov,
  abstract = {In state-of-the-art deep neural networks, both feature normalization and feature attention have become ubiquitous with significant performance improvement shown in a vast amount of tasks. They are usually studied as separate modules, however. In this paper, we propose a light-weight integration between, and thus harness the best of, the two schema. We present Attentive Normalization (AN) which generalizes the common affine transformation component in the vanilla feature normalization. Instead of learning a single affine transformation, AN learns a mixture of affine transformations and utilizes their weighted-sum as the final affine transformation applied to re-calibrate features in an instance-specific way. The weights are learned by leveraging feature attention. AN introduces negligible extra parameters and computational cost (i.e., light-weight). AN can be used as a drop-in replacement for any feature normalization technique which includes the affine transformation component. In experiments, we test the proposed AN using three representative neural architectures (ResNets, MobileNets-v2 and AOGNets) in the ImageNet-1000 classification benchmark and the MS-COCO 2107 object detection and instance segmentation benchmark. AN obtains consistent performance improvement for different neural architectures in both benchmarks with absolute increase of top-1 accuracy in ImageNet-1000 between 0.5\% and 2.0\%, and absolute increase up to 1.8\% and 2.2\% for bounding box and mask AP in MS-COCO respectively. The source codes are publicly available(Classification in ImageNet: \textbackslash url\{https://github.com/iVMCL/AOGNets-v2\} and Detection in MS-COCO: \textbackslash url\{https://github.com/iVMCL/AttentiveNorm\textbackslash\_Detection\}\vphantom\{\}).},
  archivePrefix = {arXiv},
  eprint = {1908.01259},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\KXW7EMCL\\Li et al. - 2019 - Attentive Normalization.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\TD4BGSE3\\1908.html},
  journal = {arXiv:1908.01259 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: Preprint},
  primaryClass = {cs}
}

@article{liawTuneResearchPlatform2018,
  title = {Tune: {{A Research Platform}} for {{Distributed Model Selection}} and {{Training}}},
  shorttitle = {Tune},
  author = {Liaw, Richard and Liang, Eric and Nishihara, Robert and Moritz, Philipp and Gonzalez, Joseph E. and Stoica, Ion},
  year = {2018},
  month = jul,
  abstract = {Modern machine learning algorithms are increasingly computationally demanding, requiring specialized hardware and distributed computation to achieve high performance in a reasonable time frame. Many hyperparameter search algorithms have been proposed for improving the efficiency of model selection, however their adaptation to the distributed compute environment is often ad-hoc. We propose Tune, a unified framework for model selection and training that provides a narrow-waist interface between training scripts and search algorithms. We show that this interface meets the requirements for a broad range of hyperparameter search algorithms, allows straightforward scaling of search to large clusters, and simplifies algorithm implementation. We demonstrate the implementation of several state-of-the-art hyperparameter search algorithms in Tune. Tune is available at http://ray.readthedocs.io/en/latest/tune.html.},
  archivePrefix = {arXiv},
  eprint = {1807.05118},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\DN3KT6H3\\Liaw et al. - 2018 - Tune A Research Platform for Distributed Model Se.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\6HVCS84I\\1807.html},
  journal = {arXiv:1807.05118 [cs, stat]},
  keywords = {Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: 8 Pages, Presented at the 2018 ICML AutoML workshop},
  primaryClass = {cs, stat}
}

@article{liDifferentiableMonteCarlo2019,
  title = {Differentiable {{Monte Carlo}} Ray Tracing through Edge Sampling},
  author = {Li, Tzu-Mao and Aittala, Miika and Durand, Fr{\'e}do and Lehtinen, Jaakko},
  year = {2019},
  month = jan,
  volume = {37},
  pages = {1--11},
  issn = {0730-0301, 1557-7368},
  doi = {10.1145/3272127.3275109},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PJH52TTP\\Li et al. - 2019 - Differentiable Monte Carlo ray tracing through edg.pdf},
  journal = {ACM Transactions on Graphics},
  language = {en},
  number = {6}
}

@misc{liMeasuringIntrinsicDimension2018,
  title = {Measuring the {{Intrinsic Dimension}} of {{Objective Landscapes}}},
  author = {Li, Chunyuan},
  year = {2018},
  month = apr,
  abstract = {Curious about what it is like to traverse the high-dimensional loss landscapes of modern neural networks? Check out Uber AI Labs' latest research on measuring intrinsic dimension to find out.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2UZVZEAI\\intrinsic-dimension.html},
  howpublished = {https://eng.uber.com/intrinsic-dimension/},
  journal = {Uber Engineering Blog},
  language = {en-US}
}

@article{liMeasuringIntrinsicDimension2018a,
  title = {Measuring the Intrinsic Dimension of Objective Landscapes},
  author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  year = {2018},
  archivePrefix = {arXiv},
  eprint = {1804.08838},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\X3VCGC5U\\Li et al. - 2018 - Measuring the intrinsic dimension of objective lan.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\6FMGVNZY\\1804.html},
  journal = {arXiv preprint arXiv:1804.08838}
}

@article{liMeasuringIntrinsicDimension2018b,
  title = {Measuring the {{Intrinsic Dimension}} of {{Objective Landscapes}}},
  author = {Li, Chunyuan and Farkhoor, Heerad and Liu, Rosanne and Yosinski, Jason},
  year = {2018},
  month = apr,
  abstract = {Many recently trained neural networks employ large numbers of parameters to achieve good performance. One may intuitively use the number of parameters required as a rough gauge of the difficulty of a problem. But how accurate are such notions? How many parameters are really needed? In this paper we attempt to answer this question by training networks not in their native parameter space, but instead in a smaller, randomly oriented subspace. We slowly increase the dimension of this subspace, note at which dimension solutions first appear, and define this to be the intrinsic dimension of the objective landscape. The approach is simple to implement, computationally tractable, and produces several suggestive conclusions. Many problems have smaller intrinsic dimensions than one might suspect, and the intrinsic dimension for a given dataset varies little across a family of models with vastly different sizes. This latter result has the profound implication that once a parameter space is large enough to solve a problem, extra parameters serve directly to increase the dimensionality of the solution manifold. Intrinsic dimension allows some quantitative comparison of problem difficulty across supervised, reinforcement, and other types of learning where we conclude, for example, that solving the inverted pendulum problem is 100 times easier than classifying digits from MNIST, and playing Atari Pong from pixels is about as hard as classifying CIFAR-10. In addition to providing new cartography of the objective landscapes wandered by parameterized models, the method is a simple technique for constructively obtaining an upper bound on the minimum description length of a solution. A byproduct of this construction is a simple approach for compressing networks, in some cases by more than 100 times.},
  archivePrefix = {arXiv},
  eprint = {1804.08838},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\XESXCAIP\\Li et al. - 2018 - Measuring the Intrinsic Dimension of Objective Lan.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\AUA5F8ZH\\1804.html},
  journal = {arXiv:1804.08838 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: Published in ICLR 2018},
  primaryClass = {cs, stat}
}

@article{liNeuralNetworkRenormalization2018,
  title = {Neural {{Network Renormalization Group}}},
  author = {Li, Shuo-Hui and Wang, Lei},
  year = {2018},
  month = dec,
  volume = {121},
  pages = {260601},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.121.260601},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\X857J4L2\\Li and Wang - 2018 - Neural Network Renormalization Group.pdf},
  journal = {Physical Review Letters},
  language = {en},
  number = {26}
}

@article{liNeuralNetworkRenormalization2018a,
  title = {Neural {{Network Renormalization Group}}},
  author = {Li, Shuo-Hui and Wang, Lei},
  year = {2018},
  month = dec,
  volume = {121},
  pages = {260601},
  issn = {0031-9007, 1079-7114},
  doi = {10.1103/PhysRevLett.121.260601},
  abstract = {We present a variational renormalization group (RG) approach using a deep generative model based on normalizing flows. The model performs hierarchical change-of-variables transformations from the physical space to a latent space with reduced mutual information. Conversely, the neural net directly maps independent Gaussian noises to physical configurations following the inverse RG flow. The model has an exact and tractable likelihood, which allows unbiased training and direct access to the renormalized energy function of the latent variables. To train the model, we employ probability density distillation for the bare energy function of the physical problem, in which the training loss provides a variational upper bound of the physical free energy. We demonstrate practical usage of the approach by identifying mutually independent collective variables of the Ising model and performing accelerated hybrid Monte Carlo sampling in the latent space. Lastly, we comment on the connection of the present approach to the wavelet formulation of RG and the modern pursuit of information preserving RG.},
  archivePrefix = {arXiv},
  eprint = {1802.02840},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\3QL2PEJ8\\Li and Wang - 2018 - Neural Network Renormalization Group.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\ZAPZH5J9\\1802.html},
  journal = {Physical Review Letters},
  keywords = {Computer Science - Machine Learning,Condensed Matter - Statistical Mechanics,Statistics - Machine Learning},
  note = {Comment: Main text: 4.5 pages, 4 figures. Supplement: 3 pages. Github link: https://github.com/li012589/NeuralRG},
  number = {26}
}

@inproceedings{linFocalLossDense2017,
  title = {Focal {{Loss}} for {{Dense Object Detection}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Lin, Tsung-Yi and Goyal, Priya and Girshick, Ross and He, Kaiming and Dollar, Piotr},
  year = {2017},
  pages = {2980--2988},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6BVGHJT5\\Lin et al. - 2017 - Focal Loss for Dense Object Detection.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\HX84BLDE\\Lin_Focal_Loss_for_ICCV_2017_paper.html},
  keywords = {read-soon}
}

@article{linNetworkNetwork2014,
  title = {Network {{In Network}}},
  author = {Lin, Min and Chen, Qiang and Yan, Shuicheng},
  year = {2014},
  month = mar,
  abstract = {We propose a novel deep network structure called "Network In Network" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets.},
  archivePrefix = {arXiv},
  eprint = {1312.4400},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CUZKD59N\\Lin et al. - 2014 - Network In Network.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\AXXRDTFM\\1312.html},
  journal = {arXiv:1312.4400 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-soon},
  note = {Comment: 10 pages, 4 figures, for iclr2014},
  primaryClass = {cs}
}

@article{liPruningFiltersEfficient2017,
  title = {Pruning {{Filters}} for {{Efficient ConvNets}}},
  author = {Li, Hao and Kadav, Asim and Durdanovic, Igor and Samet, Hanan and Graf, Hans Peter},
  year = {2017},
  month = mar,
  abstract = {The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34\% and ResNet-110 by up to 38\% on CIFAR10 while regaining close to the original accuracy by retraining the networks.},
  archivePrefix = {arXiv},
  eprint = {1608.08710},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\BPMPSIWP\\Li et al. - 2017 - Pruning Filters for Efficient ConvNets.pdf},
  journal = {arXiv:1608.08710 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  note = {Comment: Published as a conference paper at ICLR 2017},
  primaryClass = {cs}
}

@article{liuDARTSDifferentiableArchitecture2019,
  title = {{{DARTS}}: {{Differentiable Architecture Search}}},
  shorttitle = {{{DARTS}}},
  author = {Liu, Hanxiao and Simonyan, Karen and Yang, Yiming},
  year = {2019},
  month = apr,
  abstract = {This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms.},
  archivePrefix = {arXiv},
  eprint = {1806.09055},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NF3MGHCK\\Liu et al. - 2019 - DARTS Differentiable Architecture Search.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\GDUAISVN\\1806.html},
  journal = {arXiv:1806.09055 [cs, stat]},
  keywords = {architecture,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published at ICLR 2019; Code and pretrained models available at https://github.com/quark0/darts},
  primaryClass = {cs, stat}
}

@article{liuIntriguingFailingConvolutional2018,
  ids = {liuIntriguingFailingConvolutional2018a},
  title = {An {{Intriguing Failing}} of {{Convolutional Neural Networks}} and the {{CoordConv Solution}}},
  author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
  year = {2018},
  month = dec,
  abstract = {Few ideas have enjoyed as large an impact on deep learning as convolution. For any problem involving pixels or spatial representations, common intuition holds that convolutional neural networks may be appropriate. In this paper we show a striking counterexample to this intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x,y) Cartesian space and one-hot pixel space. Although convolutional networks would seem appropriate for this task, we show that they fail spectacularly. We demonstrate and carefully analyze the failure first on a toy problem, at which point a simple fix becomes obvious. We call this solution CoordConv, which works by giving convolution access to its own input coordinates through the use of extra coordinate channels. Without sacrificing the computational and parametric efficiency of ordinary convolution, CoordConv allows networks to learn either complete translation invariance or varying degrees of translation dependence, as required by the end task. CoordConv solves the coordinate transform problem with perfect generalization and 150 times faster with 10--100 times fewer parameters than convolution. This stark contrast raises the question: to what extent has this inability of convolution persisted insidiously inside other tasks, subtly hampering performance from within? A complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for CoordConv can improve models on a diverse set of tasks. Using CoordConv in a GAN produced less mode collapse as the transform between high-level spatial latents and pixels becomes easier to learn. A Faster R-CNN detection model trained on MNIST showed 24\% better IOU when using CoordConv, and in the RL domain agents playing Atari games benefit significantly from the use of CoordConv layers.},
  archivePrefix = {arXiv},
  eprint = {1807.03247},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZSJRX538\\Liu et al. - 2018 - An Intriguing Failing of Convolutional Neural Netw.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\MYIS5U2N\\1807.html},
  journal = {arXiv:1807.03247 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: Published in NeurIPS 2018
\par
Comment: Published in NeurIPS 2018},
  primaryClass = {cs, stat}
}

@inproceedings{liuIntriguingFailingConvolutional2018a,
  title = {An Intriguing Failing of Convolutional Neural Networks and the Coordconv Solution},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Such, Felipe Petroski and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
  year = {2018},
  pages = {9605--9616},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\UATW6ZD4\\Liu et al. - 2018 - An intriguing failing of convolutional neural netw.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\9MTRT3GW\\8169-an-intriguing-failing-of-convolutional-neural-networks-and-the-coordconv-solution.html}
}

@incollection{liuIntriguingFailingConvolutional2018b,
  title = {An Intriguing Failing of Convolutional Neural Networks and the {{CoordConv}} Solution},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Liu, Rosanne and Lehman, Joel and Molino, Piero and Petroski Such, Felipe and Frank, Eric and Sergeev, Alex and Yosinski, Jason},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {9605--9616},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\UHJFPMIE\\Liu et al. - 2018 - An intriguing failing of convolutional neural netw.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\M5VH2PNS\\8169-an-intriguing-failing-of-convolutional-neural-networks-and-the-coordconv-solution.html}
}

@inproceedings{liUnderstandingDisharmonyDropout2019,
  title = {Understanding the {{Disharmony Between Dropout}} and {{Batch Normalization}} by {{Variance Shift}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Li, Xiang and Chen, Shuo and Hu, Xiaolin and Yang, Jian},
  year = {2019},
  month = jun,
  pages = {2677--2685},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00279},
  abstract = {This paper first answers the question ``why do the two most powerful techniques Dropout and Batch Normalization (BN) often lead to a worse performance when they are combined together in many modern neural networks, but cooperate well sometimes as in Wide ResNet (WRN)?'' in both theoretical and empirical aspects. Theoretically, we find that Dropout shifts the variance of a specific neural unit when we transfer the state of that network from training to test. However, BN maintains its statistical variance, which is accumulated from the entire learning procedure, in the test phase. The inconsistency of variances in Dropout and BN (we name this scheme ``variance shift'') causes the unstable numerical behavior in inference that leads to erroneous predictions finally. Meanwhile, the large feature dimension in WRN further reduces the ``variance shift'' to bring benefits to the overall performance. Thorough experiments on representative modern convolutional networks like DenseNet, ResNet, ResNeXt and Wide ResNet confirm our findings. According to the uncovered mechanism, we get better understandings in the combination of these two techniques and summarize guidelines for better practices.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6876XZPL\\Li et al. - 2019 - Understanding the Disharmony Between Dropout and B.pdf},
  isbn = {978-1-72813-293-8},
  keywords = {read},
  language = {en}
}

@article{liuNeuralSDEStabilizing2019,
  title = {Neural {{SDE}}: {{Stabilizing Neural ODE Networks}} with {{Stochastic Noise}}},
  shorttitle = {Neural {{SDE}}},
  author = {Liu, Xuanqing and Xiao, Tesi and Si, Si and Cao, Qin and Kumar, Sanjiv and Hsieh, Cho-Jui},
  year = {2019},
  month = jun,
  abstract = {Neural Ordinary Differential Equation (Neural ODE) has been proposed as a continuous approximation to the ResNet architecture. Some commonly used regularization mechanisms in discrete neural networks (e.g. dropout, Gaussian noise) are missing in current Neural ODE networks. In this paper, we propose a new continuous neural network framework called Neural Stochastic Differential Equation (Neural SDE) network, which naturally incorporates various commonly used regularization mechanisms based on random noise injection. Our framework can model various types of noise injection frequently used in discrete networks for regularization purpose, such as dropout and additive/multiplicative noise in each block. We provide theoretical analysis explaining the improved robustness of Neural SDE models against input perturbations/adversarial attacks. Furthermore, we demonstrate that the Neural SDE network can achieve better generalization than the Neural ODE and is more resistant to adversarial and non-adversarial input perturbations.},
  archivePrefix = {arXiv},
  eprint = {1906.02355},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PXGGFZAY\\Liu et al. - 2019 - Neural SDE Stabilizing Neural ODE Networks with S.pdf},
  journal = {arXiv:1906.02355 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,NODE,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@incollection{liuProgressiveNeuralArchitecture2018,
  title = {Progressive {{Neural Architecture Search}}},
  booktitle = {Computer {{Vision}} \textendash{} {{ECCV}} 2018},
  author = {Liu, Chenxi and Zoph, Barret and Neumann, Maxim and Shlens, Jonathon and Hua, Wei and Li, Li-Jia and {Fei-Fei}, Li and Yuille, Alan and Huang, Jonathan and Murphy, Kevin},
  editor = {Ferrari, Vittorio and Hebert, Martial and Sminchisescu, Cristian and Weiss, Yair},
  year = {2018},
  volume = {11205},
  pages = {19--35},
  publisher = {{Springer International Publishing}},
  address = {{Cham}},
  doi = {10.1007/978-3-030-01246-5_2},
  abstract = {We propose a new method for learning the structure of convolutional neural networks (CNNs) that is more efficient than recent state-of-the-art methods based on reinforcement learning and evolutionary algorithms. Our approach uses a sequential model-based optimization (SMBO) strategy, in which we search for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the search through structure space. Direct comparison under the same search space shows that our method is up to 5 times more efficient than the RL method of Zoph et al. (2018) in terms of number of models evaluated, and 8 times faster in terms of total compute. The structures we discover in this way achieve state of the art classification accuracies on CIFAR-10 and ImageNet.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GX57TEET\\Chenxi_Liu_Progressive_Neural_Architecture_ECCV_2018_paper.pdf},
  isbn = {978-3-030-01245-8 978-3-030-01246-5},
  keywords = {read-soon},
  language = {en}
}

@article{liuSSDSingleShot2016,
  title = {{{SSD}}: {{Single Shot MultiBox Detector}}},
  shorttitle = {{{SSD}}},
  author = {Liu, Wei and Anguelov, Dragomir and Erhan, Dumitru and Szegedy, Christian and Reed, Scott and Fu, Cheng-Yang and Berg, Alexander C.},
  year = {2016},
  volume = {9905},
  pages = {21--37},
  doi = {10.1007/978-3-319-46448-0_2},
  abstract = {We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. SSD is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stages and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, COCO, and ILSVRC datasets confirm that SSD has competitive accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. For 300 \texttimes{} 300 input, SSD achieves 74.3\% mAP1 on VOC2007 test at 59 FPS on a Nvidia Titan X and for 512 \texttimes{} 512 input, SSD achieves 76.9\% mAP, outperforming a comparable state-of-the-art Faster R-CNN model. Compared to other single stage methods, SSD has much better accuracy even with a smaller input image size. Code is available at: https://github.com/weiliu89/caffe/tree/ssd .},
  archivePrefix = {arXiv},
  eprint = {1512.02325},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZPUUADD9\\Liu et al. - 2016 - SSD Single Shot MultiBox Detector.pdf},
  journal = {arXiv:1512.02325 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-soon},
  language = {en},
  note = {Comment: ECCV 2016},
  primaryClass = {cs}
}

@article{liuVarianceAdaptiveLearning2020,
  title = {On the {{Variance}} of the {{Adaptive Learning Rate}} and {{Beyond}}},
  author = {Liu, Liyuan and Jiang, Haoming and He, Pengcheng and Chen, Weizhu and Liu, Xiaodong and Gao, Jianfeng and Han, Jiawei},
  year = {2020},
  month = apr,
  abstract = {The learning rate warmup heuristic achieves remarkable success in stabilizing training, accelerating convergence and improving generalization for adaptive stochastic optimization algorithms like RMSprop and Adam. Here, we study its mechanism in details. Pursuing the theory behind warmup, we identify a problem of the adaptive learning rate (i.e., it has problematically large variance in the early stage), suggest warmup works as a variance reduction technique, and provide both empirical and theoretical evidence to verify our hypothesis. We further propose RAdam, a new variant of Adam, by introducing a term to rectify the variance of the adaptive learning rate. Extensive experimental results on image classification, language modeling, and neural machine translation verify our intuition and demonstrate the effectiveness and robustness of our proposed method. All implementations are available at: https://github.com/LiyuanLucasLiu/RAdam.},
  archivePrefix = {arXiv},
  eprint = {1908.03265},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YQLXIB3W\\Liu et al. - 2020 - On the Variance of the Adaptive Learning Rate and .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\Z446ISFY\\1908.html},
  journal = {arXiv:1908.03265 [cs, stat]},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,lr,RAdam,read,Statistics - Machine Learning},
  note = {Comment: ICLR 2020. Fix several typos in the previous version},
  primaryClass = {cs, stat}
}

@inproceedings{longFullyConvolutionalNetworks2015,
  title = {Fully {{Convolutional Networks}} for {{Semantic Segmentation}}},
  booktitle = {Proceedings of the {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Long, Jonathan and Shelhamer, Evan and Darrell, Trevor},
  year = {2015},
  pages = {3431--3440},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4LRF2A75\\Long et al. - 2015 - Fully Convolutional Networks for Semantic Segmenta.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\8NSVIH2M\\Long_Fully_Convolutional_Networks_2015_CVPR_paper.html}
}

@article{longPPYOLOEffectiveEfficient2020,
  title = {{{PP}}-{{YOLO}}: {{An Effective}} and {{Efficient Implementation}} of {{Object Detector}}},
  shorttitle = {{{PP}}-{{YOLO}}},
  author = {Long, Xiang and Deng, Kaipeng and Wang, Guanzhong and Zhang, Yang and Dang, Qingqing and Gao, Yuan and Shen, Hui and Ren, Jianguo and Han, Shumin and Ding, Errui and Wen, Shilei},
  year = {2020},
  month = jul,
  abstract = {Object detection is one of the most important areas in computer vision, which plays a key role in various practical scenarios. Due to limitation of hardware, it is often necessary to sacrifice accuracy to ensure the infer speed of the detector in practice. Therefore, the balance between effectiveness and efficiency of object detector must be considered. The goal of this paper is to implement an object detector with relatively balanced effectiveness and efficiency that can be directly applied in actual application scenarios, rather than propose a novel detection model. Considering that YOLOv3 has been widely used in practice, we develop a new object detector based on YOLOv3. We mainly try to combine various existing tricks that almost not increase the number of model parameters and FLOPs, to achieve the goal of improving the accuracy of detector as much as possible while ensuring that the speed is almost unchanged. Since all experiments in this paper are conducted based on PaddlePaddle, we call it PPYOLO. By combining multiple tricks, PP-YOLO can achieve a better balance between effectiveness (45.2\% mAP) and efficiency (72.9 FPS), surpassing the existing state-of-theart detectors such as EfficientDet and YOLOv4. Source code is at https://github.com/PaddlePaddle/ PaddleDetection.},
  archivePrefix = {arXiv},
  eprint = {2007.12099},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\UFE4J7MW\\Long et al. - 2020 - PP-YOLO An Effective and Efficient Implementation.pdf},
  journal = {arXiv:2007.12099 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-next},
  language = {en},
  primaryClass = {cs}
}

@article{loshchilovFixingWeightDecay2018,
  title = {Fixing {{Weight Decay Regularization}} in {{Adam}}},
  author = {Loshchilov, Ilya and Hutter, Frank},
  year = {2018},
  month = feb,
  abstract = {We note that common implementations of adaptive gradient algorithms, such as Adam, limit the potential benefit of weight decay regularization, because the weights do not decay multiplicatively (as...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\K59HKPAX\\Loshchilov and Hutter - 2018 - Fixing Weight Decay Regularization in Adam.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\2QLSQ35N\\forum.html},
  keywords = {batch_size,lr,read,weight decay}
}

@article{lu12in1MultiTaskVision2019,
  title = {12-in-1: {{Multi}}-{{Task Vision}} and {{Language Representation Learning}}},
  shorttitle = {12-in-1},
  author = {Lu, Jiasen and Goswami, Vedanuj and Rohrbach, Marcus and Parikh, Devi and Lee, Stefan},
  year = {2019},
  month = dec,
  abstract = {Much of vision-and-language research focuses on a small but diverse set of independent tasks and supporting datasets often studied in isolation; however, the visuallygrounded language understanding skills required for success at these tasks overlap significantly. In this work, we investigate these relationships between vision-and-language tasks by developing a large-scale, multi-task training regime. Our approach culminates in a single model on 12 datasets from four broad categories of task including visual question answering, caption-based image retrieval, grounding referring expressions, and multi-modal verification. Compared to independently trained single-task models, this represents a reduction from approximately 3 billion parameters to 270 million while simultaneously improving performance by 2.05 points on average across tasks. We use our multi-task framework to perform in-depth analysis of the effect of joint training diverse tasks. Further, we show that finetuning task-specific models from our single multi-task model can lead to further improvements, achieving performance at or above the state-of-the-art.},
  archivePrefix = {arXiv},
  eprint = {1912.02315},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HP6KVFQ4\\Lu et al. - 2019 - 12-in-1 Multi-Task Vision and Language Representa.pdf},
  journal = {arXiv:1912.02315 [cs]},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,multi-task},
  language = {en},
  note = {Comment: Jiasen Lu and Vedanuj Goswami contributed equally to this work},
  primaryClass = {cs}
}

@book{luCollapseDeepNarrow2018,
  title = {Collapse of {{Deep}} and {{Narrow Neural Nets}}},
  author = {Lu, Lu and Su, Yan-Hui and Karniadakis, George},
  year = {2018},
  month = aug,
  abstract = {Recent theoretical work has demonstrated that deep neural networks have superior performance over shallow networks, but their training is more difficult, e.g., they suffer from the vanishing gradient problem. This problem can be typically resolved by the rectified linear unit (ReLU) activation. However, here we show that even for such activation, deep and narrow neural networks (NNs) will converge to erroneous mean or median states of the target function depending on the loss with high probability. Deep and narrow NNs are encountered in solving partial differential equations with high-order derivatives. We demonstrate this collapse of such NNs both numerically and theoretically, and provide estimates of the probability of collapse. We also construct a diagram of a safe region for designing NNs that avoid the collapse to erroneous states. Finally, we examine different ways of initialization and normalization that may avoid the collapse problem. Asymmetric initializations may reduce the probability of collapse but do not totally eliminate it.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4LAM3S8T\\Lu et al. - 2018 - Collapse of Deep and Narrow Neural Nets.pdf}
}

@article{luMeanfieldAnalysisDeep,
  title = {A {{Mean}}-Field {{Analysis}} of {{Deep ResNet}} and {{Beyond}}: {{Towards Provable Optimization Via Overparameterization From Depth}}},
  author = {Lu, Yiping and Ma, Chao and Lu, Yulong and Lu, Jianfeng and Ying, Lexing},
  pages = {11},
  abstract = {Training deep neural networks with stochastic gradient descent (SGD) can often achieve zero training loss on real-world tasks although the optimization landscape is known to be highly non-convex. To understand the success of SGD for training deep neural networks, this work presents a meanfield analysis of deep residual networks, based on a line of works that interpret the continuum limit of the deep residual network as an ordinary differential equation when the network capacity tends to infinity. Specifically, we propose a new continuum limit of deep residual networks, which enjoys a good landscape in the sense that every local minimizer is global. This characterization enables us to derive the first global convergence result for multilayer neural networks in the meanfield regime. Furthermore, without assuming the convexity of the loss landscape, our proof relies on a zero-loss assumption at the global minimizer that can be achieved when the model shares a universal approximation property. Key to our result is the observation that a deep residual network resembles a shallow network ensemble (Veit et al., 2016), i.e. a two-layer network. We bound the difference between the shallow network and our ResNet model via the adjoint sensitivity method, which enables us to apply existing mean-field analyses of two-layer networks to deep networks. Furthermore, we propose several novel training schemes based on the new continuous model, including one training procedure that switches the order of the residual blocks and results in strong empirical performance on the benchmark datasets.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\T24DAQE9\\Lu et al. - A Mean-field Analysis of Deep ResNet and Beyond T.pdf},
  language = {en}
}

@inproceedings{luoAdaptiveGradientMethods2018,
  title = {Adaptive {{Gradient Methods}} with {{Dynamic Bound}} of {{Learning Rate}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Luo, Liangchen and Xiong, Yuanhao and Liu, Yan and Sun, Xu},
  year = {2018},
  month = sep,
  abstract = {Adaptive optimization methods such as AdaGrad, RMSprop and Adam have been proposed to achieve a rapid training process with an element-wise scaling term on learning rates. Though prevailing, they...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\3E53BETF\\Luo et al. - 2018 - Adaptive Gradient Methods with Dynamic Bound of Le.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\WUZIXFPD\\forum.html},
  keywords = {lr,read}
}

@misc{MachineLearning,
  title = {R/{{MachineLearning}}},
  abstract = {r/MachineLearning:},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YXYUKVED\\d_why_ml_conference_reviews_suck_a_video_analysis.html},
  howpublished = {https://www.reddit.com/r/MachineLearning/},
  journal = {reddit},
  language = {en-US}
}

@misc{MachineLearninga,
  title = {R/{{MachineLearning}}},
  abstract = {r/MachineLearning:},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7P8B3L3Q\\d_is_there_a_theoretically_justified_reason_for.html},
  howpublished = {https://www.reddit.com/r/MachineLearning/},
  journal = {reddit},
  language = {en-US}
}

@misc{MachineLearningb,
  title = {R/{{MachineLearning}}},
  abstract = {r/MachineLearning:},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZAM3HV7J\\d_confused_mathematician_looking_for_clarity_on.html},
  howpublished = {https://www.reddit.com/r/MachineLearning/},
  journal = {reddit},
  language = {en-US}
}

@misc{MachineLearningBeginnersFriendly,
  title = {R/{{MachineLearning}} - [{{D}}] {{Beginners}} Friendly {{Machine Learning}} Conferences or {{How}} to Start as an Individual Researcher},
  abstract = {154 votes and 21 comments so far on Reddit},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HU9QGASY\\d_beginners_friendly_machine_learning_conferences.html},
  howpublished = {https://www.reddit.com/r/MachineLearning/comments/gm31b1/d\_beginners\_friendly\_machine\_learning\_conferences/},
  journal = {reddit},
  language = {en-US}
}

@misc{MachineLearningFactorsSuccessful,
  title = {R/{{MachineLearning}} - [{{D}}] {{Factors}} of Successful {{ML}}({{Ops}}) after 3+ Years of {{ML}} in {{Production}}},
  abstract = {325 votes and 76 comments so far on Reddit},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\MHK5W6YR\\d_factors_of_successful_mlops_after_3_years_of_ml.html},
  howpublished = {https://www.reddit.com/r/MachineLearning/comments/j1xlcm/d\_factors\_of\_successful\_mlops\_after\_3\_years\_of\_ml/},
  journal = {reddit},
  language = {en-US}
}

@misc{MachineLearningJobApplication,
  title = {R/{{MachineLearning}} - [{{D}}] - {{Job Application}} - {{Failed Technical Assessment}} - {{Feedback}} Please},
  abstract = {119 votes and 57 comments so far on Reddit},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HYDEN4TG\\d_job_application_failed_technical_assessment.html},
  howpublished = {https://www.reddit.com/r/MachineLearning/comments/ixvyyv/d\_job\_application\_failed\_technical\_assessment/},
  journal = {reddit},
  language = {en-US}
}

@misc{MachineLearningThereTheoretically,
  title = {R/{{MachineLearning}} - [{{D}}] {{Is}} There a Theoretically Justified Reason for Choosing an Optimizer for Training Neural Networks yet in 2020?},
  abstract = {280 votes and 135 comments so far on Reddit},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TYAYJFEF\\d_is_there_a_theoretically_justified_reason_for.html},
  howpublished = {https://www.reddit.com/r/MachineLearning/comments/j302g8/d\_is\_there\_a\_theoretically\_justified\_reason\_for/},
  journal = {reddit},
  keywords = {lr},
  language = {en-US},
  note = {Classical optimization methods are designed to solve very different problems.
\par
First of all, the numbers of parameters in NNs are huge. While newton method converges very rapidly even for ill-conditioned problems, it needs O(n\textsuperscript{2)} of memory and has O(n\textsuperscript{3)} complexity per iteration, where n is the number of parameters. When n is in millions, it's practically impossible. Higher order methods just don't scale in the number of parameters.
\par
Next big difference - S in SGD stands for \emph{stochastic}. Classical algorithms usually require exact gradients. You would need to go through entire dataset and average gradients, all for a single parameter update. Training one epoch with SGD will have the same cost. It will make much better progress, given that the dataset is big enough.
\par
Going on, NNs are greatly overparameterized. Classical methods are designed to solve badly conditioned problems, because usually you don't get to choose parametrization. There are no additional degrees of freedom in solution. If it happens to lie ill-conditioned region - you have to deal with it. NNs on the other hand lots of freedom. Gradient descent can find a region where the objective is relatively well conditioned. It's like gimbal lock. Redundant gimbals make the problem go away.
\par
Finally, NNs don't need superlinear convergence. Convergence rate is a very important property in optimization theory. You can get thousands, even millions of accurate digits in just a handful of iterations. But that doesn't matter for training NNs.}
}

@article{madasuEffectivenessSelfNormalizing2019,
  title = {Effectiveness of {{Self Normalizing Neural Networks}} for {{Text Classification}}},
  author = {Madasu, Avinash and Rao, Vijjini Anvesh},
  year = {2019},
  month = may,
  abstract = {Self Normalizing Neural Networks(SNN) proposed on Feed Forward Neural Networks(FNN) outperform regular FNN architectures in various machine learning tasks. Particularly in the domain of Computer Vision, the activation function Scaled Exponential Linear Units (SELU) proposed for SNNs, perform better than other non linear activations such as ReLU. The goal of SNN is to produce a normalized output for a normalized input. Established neural network architectures like feed forward networks and Convolutional Neural Networks(CNN) lack the intrinsic nature of normalizing outputs. Hence, requiring additional layers such as Batch Normalization. Despite the success of SNNs, their characteristic features on other network architectures like CNN haven't been explored, especially in the domain of Natural Language Processing. In this paper we aim to show the effectiveness of proposed, Self Normalizing Convolutional Neural Networks(SCNN) on text classification. We analyze their performance with the standard CNN architecture used on several text classification datasets. Our experiments demonstrate that SCNN achieves comparable results to standard CNN model with significantly fewer parameters. Furthermore it also outperforms CNN with equal number of parameters.},
  archivePrefix = {arXiv},
  eprint = {1905.01338},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\32MBMDQY\\Madasu and Rao - 2019 - Effectiveness of Self Normalizing Neural Networks .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\D3YNY8V3\\1905.html},
  journal = {arXiv:1905.01338 [cs]},
  keywords = {Computer Science - Computation and Language,read-soon},
  note = {Comment: Accepted Long Paper at 20th International Conference on Computational Linguistics and Intelligent Text Processing, April 2019, La Rochelle, France},
  primaryClass = {cs}
}

@inproceedings{maFixupInitializationResidual2018,
  title = {Fixup {{Initialization}}: {{Residual Learning Without Normalization}}},
  shorttitle = {Fixup {{Initialization}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Ma, Tengyu},
  year = {2018},
  month = sep,
  abstract = {All you need to train deep residual networks is a good initialization; normalization layers are not necessary.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7KDWDLRZ\\Ma - 2018 - Fixup Initialization Residual Learning Without No.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\GDQQ3SED\\forum.html},
  keywords = {batch norm,normalization,read-next}
}

@article{maiConvergenceStochasticGradient,
  title = {Convergence of a {{Stochastic Gradient Method}} with {{Momentum}} for {{Non}}-{{Smooth Non}}-{{Convex Optimization}}},
  author = {Mai, Vien V and Johansson, Mikael},
  pages = {10},
  abstract = {Stochastic gradient methods with momentum are widely used in applications and at the core of optimization subroutines in many popular machine learning libraries. However, their sample complexities have not been obtained for problems beyond those that are convex or smooth. This paper establishes the convergence rate of a stochastic subgradient method with a momentum term of Polyak type for a broad class of non-smooth, non-convex, and constrained optimization problems. Our key innovation is the construction of a special Lyapunov function for which the proven complexity can be achieved without any tuning of the momentum parameter. For smooth problems, we extend the known complexity bound to the constrained case and demonstrate how the unconstrained case can be analyzed under weaker assumptions than the state-of-the-art. Numerical results confirm our theoretical developments.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\X8D7KXSU\\Mai and Johansson - Convergence of a Stochastic Gradient Method with M.pdf},
  journal = {Convex Optimization},
  language = {en}
}

@article{maiorovLowerBoundsApproximation1999,
  title = {Lower Bounds for Approximation by {{MLP}} Neural Networks},
  author = {Maiorov, Vitaly and Pinkus, Allan},
  year = {1999},
  month = apr,
  volume = {25},
  pages = {81--91},
  issn = {09252312},
  doi = {10.1016/S0925-2312(98)00111-8},
  abstract = {The degree of approximation by a single hidden layer MLP model with n units in the hidden layer is bounded below by the degree of approximation by a linear combination of n ridge functions. We prove that there exists an analytic, strictly monotone, sigmoidal activation function for which this lower bound is essentially attained. We also prove, using this same activation function, that one can approximate arbitrarily well any continuous function on any compact domain by a two hidden layer MLP using a "xed "nite number of units in each layer. 1999 Elsevier Science B.V. All rights reserved.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JBPC2VI2\\Maiorov and Pinkus - 1999 - Lower bounds for approximation by MLP neural netwo.pdf},
  journal = {Neurocomputing},
  keywords = {read-soon},
  language = {en},
  number = {1-3}
}

@misc{MakingPublicationReady,
  title = {Making Publication Ready {{Python Notebooks}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GXIPHFQE\\ultimate-ipython-notebook.html},
  howpublished = {http://blog.juliusschulz.de/blog/ultimate-ipython-notebook}
}

@article{makridakisStatisticalMachineLearning2018,
  title = {Statistical and {{Machine Learning}} Forecasting Methods: {{Concerns}} and Ways Forward},
  shorttitle = {Statistical and {{Machine Learning}} Forecasting Methods},
  author = {Makridakis, Spyros and Spiliotis, Evangelos and Assimakopoulos, Vassilios},
  editor = {Hernandez Montoya, Alejandro Raul},
  year = {2018},
  month = mar,
  volume = {13},
  pages = {e0194889},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0194889},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NMIPU8WD\\Makridakis et al. - 2018 - Statistical and Machine Learning forecasting metho.pdf},
  journal = {PLOS ONE},
  language = {en},
  number = {3}
}

@inproceedings{martinezSimpleEffectiveBaseline2017,
  title = {A {{Simple}} yet {{Effective Baseline}} for {{3D Human Pose Estimation}}},
  booktitle = {Proceedings of the {{IEEE International Conference}} on {{Computer Vision}}},
  author = {Martinez, Julieta and Hossain, Rayat and Romero, Javier and Little, James J.},
  year = {2017},
  pages = {2640--2649},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\V7FAWGBJ\\Martinez et al. - 2017 - A Simple yet Effective Baseline for 3D Human Pose .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\V897I8WT\\Martinez_A_Simple_yet_ICCV_2017_paper.html}
}

@article{martinImplicitSelfRegularizationDeep2018,
  title = {Implicit {{Self}}-{{Regularization}} in {{Deep Neural Networks}}: {{Evidence}} from {{Random Matrix Theory}} and {{Implications}} for {{Learning}}},
  shorttitle = {Implicit {{Self}}-{{Regularization}} in {{Deep Neural Networks}}},
  author = {Martin, Charles H. and Mahoney, Michael W.},
  year = {2018},
  month = oct,
  abstract = {Random Matrix Theory (RMT) is applied to analyze weight matrices of Deep Neural Networks (DNNs), including both production quality, pre-trained models such as AlexNet and Inception, and smaller models trained from scratch, such as LeNet5 and a miniature-AlexNet. Empirical and theoretical results clearly indicate that the DNN training process itself implicitly implements a form of Self-Regularization. The empirical spectral density (ESD) of DNN layer matrices displays signatures of traditionally-regularized statistical models, even in the absence of exogenously specifying traditional forms of explicit regularization. Building on relatively recent results in RMT, most notably its extension to Universality classes of Heavy-Tailed matrices, we develop a theory to identify 5+1 Phases of Training, corresponding to increasing amounts of Implicit Self-Regularization. These phases can be observed during the training process as well as in the final learned DNNs. For smaller and/or older DNNs, this Implicit Self-Regularization is like traditional Tikhonov regularization, in that there is a "size scale" separating signal from noise. For state-of-the-art DNNs, however, we identify a novel form of Heavy-Tailed Self-Regularization, similar to the self-organization seen in the statistical physics of disordered systems. This results from correlations arising at all size scales, which arises implicitly due to the training process itself. This implicit Self-Regularization can depend strongly on the many knobs of the training process. By exploiting the generalization gap phenomena, we demonstrate that we can cause a small model to exhibit all 5+1 phases of training simply by changing the batch size. This demonstrates that---all else being equal---DNN optimization with larger batch sizes leads to less-well implicitly-regularized models, and it provides an explanation for the generalization gap phenomena.},
  archivePrefix = {arXiv},
  eprint = {1810.01075},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Y6B9HLTL\\Martin and Mahoney - 2018 - Implicit Self-Regularization in Deep Neural Networ.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\YWV9SZ9J\\1810.html},
  journal = {arXiv:1810.01075 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  note = {Comment: 59 pages, 31 figures},
  primaryClass = {cs, stat}
}

@article{maslovSpecificityStabilityTopology2002,
  title = {Specificity and {{Stability}} in {{Topology}} of {{Protein Networks}}},
  author = {Maslov, S.},
  year = {2002},
  month = may,
  volume = {296},
  pages = {910--913},
  issn = {00368075, 10959203},
  doi = {10.1126/science.1065103},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\84H5RRT6\\Maslov - 2002 - Specificity and Stability in Topology of Protein N.pdf},
  journal = {Science},
  language = {en},
  number = {5569}
}

@misc{MathematicsComputationIntuitionistic,
  title = {Mathematics and {{Computation}} | {{Intuitionistic}} Mathematics for Physics},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NP5DKMIC\\intuitionistic-mathematics-for-physics.html},
  howpublished = {http://math.andrej.com/2008/08/13/intuitionistic-mathematics-for-physics/}
}

@article{melas-kyriaziMathematicalFoundationsManifold2020,
  title = {The {{Mathematical Foundations}} of {{Manifold Learning}}},
  author = {{Melas-Kyriazi}, Luke},
  year = {2020},
  month = oct,
  abstract = {Manifold learning is a popular and quickly-growing subfield of machine learning based on the assumption that one's observed data lie on a low-dimensional manifold embedded in a higher-dimensional space. This thesis presents a mathematical perspective on manifold learning, delving into the intersection of kernel learning, spectral graph theory, and differential geometry. Emphasis is placed on the remarkable interplay between graphs and manifolds, which forms the foundation for the widely-used technique of manifold regularization. This work is written to be accessible to a broad mathematical audience, including machine learning researchers and practitioners interested in understanding the theorems underlying popular manifold learning algorithms and dimensionality reduction techniques.},
  archivePrefix = {arXiv},
  eprint = {2011.01307},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NEBUCWSP\\Melas-Kyriazi - 2020 - The Mathematical Foundations of Manifold Learning.pdf},
  journal = {arXiv:2011.01307 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  language = {en},
  note = {Comment: Undergraduate Thesis (Harvard Mathematics Department)},
  primaryClass = {cs}
}

@article{mengStatisticalParadisesParadoxes2018,
  title = {Statistical Paradises and Paradoxes in Big Data ({{I}}): {{Law}} of Large Populations, Big Data Paradox, and the 2016 {{US}} Presidential Election},
  shorttitle = {Statistical Paradises and Paradoxes in Big Data ({{I}})},
  author = {Meng, Xiao-Li},
  year = {2018},
  month = jun,
  volume = {12},
  pages = {685--726},
  issn = {1932-6157},
  doi = {10.1214/18-AOAS1161SF},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\8CHWG74X\\statistical_paradises_and_paradoxes.pdf},
  journal = {The Annals of Applied Statistics},
  keywords = {read-next},
  language = {en},
  number = {2}
}

@article{meritySingleHeadedAttention2019,
  title = {Single {{Headed Attention RNN}}: {{Stop Thinking With Your Head}}},
  shorttitle = {Single {{Headed Attention RNN}}},
  author = {Merity, Stephen},
  year = {2019},
  month = nov,
  abstract = {The leading approaches in language modeling are all obsessed with TV shows of my youth - namely Transformers and Sesame Street. Transformers this, Transformers that, and over here a bonfire worth of GPU-TPU-neuromorphic wafer scale silicon. We opt for the lazy path of old and proven techniques with a fancy crypto1 inspired acronym: the Single Headed Attention RNN (SHA-RNN). The author's lone goal is to show that the entire field might have evolved a different direction if we had instead been obsessed with a slightly different acronym and slightly different result. We take a previously strong language model based only on boring LSTMs and get it to within a stone's throw of a stone's throw of state-of-the-art byte level language model results on enwik8. This work has undergone no intensive hyperparameter optimization and lived entirely on a commodity desktop machine that made the author's small studio apartment far too warm in the midst of a San Franciscan summer2. The final results are achievable in plus or minus 24 hours on a single GPU as the author is impatient. The attention mechanism is also readily extended to large contexts with minimal computation. Take that Sesame Street.},
  archivePrefix = {arXiv},
  eprint = {1911.11423},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\L7P5YJUG\\Merity - 2019 - Single Headed Attention RNN Stop Thinking With Yo.pdf},
  journal = {arXiv:1911.11423 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  note = {Comment: Addition of citations and contextual results (no attention head, single attention head, attention per layer), removal of wordpiece WikiText-103 numbers due to normalization issues, fix of SHA attention figure Q arrow, other minor fixes},
  primaryClass = {cs}
}

@article{metzTasksStabilityArchitecture2020,
  title = {Tasks, Stability, Architecture, and Compute: {{Training}} More Effective Learned Optimizers, and Using Them to Train Themselves},
  shorttitle = {Tasks, Stability, Architecture, and Compute},
  author = {Metz, Luke and Maheswaranathan, Niru and Freeman, C. Daniel and Poole, Ben and {Sohl-Dickstein}, Jascha},
  year = {2020},
  month = sep,
  abstract = {Much as replacing hand-designed features with learned functions has revolutionized how we solve perceptual tasks, we believe learned algorithms will transform how we train models. In this work we focus on general-purpose learned optimizers capable of training a wide variety of problems with no user-specified hyperparameters. We introduce a new, neural network parameterized, hierarchical optimizer with access to additional features such as validation loss to enable automatic regularization. Most learned optimizers have been trained on only a single task, or a small number of tasks. We train our optimizers on thousands of tasks, making use of orders of magnitude more compute, resulting in optimizers that generalize better to unseen tasks. The learned optimizers not only perform well, but learn behaviors that are distinct from existing first order optimizers. For instance, they generate update steps that have implicit regularization and adapt as the problem hyperparameters (e.g. batch size) or architecture (e.g. neural network width) change. Finally, these learned optimizers show evidence of being useful for out of distribution tasks such as training themselves from scratch.},
  archivePrefix = {arXiv},
  eprint = {2009.11243},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\AS6M84C5\\Metz et al. - 2020 - Tasks, stability, architecture, and compute Train.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\CFBCM48L\\2009.html},
  journal = {arXiv:2009.11243 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,lr,read-next,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{mianjyImplicitBiasDropout,
  title = {On the {{Implicit Bias}} of {{Dropout}}},
  author = {Mianjy, Poorya and Arora, Raman and Vidal, Rene},
  pages = {9},
  abstract = {Algorithmic approaches endow deep learning systems with implicit bias that helps them generalize even in over-parametrized settings. In this paper, we focus on understanding such a bias induced in learning through dropout, a popular technique to avoid overfitting in deep learning. For single hidden-layer linear neural networks, we show that dropout tends to make the norm of incoming/outgoing weight vectors of all the hidden nodes equal. In addition, we provide a complete characterization of the optimization landscape induced by dropout.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\B9YRZE74\\Mianjy et al. - On the Implicit Bias of Dropout.pdf},
  keywords = {read-soon},
  language = {en}
}

@article{michelsenPhysicistApproachMachine,
  title = {A {{Physicist}}'s {{Approach To Machine Learning}} \textendash{} {{Understanding The Basic Bricks}}},
  author = {Michelsen, Christian},
  pages = {192},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\9MACMVCX\\Michelsen - A Physicist's Approach To Machine Learning – Under.pdf},
  language = {en}
}

@article{migaczPYTORCHPERFORMANCETUNING,
  title = {{{PYTORCH PERFORMANCE TUNING GUIDE}}},
  author = {Migacz, Szymon},
  pages = {16},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\MVLRDX4U\\Migacz - PYTORCH PERFORMANCE TUNING GUIDE.pdf},
  keywords = {performance guide,read-next},
  language = {en}
}

@article{mildenhallNeRFRepresentingScenes2020,
  title = {{{NeRF}}: {{Representing Scenes}} as {{Neural Radiance Fields}} for {{View Synthesis}}},
  shorttitle = {{{NeRF}}},
  author = {Mildenhall, Ben and Srinivasan, Pratul P. and Tancik, Matthew and Barron, Jonathan T. and Ramamoorthi, Ravi and Ng, Ren},
  year = {2020},
  month = mar,
  abstract = {We present a method that achieves state-of-the-art results for synthesizing novel views of complex scenes by optimizing an underlying continuous volumetric scene function using a sparse set of input views. Our algorithm represents a scene using a fully-connected (non-convolutional) deep network, whose input is a single continuous 5D coordinate (spatial location \$(x,y,z)\$ and viewing direction \$(\textbackslash theta, \textbackslash phi)\$) and whose output is the volume density and view-dependent emitted radiance at that spatial location. We synthesize views by querying 5D coordinates along camera rays and use classic volume rendering techniques to project the output colors and densities into an image. Because volume rendering is naturally differentiable, the only input required to optimize our representation is a set of images with known camera poses. We describe how to effectively optimize neural radiance fields to render photorealistic novel views of scenes with complicated geometry and appearance, and demonstrate results that outperform prior work on neural rendering and view synthesis. View synthesis results are best viewed as videos, so we urge readers to view our supplementary video for convincing comparisons.},
  archivePrefix = {arXiv},
  eprint = {2003.08934},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\58SSC9EW\\Mildenhall et al. - 2020 - NeRF Representing Scenes as Neural Radiance Field.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\LKC365YP\\2003.html},
  journal = {arXiv:2003.08934 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Graphics},
  note = {Comment: Project page with videos and code: http://tancik.com/nerf},
  primaryClass = {cs}
}

@article{misraMishSelfRegularized2019,
  ids = {misraMishSelfRegularized2019a},
  title = {Mish: {{A Self Regularized Non}}-{{Monotonic Neural Activation Function}}},
  shorttitle = {Mish},
  author = {Misra, Diganta},
  year = {2019},
  month = oct,
  abstract = {The concept of non-linearity in a Neural Network is introduced by an activation function which serves an integral role in the training and performance evaluation of the network. Over the years of theoretical research, many activation functions have been proposed, however, only a few are widely used in mostly all applications which include ReLU (Rectified Linear Unit), TanH (Tan Hyperbolic), Sigmoid, Leaky ReLU and Swish. In this work, a novel neural activation function called as Mish is proposed. The experiments show that Mish tends to work better than both ReLU and Swish along with other standard activation functions in many deep networks across challenging datasets. For instance, in Squeeze Excite Net- 18 for CIFAR 100 classification, the network with Mish had an increase in Top-1 test accuracy by 0.494\% and 1.671\% as compared to the same network with Swish and ReLU respectively. The similarity to Swish along with providing a boost in performance and its simplicity in implementation makes it easier for researchers and developers to use Mish in their Neural Network Models.},
  archivePrefix = {arXiv},
  eprint = {1908.08681},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SQ78VFHM\\Misra - 2019 - Mish A Self Regularized Non-Monotonic Neural Acti.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\XL3NKL5U\\Misra - 2019 - Mish A Self Regularized Non-Monotonic Neural Acti.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\6439R9SH\\1908.html;C\:\\Users\\ext1150\\Zotero\\storage\\I2BWIXEN\\1908.html},
  journal = {arXiv:1908.08681 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-half,read-soon,Statistics - Machine Learning},
  note = {Comment: 13 pages, 26 figures and 6 tables. Draft Version -2
\par
Comment: 13 pages, 26 figures and 6 tables. Draft Version -2},
  primaryClass = {cs, stat}
}

@book{mitchellMachineLearning1997,
  ids = {mitchellMachineLearning1997a},
  title = {Machine {{Learning}}},
  author = {Mitchell, Tom M.},
  year = {1997},
  publisher = {{McGraw-Hill}},
  address = {{New York}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\G893QPNM\\Mitchell - 1997 - Machine Learning.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\X3MLRSN9\\Mitchell - 1997 - Machine Learning.pdf},
  isbn = {978-0-07-042807-2},
  keywords = {Computer algorithms,lr,Machine learning},
  language = {en},
  lccn = {Q325.5 .M58 1997},
  series = {{{McGraw}}-{{Hill}} Series in Computer Science}
}

@article{miyatoSpectralNormalizationGenerative2018,
  title = {Spectral {{Normalization}} for {{Generative Adversarial Networks}}},
  author = {Miyato, Takeru and Kataoka, Toshiki and Koyama, Masanori and Yoshida, Yuichi},
  year = {2018},
  month = feb,
  abstract = {One of the challenges in the study of generative adversarial networks is the instability of its training. In this paper, we propose a novel weight normalization technique called spectral normalization to stabilize the training of the discriminator. Our new normalization technique is computationally light and easy to incorporate into existing implementations. We tested the efficacy of spectral normalization on CIFAR10, STL-10, and ILSVRC2012 dataset, and we experimentally confirmed that spectrally normalized GANs (SN-GANs) is capable of generating images of better or equal quality relative to the previous training stabilization techniques. The code with Chainer (Tokui et al., 2015), generated images and pretrained models are available at https://github.com/pfnet-research/sngan\_ projection.},
  archivePrefix = {arXiv},
  eprint = {1802.05957},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\258SAWAC\\Miyato et al. - 2018 - Spectral Normalization for Generative Adversarial .pdf},
  journal = {arXiv:1802.05957 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  note = {Comment: Published as a conference paper at ICLR 2018},
  primaryClass = {cs, stat}
}

@incollection{morcosInsightsRepresentationalSimilarity2018,
  title = {Insights on Representational Similarity in Neural Networks with Canonical Correlation},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Morcos, Ari and Raghu, Maithra and Bengio, Samy},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {5727--5736},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HUNCCEYP\\Morcos et al. - 2018 - Insights on representational similarity in neural .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\Q5YTR5YF\\7815-insights-on-representational-similarity-in-neural-networks-with-canonical-correlation.html}
}

@article{mordatchConceptLearningEnergyBased2018,
  title = {Concept {{Learning}} with {{Energy}}-{{Based Models}}},
  author = {Mordatch, Igor},
  year = {2018},
  month = nov,
  abstract = {Many hallmarks of human intelligence, such as generalizing from limited experience, abstract reasoning and planning, analogical reasoning, creative problem solving, and capacity for language require the ability to consolidate experience into concepts, which act as basic building blocks of understanding and reasoning. We present a framework that defines a concept by an energy function over events in the environment, as well as an attention mask over entities participating in the event. Given few demonstration events, our method uses inference-time optimization procedure to generate events involving similar concepts or identify entities involved in the concept. We evaluate our framework on learning visual, quantitative, relational, temporal concepts from demonstration events in an unsupervised manner. Our approach is able to successfully generate and identify concepts in a few-shot setting and resulting learned concepts can be reused across environments. Example videos of our results are available at sites.google.com/site/energyconceptmodels},
  archivePrefix = {arXiv},
  eprint = {1811.02486},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SWWR2UB8\\1811.02486.pdf},
  journal = {arXiv:1811.02486 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-maybe},
  language = {en},
  primaryClass = {cs}
}

@article{morerioCurriculumDropout2017,
  title = {Curriculum {{Dropout}}},
  author = {Morerio, Pietro and Cavazza, Jacopo and Volpi, Riccardo and Vidal, Rene and Murino, Vittorio},
  year = {2017},
  month = aug,
  abstract = {Dropout is a very effective way of regularizing neural networks. Stochastically "dropping out" units with a certain probability discourages over-specific co-adaptations of feature detectors, preventing overfitting and improving network generalization. Besides, Dropout can be interpreted as an approximate model aggregation technique, where an exponential number of smaller networks are averaged in order to get a more powerful ensemble. In this paper, we show that using a fixed dropout probability during training is a suboptimal choice. We thus propose a time scheduling for the probability of retaining neurons in the network. This induces an adaptive regularization scheme that smoothly increases the difficulty of the optimization problem. This idea of "starting easy" and adaptively increasing the difficulty of the learning problem has its roots in curriculum learning and allows one to train better models. Indeed, we prove that our optimization strategy implements a very general curriculum scheme, by gradually adding noise to both the input and intermediate feature representations within the network architecture. Experiments on seven image classification datasets and different network architectures show that our method, named Curriculum Dropout, frequently yields to better generalization and, at worst, performs just as well as the standard Dropout method.},
  archivePrefix = {arXiv},
  eprint = {1703.06229},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZXCHIIVE\\Morerio et al. - 2017 - Curriculum Dropout.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\LF8MMKJN\\1703.html},
  journal = {arXiv:1703.06229 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Statistics - Machine Learning},
  note = {Comment: Accepted at ICCV (International Conference on Computer Vision) 2017},
  primaryClass = {cs, stat}
}

@inproceedings{mosinskaPixelWiseLossTopologyAware2018,
  title = {Beyond the {{Pixel}}-{{Wise Loss}} for {{Topology}}-{{Aware Delineation}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author = {Mosinska, Agata and {Marquez-Neila}, Pablo and Kozinski, Mateusz and Fua, Pascal},
  year = {2018},
  month = jun,
  pages = {3136--3145},
  publisher = {{IEEE}},
  address = {{Salt Lake City, UT, USA}},
  doi = {10.1109/CVPR.2018.00331},
  abstract = {Delineation of curvilinear structures is an important problem in Computer Vision with multiple practical applications. With the advent of Deep Learning, many current approaches on automatic delineation have focused on finding more powerful deep architectures, but have continued using the habitual pixel-wise losses such as binary crossentropy. In this paper we claim that pixel-wise losses alone are unsuitable for this problem because of their inability to reflect the topological impact of mistakes in the final prediction. We propose a new loss term that is aware of the higherorder topological features of linear structures. We also exploit a refinement pipeline that iteratively applies the same model over the previous delineation to refine the predictions at each step, while keeping the number of parameters and the complexity of the model constant.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\KW93JWVQ\\Mosinska et al. - 2018 - Beyond the Pixel-Wise Loss for Topology-Aware Deli.pdf},
  isbn = {978-1-5386-6420-9},
  language = {en}
}

@article{mullerWhenDoesLabel,
  title = {When Does Label Smoothing Help?},
  author = {M{\"u}ller, Rafael and Kornblith, Simon and Hinton, Geoffrey E},
  pages = {10},
  abstract = {The generalization and learning speed of a multi-class neural network can often be significantly improved by using soft targets that are a weighted average of the hard targets and the uniform distribution over labels. Smoothing the labels in this way prevents the network from becoming over-confident and label smoothing has been used in many state-of-the-art models, including image classification, language translation and speech recognition. Despite its widespread use, label smoothing is still poorly understood. Here we show empirically that in addition to improving generalization, label smoothing improves model calibration which can significantly improve beam-search. However, we also observe that if a teacher network is trained with label smoothing, knowledge distillation into a student network is much less effective. To explain these observations, we visualize how label smoothing changes the representations learned by the penultimate layer of the network. We show that label smoothing encourages the representations of training examples from the same class to group in tight clusters. This results in loss of information in the logits about resemblances between instances of different classes, which is necessary for distillation, but does not hurt generalization or calibration of the model's predictions.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\UFUNPRQE\\Müller et al. - When does label smoothing help.pdf},
  keywords = {read-soon},
  language = {en}
}

@article{musgraveMetricLearningReality2020,
  title = {A {{Metric Learning Reality Check}}},
  author = {Musgrave, Kevin and Belongie, Serge and Lim, Ser-Nam},
  year = {2020},
  month = mar,
  abstract = {Deep metric learning papers from the past four years have consistently claimed great advances in accuracy, often more than doubling the performance of decade-old methods. In this paper, we take a closer look at the field to see if this is actually true. We find flaws in the experimental setup of these papers, and propose a new way to evaluate metric learning algorithms. Finally, we present experimental results that show that the improvements over time have been marginal at best.},
  archivePrefix = {arXiv},
  eprint = {2003.08505},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\3ZHRP3XV\\Musgrave et al. - 2020 - A Metric Learning Reality Check.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\GURA6VQ9\\2003.html},
  journal = {arXiv:2003.08505 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-someday},
  primaryClass = {cs}
}

@article{mutschlerParabolicApproximationLine2020,
  title = {Parabolic {{Approximation Line Search}} for {{DNNs}}},
  author = {Mutschler, Maximus and Zell, Andreas},
  year = {2020},
  month = jun,
  abstract = {A major challenge in current optimization research for deep learning is to automatically find optimal step sizes for each update step. The optimal step size is closely related to the shape of the loss in the update step direction. However, this shape has not yet been examined in detail. This work shows empirically that the sample loss over lines in negative gradient direction is mostly convex and well suited for one-dimensional parabolic approximations. Exploiting this parabolic property we introduce a simple and robust line search approach, which performs loss-shape dependent update steps. Our approach combines well-known methods such as parabolic approximation, line search and conjugate gradient, to perform efficiently. It successfully competes with common and state-of-the-art optimization methods on a large variety of experiments without the need of hand-designed step size schedules. Thus, it is of interest for objectives where step-size schedules are unknown or do not perform well. Our excessive evaluation includes multiple comprehensive hyperparameter grid searches on several datasets and architectures. We provide proof of convergence for an adapted scenario. Finally, we give a general investigation of exact line searches in the context of sample losses and exact losses, including their relation to our line search approach.},
  archivePrefix = {arXiv},
  eprint = {1903.11991},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\R9BSCCUQ\\Mutschler and Zell - 2020 - Parabolic Approximation Line Search for DNNs.pdf},
  journal = {arXiv:1903.11991 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,lr,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@misc{MyLibraryZotero,
  title = {My {{Library}} | {{Zotero}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4IE7WGIY\\library.html},
  howpublished = {https://www.zotero.org/jrpedersen/library}
}

@article{nairRectifiedLinearUnits,
  title = {Rectified {{Linear Units Improve Restricted Boltzmann Machines}}},
  author = {Nair, Vinod and Hinton, Geoffrey E},
  pages = {8},
  abstract = {Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these ``Stepped Sigmoid Units'' are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\V4A49Z48\\Nair and Hinton - Rectified Linear Units Improve Restricted Boltzman.pdf},
  keywords = {read-soon},
  language = {en}
}

@article{nakkiranDeepDoubleDescent2019,
  title = {Deep {{Double Descent}}: {{Where Bigger Models}} and {{More Data Hurt}}},
  shorttitle = {Deep {{Double Descent}}},
  author = {Nakkiran, Preetum and Kaplun, Gal and Bansal, Yamini and Yang, Tristan and Barak, Boaz and Sutskever, Ilya},
  year = {2019},
  month = dec,
  abstract = {We show that a variety of modern deep learning tasks exhibit a "double-descent" phenomenon where, as we increase model size, performance first gets worse and then gets better. Moreover, we show that double descent occurs not just as a function of model size, but also as a function of the number of training epochs. We unify the above phenomena by defining a new complexity measure we call the effective model complexity and conjecture a generalized double descent with respect to this measure. Furthermore, our notion of model complexity allows us to identify certain regimes where increasing (even quadrupling) the number of train samples actually hurts test performance.},
  archivePrefix = {arXiv},
  eprint = {1912.02292},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6VAKHPT4\\Nakkiran et al. - 2019 - Deep Double Descent Where Bigger Models and More .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\SMJYJ3U8\\1912.html},
  journal = {arXiv:1912.02292 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-next,Statistics - Machine Learning},
  note = {Comment: G.K. and Y.B. contributed equally},
  primaryClass = {cs, stat}
}

@book{nesterovIntroductoryLecturesConvex2013,
  title = {Introductory Lectures on Convex Optimization: {{A}} Basic Course},
  shorttitle = {Introductory Lectures on Convex Optimization},
  author = {Nesterov, Yurii},
  year = {2013},
  volume = {87},
  publisher = {{Springer Science \& Business Media}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Q86CCNY5\\books.html}
}

@article{nesterovMethodSolvingConvex1983,
  title = {A Method for Solving the Convex Programming Problem with Convergence Rate {{O}}(1/K\^2)},
  author = {NESTEROV, Y. E.},
  year = {1983},
  volume = {269},
  pages = {543--547},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CCJ4VQC5\\10029946121.html},
  journal = {Dokl. Akad. Nauk SSSR}
}

@inproceedings{nesterovMethodSolvingConvex1983a,
  title = {A Method for Solving the Convex Programming Problem with Convergence Rate {{O}} (1/K\^ 2)},
  booktitle = {Dokl. Akad. Nauk {{Sssr}}},
  author = {Nesterov, Yurii E.},
  year = {1983},
  volume = {269},
  pages = {543--547},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GM6GZRYS\\10029946121.html}
}

@misc{nesterovMethodUnconstrainedConvex1983,
  title = {A Method for Unconstrained Convex Minimization Problem with the Rate of Convergence o(1/K\^2)},
  author = {Nesterov, Y.},
  year = {1983},
  abstract = {Semantic Scholar extracted view of \&quot;A method for unconstrained convex minimization problem with the rate of convergence o(1/k\^2)\&quot; by Y. Nesterov},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\W5SMFXT3\\ed910d96802212c9e45d956adaa27d915f5d7469.html},
  howpublished = {/paper/A-method-for-unconstrained-convex-minimization-with-Nesterov/ed910d96802212c9e45d956adaa27d915f5d7469},
  language = {en}
}

@article{neyshaburExploringGeneralizationDeep,
  title = {Exploring {{Generalization}} in {{Deep Learning}}},
  author = {Neyshabur, Behnam and Bhojanapalli, Srinadh and Mcallester, David and Srebro, Nati},
  pages = {10},
  abstract = {With a goal of understanding what drives generalization in deep networks, we consider several recently suggested explanations, including norm-based control, sharpness and robustness. We study how these measures can ensure generalization, highlighting the importance of scale normalization, and making a connection between sharpness and PAC-Bayes theory. We then investigate how well the measures explain different observed phenomena.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RJ7FCTE9\\Neyshabur et al. - Exploring Generalization in Deep Learning.pdf},
  keywords = {generalization},
  language = {en}
}

@article{neyshaburLearningConvolutionsScratch2020,
  title = {Towards {{Learning Convolutions}} from {{Scratch}}},
  author = {Neyshabur, Behnam},
  year = {2020},
  month = jul,
  abstract = {Convolution is one of the most essential components of architectures used in computer vision. As machine learning moves towards reducing the expert bias and learning it from data, a natural next step seems to be learning convolution-like structures from scratch. This, however, has proven elusive. For example, current state-of-the-art architecture search algorithms use convolution as one of the existing modules rather than learning it from data. In an attempt to understand the inductive bias that gives rise to convolutions, we investigate minimum description length as a guiding principle and show that in some settings, it can indeed be indicative of the performance of architectures. To find architectures with small description length, we propose {$\beta$}-lasso, a simple variant of lasso algorithm that, when applied on fully-connected networks for image classification tasks, learns architectures with local connections and achieves state-of-the-art accuracies for training fully-connected nets on CIFAR-10 (85.19\%), CIFAR-100 (59.56\%) and SVHN (94.07\%) bridging the gap between fully-connected and convolutional nets.},
  archivePrefix = {arXiv},
  eprint = {2007.13657},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NSB4DS7F\\Neyshabur - 2020 - Towards Learning Convolutions from Scratch.pdf},
  journal = {arXiv:2007.13657 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  language = {en},
  note = {Comment: 18 pages, 9 figures, 4 tables},
  primaryClass = {cs, stat}
}

@article{neyshaburSearchRealInductive2015,
  title = {In {{Search}} of the {{Real Inductive Bias}}: {{On}} the {{Role}} of {{Implicit Regularization}} in {{Deep Learning}}},
  shorttitle = {In {{Search}} of the {{Real Inductive Bias}}},
  author = {Neyshabur, Behnam and Tomioka, Ryota and Srebro, Nathan},
  year = {2015},
  month = apr,
  abstract = {We present experiments demonstrating that some other form of capacity control, different from network size, plays a central role in learning multilayer feed-forward networks. We argue, partially through analogy to matrix factorization, that this is an inductive bias that can help shed light on deep learning.},
  archivePrefix = {arXiv},
  eprint = {1412.6614},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\8R34RNZY\\Neyshabur et al. - 2015 - In Search of the Real Inductive Bias On the Role .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\XQZWK4G4\\1412.html},
  journal = {arXiv:1412.6614 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,generalization,Statistics - Machine Learning},
  note = {Comment: 9 pages, 2 figures},
  primaryClass = {cs, stat}
}

@inproceedings{nguyenDeepNeuralNetworks2015,
  title = {Deep Neural Networks Are Easily Fooled: {{High}} Confidence Predictions for Unrecognizable Images},
  shorttitle = {Deep Neural Networks Are Easily Fooled},
  booktitle = {Proceedings of the {{IEEE}} Conference on Computer Vision and Pattern Recognition},
  author = {Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  year = {2015},
  pages = {427--436},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\47RQZAHT\\Nguyen et al. - 2015 - Deep neural networks are easily fooled High confi.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\LELNJWZ5\\Nguyen_Deep_Neural_Networks_2015_CVPR_paper.html}
}

@misc{NobelPrizePhysics,
  title = {The {{Nobel Prize}} in {{Physics}} 1901-2000},
  abstract = {The Nobel Prize in Physics 1901-2000},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GKAWAWZA\\the-nobel-prize-in-physics-1901-2000-2.html},
  howpublished = {https://www.nobelprize.org/prizes/uncategorized/the-nobel-prize-in-physics-1901-2000-2},
  journal = {NobelPrize.org},
  language = {en-US}
}

@misc{nov28HalifaxPoliceInvestigating2016,
  title = {Halifax Police Investigating after Needle Found in Potato | {{CBC News}}},
  author = {Nov 28, CBC News {$\cdot$} Posted: and November 28, 2016 7:01 AM AT | Last Updated: and {2016}},
  year = {2016},
  month = nov,
  abstract = {Halifax Regional Police are investigating after they received a call Sunday night that someone found a sewing needle in a batch of leftover potatoes.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\IFI34IS2\\halifax-pei-potato-needle-no-injuries-1.html},
  howpublished = {https://www.cbc.ca/news/canada/nova-scotia/halifax-pei-potato-needle-no-injuries-1.3870465},
  journal = {CBC},
  language = {en}
}

@article{osipovParameterIdentificationMethod2015,
  title = {Parameter Identification Method for Dual-Energy {{X}}-Ray Imaging},
  author = {Osipov, Sergei and Libin, Eduard and Chakhlov, Sergei and Osipov, Oleg and Shtein, Alexander},
  year = {2015},
  month = dec,
  volume = {76},
  pages = {38--42},
  issn = {0963-8695},
  doi = {10.1016/j.ndteint.2015.08.003},
  abstract = {The paper presents a method for parameter identification of dual-energy X-ray imaging. This method is based on pre-calculated or experimentally obtained dependencies between the right sides of the system of two integral parametric equations and two required parameters within the ranges interesting to a customer. This method is characterized by a high processing speed depending on the speed of random access memory. Thus, it is used in different implementations of dual-energy X-ray imaging, namely digital radiography and computed tomography.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\G2IC77EM\\S0963869515000857.html},
  journal = {NDT \& E International},
  keywords = {Digital radiography,Dual-energy X-ray imaging,Effective atomic number,Systems of integral parametric equations,X-ray computed tomography},
  language = {en}
}

@article{osipovParameterIdentificationMethod2015a,
  title = {Parameter Identification Method for Dual-Energy {{X}}-Ray Imaging},
  author = {Osipov, Sergei and Libin, Eduard and Chakhlov, Sergei and Osipov, Oleg and Shtein, Alexander},
  year = {2015},
  month = dec,
  volume = {76},
  pages = {38--42},
  issn = {0963-8695},
  doi = {10.1016/j.ndteint.2015.08.003},
  abstract = {The paper presents a method for parameter identification of dual-energy X-ray imaging. This method is based on pre-calculated or experimentally obtained dependencies between the right sides of the system of two integral parametric equations and two required parameters within the ranges interesting to a customer. This method is characterized by a high processing speed depending on the speed of random access memory. Thus, it is used in different implementations of dual-energy X-ray imaging, namely digital radiography and computed tomography.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6RNSKYFS\\S0963869515000857.html},
  journal = {NDT \& E International},
  keywords = {Digital radiography,Dual-energy X-ray imaging,Effective atomic number,Systems of integral parametric equations,X-ray computed tomography},
  language = {en}
}

@article{osipovParameterIdentificationMethod2015b,
  title = {Parameter Identification Method for Dual-Energy {{X}}-Ray Imaging},
  author = {Osipov, Sergei and Libin, Eduard and Chakhlov, Sergei and Osipov, Oleg and Shtein, Alexander},
  year = {2015},
  month = dec,
  volume = {76},
  pages = {38--42},
  issn = {09638695},
  doi = {10.1016/j.ndteint.2015.08.003},
  abstract = {The paper presents a method for parameter identification of dual-energy X-ray imaging. This method is based on pre-calculated or experimentally obtained dependencies between the right sides of the system of two integral parametric equations and two required parameters within the ranges interesting to a customer. This method is characterized by a high processing speed depending on the speed of random access memory. Thus, it is used in different implementations of dual-energy X-ray imaging, namely digital radiography and computed tomography.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TDNMBJZE\\Osipov et al. - 2015 - Parameter identification method for dual-energy X-.pdf},
  journal = {NDT \& E International},
  keywords = {dual-energy},
  language = {en}
}

@misc{OxfamInternational,
  title = {Oxfam {{International}}},
  abstract = {Oxfam is an international confederation of 20 NGOs working with partners in over 90 countries to end the injustices that cause poverty.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JP3I6WQ7\\en.html},
  howpublished = {https://www.oxfam.org/en},
  journal = {Oxfam International},
  language = {en}
}

@article{pakinComprehensiveLaTeXSymbol,
  title = {The {{Comprehensive LaTeX Symbol List}}},
  author = {Pakin, Scott},
  pages = {348},
  abstract = {This document lists 14599 symbols and the corresponding LATEX commands that produce them. Some of these symbols are guaranteed to be available in every LATEX 2\dbend\dbend\dbend\dbend\dbend\dbend{} system; others require fonts and packages that may not accompany a given distribution and that therefore need to be installed. All of the fonts and packages used to prepare this document\textemdash as well as this document itself\textemdash are freely available from the Comprehensive TEX Archive Network (http://www.ctan.org/).},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\XJN9PIUI\\Pakin - The Comprehensive LaTeX Symbol List.pdf},
  keywords = {latex},
  language = {en}
}

@misc{PapersCodeCOCO,
  title = {Papers with {{Code}} - {{COCO}} Test-Dev {{Benchmark}} ({{Object Detection}})},
  abstract = {The current state-of-the-art on COCO test-dev is CSP-p7 + Mish (multi-scale). See a full comparison of 136 papers with code.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\XFJJH7N5\\object-detection-on-coco.html},
  howpublished = {https://paperswithcode.com/sota/object-detection-on-coco},
  language = {en}
}

@misc{PapersCodeNeural,
  title = {Papers with {{Code}} - {{Neural Controlled Differential Equations}} for {{Irregular Time Series}}},
  abstract = {Implemented in one code library.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4EPHG6DK\\neural-controlled-differential-equations-for.html},
  howpublished = {https://paperswithcode.com/paper/neural-controlled-differential-equations-for},
  language = {en}
}

@article{pathakModelFreePredictionLarge2018,
  title = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}: {{A Reservoir Computing Approach}}},
  shorttitle = {Model-{{Free Prediction}} of {{Large Spatiotemporally Chaotic Systems}} from {{Data}}},
  author = {Pathak, Jaideep and Hunt, Brian and Girvan, Michelle and Lu, Zhixin and Ott, Edward},
  year = {2018},
  month = jan,
  volume = {120},
  pages = {024102},
  publisher = {{American Physical Society}},
  doi = {10.1103/PhysRevLett.120.024102},
  abstract = {We demonstrate the effectiveness of using machine learning for model-free prediction of spatiotemporally chaotic systems of arbitrarily large spatial extent and attractor dimension purely from observations of the system's past evolution. We present a parallel scheme with an example implementation based on the reservoir computing paradigm and demonstrate the scalability of our scheme using the Kuramoto-Sivashinsky equation as an example of a spatiotemporally chaotic system.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FNQRE5CU\\Pathak et al. - 2018 - Model-Free Prediction of Large Spatiotemporally Ch.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\98UJNUIK\\PhysRevLett.120.html},
  journal = {Physical Review Letters},
  number = {2}
}

@article{penningtonUnderstandingImprovingDeep,
  title = {Understanding and {{Improving Deep Learning}} with {{Random Matrix Theory}}},
  author = {Pennington, Jeffrey},
  pages = {42},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Q475JPWE\\Pennington - Understanding and Improving Deep Learning with Ran.pdf},
  keywords = {read-soon},
  language = {en}
}

@incollection{perslevUTimeFullyConvolutional2019,
  title = {U-{{Time}}: {{A Fully Convolutional Network}} for {{Time Series Segmentation Applied}} to {{Sleep Staging}}},
  shorttitle = {U-{{Time}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Perslev, Mathias and Jensen, Michael and Darkner, Sune and rgen Jennum, Poul J{\o} and Igel, Christian},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {4415--4426},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\STCLPWLZ\\Perslev et al. - 2019 - U-Time A Fully Convolutional Network for Time Ser.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\ZWNSESRZ\\8692-u-time-a-fully-convolutional-network-for-time-series-segmentation-applied-to-sleep-staging.html}
}

@article{petersenTopologicalPropertiesSet2020,
  title = {Topological Properties of the Set of Functions Generated by Neural Networks of Fixed Size},
  author = {Petersen, Philipp and Raslan, Mones and Voigtlaender, Felix},
  year = {2020},
  month = jan,
  abstract = {We analyze the topological properties of the set of functions that can be implemented by neural networks of a fixed size. Surprisingly, this set has many undesirable properties. It is highly non-convex, except possibly for a few exotic activation functions. Moreover, the set is not closed with respect to \$L\^p\$-norms, \$0 {$<$} p {$<$} \textbackslash infty\$, for all practically-used activation functions, and also not closed with respect to the \$L\^\textbackslash infty\$-norm for all practically-used activation functions except for the ReLU and the parametric ReLU. Finally, the function that maps a family of weights to the function computed by the associated network is not inverse stable for every practically used activation function. In other words, if \$f\_1, f\_2\$ are two functions realized by neural networks and if \$f\_1, f\_2\$ are close in the sense that \$\textbackslash |f\_1 - f\_2\textbackslash |\_\{L\^\textbackslash infty\} \textbackslash leq \textbackslash varepsilon\$ for \$\textbackslash varepsilon {$>$} 0\$, it is, regardless of the size of \$\textbackslash varepsilon\$, usually not possible to find weights \$w\_1, w\_2\$ close together such that each \$f\_i\$ is realized by a neural network with weights \$w\_i\$. Overall, our findings identify potential causes for issues in the training procedure of deep learning such as no guaranteed convergence, explosion of parameters, and slow convergence.},
  archivePrefix = {arXiv},
  eprint = {1806.08459},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\A427YIK4\\Petersen et al. - 2020 - Topological properties of the set of functions gen.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\TQ85GVAK\\1806.html},
  journal = {arXiv:1806.08459 [math]},
  keywords = {54H99; 68T05; 52A30,Mathematics - Functional Analysis,Mathematics - General Topology},
  primaryClass = {math}
}

@article{petersenTopologicalPropertiesSet2020a,
  title = {Topological Properties of the Set of Functions Generated by Neural Networks of Fixed Size},
  author = {Petersen, Philipp and Raslan, Mones and Voigtlaender, Felix},
  year = {2020},
  month = jan,
  abstract = {We analyze the topological properties of the set of functions that can be implemented by neural networks of a fixed size. Surprisingly, this set has many undesirable properties. It is highly non-convex, except possibly for a few exotic activation functions. Moreover, the set is not closed with respect to \$L\^p\$-norms, \$0 {$<$} p {$<$} \textbackslash infty\$, for all practically-used activation functions, and also not closed with respect to the \$L\^\textbackslash infty\$-norm for all practically-used activation functions except for the ReLU and the parametric ReLU. Finally, the function that maps a family of weights to the function computed by the associated network is not inverse stable for every practically used activation function. In other words, if \$f\_1, f\_2\$ are two functions realized by neural networks and if \$f\_1, f\_2\$ are close in the sense that \$\textbackslash |f\_1 - f\_2\textbackslash |\_\{L\^\textbackslash infty\} \textbackslash leq \textbackslash varepsilon\$ for \$\textbackslash varepsilon {$>$} 0\$, it is, regardless of the size of \$\textbackslash varepsilon\$, usually not possible to find weights \$w\_1, w\_2\$ close together such that each \$f\_i\$ is realized by a neural network with weights \$w\_i\$. Overall, our findings identify potential causes for issues in the training procedure of deep learning such as no guaranteed convergence, explosion of parameters, and slow convergence.},
  archivePrefix = {arXiv},
  eprint = {1806.08459},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\5VXGLZSU\\Petersen et al. - 2020 - Topological properties of the set of functions gen.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\SVSLQYBA\\1806.html},
  journal = {arXiv:1806.08459 [math]},
  keywords = {54H99; 68T05; 52A30,Mathematics - Functional Analysis,Mathematics - General Topology},
  primaryClass = {math}
}

@article{piekniewskiUnsupervisedLearningContinuous2016,
  title = {Unsupervised {{Learning}} from {{Continuous Video}} in a {{Scalable Predictive Recurrent Network}}},
  author = {Piekniewski, Filip and Laurent, Patryk and Petre, Csaba and Richert, Micah and Fisher, Dimitry and Hylton, Todd},
  year = {2016},
  month = sep,
  abstract = {Understanding visual reality involves acquiring common-sense knowledge about countless regularities in the visual world, e.g., how illumination alters the appearance of objects in a scene, and how motion changes their apparent spatial relationship. These regularities are hard to label for training supervised machine learning algorithms; consequently, algorithms need to learn these regularities from the real world in an unsupervised way. We present a novel network meta-architecture that can learn world dynamics from raw, continuous video. The components of this network can be implemented using any algorithm that possesses three key capabilities: prediction of a signal over time, reduction of signal dimensionality (compression), and the ability to use supplementary contextual information to inform the prediction. The presented architecture is highly-parallelized and scalable, and is implemented using localized connectivity, processing, and learning. We demonstrate an implementation of this architecture where the components are built from multi-layer perceptrons. We apply the implementation to create a system capable of stable and robust visual tracking of objects as seen by a moving camera. Results show performance on par with or exceeding state-of-the-art tracking algorithms. The tracker can be trained in either fully supervised or unsupervisedthen-briefly-supervised regimes. Success of the briefly-supervised regime suggests that the unsupervised portion of the model extracts useful information about visual reality. The results suggest a new class of AI algorithms that uniquely combine prediction and scalability in a way that makes them suitable for learning from and \textemdash{} and eventually acting within \textemdash{} the real world.},
  archivePrefix = {arXiv},
  eprint = {1607.06854},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FCYJ32S4\\Piekniewski et al. - 2016 - Unsupervised Learning from Continuous Video in a S.pdf},
  journal = {arXiv:1607.06854 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-maybe},
  language = {en},
  note = {Comment: 38 pages, 20 figures, v3. Added several citations to relevant papers, expanded the discussion of existing approach in deep learning},
  primaryClass = {cs}
}

@article{podareanuBestPracticeGuide,
  title = {Best {{Practice Guide}} - {{Deep Learning}}},
  author = {Podareanu, Damian and Codreanu, Valeriu and Aigner, Sandra and Leeuwen, C. V. and Weinberg, Volker},
  pages = {51},
  abstract = {Artificial neural networks (ANNs) are a class of machine learning models that are loosely inspired by the biologicalneural networks that constitute animal brains. Artificial neural networks use multiple layers of nonlinear processingunits for feature extraction and transformation. This allows models to represent multiple levels of abstraction fromthe data: an approach that works well for many types of problems, such as image, sound, and text analysis. Neuralnetworks with many layers are known as deep neural networks, and the process of training such networks is knownas `deep learning'.Sparked by various inference and prediction challenges on publicly available large datasets (such as MS-COCO[1],  ImageNet,  Open  Images,  Yelp  Reviews,  the  Wikipedia  Corpus,  WMT,  Merk  Molecular  Activity,  MillionSongs and FMA) and by having available open-source frameworks (such as TensorFlow, Caffe and PyTorch), thedeep learning field has evolved rapidly over the past decade. With more complex neural networks and larger inputdata sets, the scalability of deep learning algorithms is an increasingly important topic.The main aim of this guide is to teach you how to perform deep learning at large scale. In the process, differentalgorithms, software frameworks and hardware platforms will be discussed. This should help you pick the mostsuitable framework and hardware platform for your deep learning problem. However, note that this guide onlygives a broad overview and does not aim to replace software framework manuals or a deep learning course.The guide is structured in five chapters: Hardware, Algorithms, HPC and scaling, Frameworks and Use cases. Theseparation into these different chapters is occasionally difficult, as the concepts are closely related: the type offramework you choose may be dependent upon the hardware you want to run on, which algorithms it supports,etc. We recommend reading the full guide if you want to get a complete picture.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\AL2FHCJU\\Podareanu - Best Practice Guide - Deep Learning.pdf},
  journal = {Deep Learning},
  language = {en}
}

@misc{podareanuBestPracticeGuideDeep2019,
  title = {Best {{Practice Guide}}-{{Deep Learning}}},
  author = {Podareanu, Damian and Codreanu, V. and Aigner, Sandra and Leeuwen, C. V.},
  year = {2019},
  abstract = {Semantic Scholar extracted view of \&quot;Best Practice Guide-Deep Learning\&quot; by Damian Podareanu et al.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\XBXKGPE2\\6c00d026e1048ed27e4ff66e8a287baa8febdfbf.html},
  howpublished = {/paper/Best-Practice-Guide-Deep-Learning-Podareanu-Codreanu/6c00d026e1048ed27e4ff66e8a287baa8febdfbf},
  language = {en}
}

@article{polyakMethodsSpeedingConvergence1964,
  title = {Some Methods of Speeding up the Convergence of Iteration Methods},
  author = {Polyak, B. T.},
  year = {1964},
  month = jan,
  volume = {4},
  pages = {1--17},
  issn = {0041-5553},
  doi = {10.1016/0041-5553(64)90137-5},
  abstract = {For the solution of the functional equation P (x) = 0 (1) (where P is an operator, usually linear, from B into B, and B is a Banach space) iteration methods are generally used. These consist of the construction of a series x0, \ldots, xn, \ldots, which converges to the solution (see, for example [1]). Continuous analogues of these methods are also known, in which a trajectory x(t), 0 {$\leqslant$} t {$\leqslant$} {$\infty$} is constructed, which satisfies the ordinary differential equation in B and is such that x(t) approaches the solution of (1) as t \textrightarrow{} {$\infty$} (see [2]). We shall call the method a k-step method if for the construction of each successive iteration xn+1 we use k previous iterations xn, \ldots, xn-k+1. The same term will also be used for continuous methods if x(t) satisfies a differential equation of the k-th order or k-th degree. Iteration methods which are more widely used are one-step (e.g. methods of successive approximations). They are generally simple from the calculation point of view but often converge very slowly. This is confirmed both by the evaluation of the speed of convergence and by calculation in practice (for more details see below). Therefore the question of the rate of convergence is most important. Some multistep methods, which we shall consider further, which are only slightly more complicated than the corresponding one-step methods, make it possible to speed up the convergence substantially. Note that all the methods mentioned below are applicable also to the problem of minimizing the differentiable functional (x) in Hilbert space, so long as this problem reduces to the solution of the equation grad (x) = 0.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\C7T86SX6\\0041555364901375.html},
  journal = {USSR Computational Mathematics and Mathematical Physics},
  keywords = {lr},
  language = {en},
  number = {5}
}

@misc{pressElementsCausalInference,
  title = {Elements of {{Causal Inference}} | {{The MIT Press}}},
  author = {Press, The MIT},
  publisher = {{The MIT Press}},
  abstract = {A concise and self-contained introduction to causal inference, increasingly important in data science and machine learning.                 The mathematization of causality is a relatively recent development, and has become increasingly important in data science and machine learning. This book offers a self-contained and concise introduction to causal models and how to learn them from data.                     After explaining the need for causal models and discussing some of the principles underlying causal inference, the book teaches readers how to use causal models: how to compute intervention distributions, how to infer causal models from observational and interventional data, and how causal ideas could be exploited for classical machine learning problems. All of these topics are discussed first in terms of two variables and then in the more general multivariate case. The bivariate case turns out to be a particularly hard problem for causal learning because there are no conditional independences as used by classical methods for solving multivariate cases. The authors consider analyzing statistical asymmetries between cause and effect to be highly instructive, and they report on their decade of intensive research into this problem. The book is accessible to readers with a background in machine learning or statistics, and can be used in graduate courses or as a reference for researchers. The text includes code snippets that can be copied and pasted, exercises, and an appendix with a summary of the most important technical concepts.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CRLSTZ3Q\\elements-causal-inference.html},
  howpublished = {https://mitpress.mit.edu/books/elements-causal-inference},
  language = {en}
}

@misc{PythonCreateRandom,
  title = {Python - {{Create}} Random Shape/Contour Using Matplotlib},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\87ZAVB47\\create-random-shape-contour-using-matplotlib.html;C\:\\Users\\ext1150\\Zotero\\storage\\GC4ETU5U\\create-random-shape-contour-using-matplotlib.html},
  howpublished = {https://stackoverflow.com/questions/50731785/create-random-shape-contour-using-matplotlib},
  journal = {Stack Overflow},
  keywords = {create_shapes},
  note = {\href{https://stackoverflow.com/users/4124317/importanceofbeingernest}{ImportanceOfBeingErnest}}
}

@misc{PyTorchLightningPytorchlightning2020,
  title = {{{PyTorchLightning}}/Pytorch-Lightning},
  year = {2020},
  month = sep,
  abstract = {The lightweight PyTorch wrapper for high-performance AI research. Scale your models, not the boilerplate.},
  copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
  howpublished = {Pytorch Lightning},
  keywords = {ai,artificial-intelligence,data-science,deep-learning,machine-learning,python,pytorch}
}

@article{qianMomentumTermGradient1999,
  title = {On the Momentum Term in Gradient Descent Learning Algorithms},
  author = {Qian, Ning},
  year = {1999},
  month = jan,
  volume = {12},
  pages = {145--151},
  issn = {0893-6080},
  doi = {10.1016/S0893-6080(98)00116-6},
  abstract = {A momentum term is usually included in the simulations of connectionist learning algorithms. Although it is well known that such a term greatly improves the speed of learning, there have been few rigorous studies of its mechanisms. In this paper, I show that in the limit of continuous time, the momentum parameter is analogous to the mass of Newtonian particles that move through a viscous medium in a conservative force field. The behavior of the system near a local minimum is equivalent to a set of coupled and damped harmonic oscillators. The momentum term improves the speed of convergence by bringing some eigen components of the system closer to critical damping. Similar results can be obtained for the discrete time case used in computer simulations. In particular, I derive the bounds for convergence on learning-rate and momentum parameters, and demonstrate that the momentum term can increase the range of learning rate over which the system converges. The optimal condition for convergence is also analyzed.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VLZNGYBN\\Qian - 1999 - On the momentum term in gradient descent learning .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\JD8J6Q43\\S0893608098001166.html},
  journal = {Neural Networks},
  keywords = {Critical damping,Damped harmonic oscillator,Gradient descent learning algorithm,Learning rate,lr,Momentum,read-next,Speed of convergence},
  language = {en},
  number = {1}
}

@article{qiaoWeightStandardization2019,
  title = {Weight {{Standardization}}},
  author = {Qiao, Siyuan and Wang, Huiyu and Liu, Chenxi and Shen, Wei and Yuille, Alan},
  year = {2019},
  month = mar,
  abstract = {In this paper, we propose Weight Standardization (WS) to accelerate deep network training. WS is targeted at the micro-batch training setting where each GPU typically has only 1-2 images for training. The micro-batch training setting is hard because small batch sizes are not enough for training networks with Batch Normalization (BN), while other normalization methods that do not rely on batch knowledge still have difficulty matching the performances of BN in large-batch training. Our WS ends this problem because when used with Group Normalization and trained with 1 image/GPU, WS is able to match or outperform the performances of BN trained with large batch sizes with only 2 more lines of code. In micro-batch training, WS significantly outperforms other normalization methods. WS achieves these superior results by standardizing the weights in the convolutional layers, which we show is able to smooth the loss landscape by reducing the Lipschitz constants of the loss and the gradients. The effectiveness of WS is verified on many tasks, including image classification, object detection, instance segmentation, video recognition, semantic segmentation, and point cloud recognition. The code is available here: https://github.com/joe-siyuan-qiao/WeightStandardization.},
  archivePrefix = {arXiv},
  eprint = {1903.10520},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LRGGGYVR\\Qiao et al. - 2019 - Weight Standardization.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\WHPEM6NK\\1903.html},
  journal = {arXiv:1903.10520 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@misc{QuantaMagazineIlluminating,
  title = {Quanta {{Magazine}} - {{Illuminating Science}}},
  abstract = {Illuminating mathematics, physics, biology and computer science research through public service journalism.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\QR9UF587\\how-godels-incompleteness-theorems-work-20200714.html},
  howpublished = {https://www.quantamagazine.org/},
  journal = {Quanta Magazine},
  language = {en}
}

@article{radosavovicDesigningNetworkDesign2020,
  title = {Designing {{Network Design Spaces}}},
  author = {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Doll{\'a}r, Piotr},
  year = {2020},
  month = mar,
  abstract = {In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5\texttimes{} faster on GPUs.},
  archivePrefix = {arXiv},
  eprint = {2003.13678},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SPA99VXE\\Radosavovic et al. - 2020 - Designing Network Design Spaces.pdf},
  journal = {arXiv:2003.13678 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  note = {Comment: CVPR 2020},
  primaryClass = {cs}
}

@inproceedings{radosavovicDesigningNetworkDesign2020a,
  title = {Designing {{Network Design Spaces}}},
  booktitle = {2020 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Radosavovic, Ilija and Kosaraju, Raj Prateek and Girshick, Ross and He, Kaiming and Dollar, Piotr},
  year = {2020},
  month = jun,
  pages = {10425--10433},
  publisher = {{IEEE}},
  address = {{Seattle, WA, USA}},
  doi = {10.1109/CVPR42600.2020.01044},
  abstract = {In this work, we present a new network design paradigm. Our goal is to help advance the understanding of network design and discover design principles that generalize across settings. Instead of focusing on designing individual network instances, we design network design spaces that parametrize populations of networks. The overall process is analogous to classic manual design of networks, but elevated to the design space level. Using our methodology we explore the structure aspect of network design and arrive at a low-dimensional design space consisting of simple, regular networks that we call RegNet. The core insight of the RegNet parametrization is surprisingly simple: widths and depths of good networks can be explained by a quantized linear function. We analyze the RegNet design space and arrive at interesting findings that do not match the current practice of network design. The RegNet design space provides simple and fast networks that work well across a wide range of flop regimes. Under comparable training settings and flops, the RegNet models outperform the popular EfficientNet models while being up to 5\texttimes{} faster on GPUs.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\F8SUPCDT\\Radosavovic et al. - 2020 - Designing Network Design Spaces.pdf},
  isbn = {978-1-72817-168-5},
  keywords = {read-next},
  language = {en}
}

@article{radosavovicNetworkDesignSpaces2019a,
  title = {On {{Network Design Spaces}} for {{Visual Recognition}}},
  author = {Radosavovic, Ilija and Johnson, Justin and Xie, Saining and Lo, Wan-Yen and Doll{\'a}r, Piotr},
  year = {2019},
  month = may,
  abstract = {Over the past several years progress in designing better neural network architectures for visual recognition has been substantial. To help sustain this rate of progress, in this work we propose to reexamine the methodology for comparing network architectures. In particular, we introduce a new comparison paradigm of distribution estimates, in which network design spaces are compared by applying statistical techniques to populations of sampled models, while controlling for confounding factors like network complexity. Compared to current methodologies of comparing point and curve estimates of model families, distribution estimates paint a more complete picture of the entire design landscape. As a case study, we examine design spaces used in neural architecture search (NAS). We find significant statistical differences between recent NAS design space variants that have been largely overlooked. Furthermore, our analysis reveals that the design spaces for standard model families like ResNeXt can be comparable to the more complex ones used in recent NAS work. We hope these insights into distribution analysis will enable more robust progress toward discovering better networks for visual recognition.},
  archivePrefix = {arXiv},
  eprint = {1905.13214},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\EQZ7JHEU\\Radosavovic et al. - 2019 - On Network Design Spaces for Visual Recognition.pdf},
  journal = {arXiv:1905.13214 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  language = {en},
  note = {Comment: tech report},
  primaryClass = {cs}
}

@article{rakhlinMakingGradientDescent2012,
  title = {Making {{Gradient Descent Optimal}} for {{Strongly Convex Stochastic Optimization}}},
  author = {Rakhlin, Alexander and Shamir, Ohad and Sridharan, Karthik},
  year = {2012},
  month = dec,
  abstract = {Stochastic gradient descent (SGD) is a simple and popular method to solve stochastic optimization problems which arise in machine learning. For strongly convex problems, its convergence rate was known to be O(\textbackslash log(T)/T), by running SGD for T iterations and returning the average point. However, recent results showed that using a different algorithm, one can get an optimal O(1/T) rate. This might lead one to believe that standard SGD is suboptimal, and maybe should even be replaced as a method of choice. In this paper, we investigate the optimality of SGD in a stochastic setting. We show that for smooth problems, the algorithm attains the optimal O(1/T) rate. However, for non-smooth problems, the convergence rate with averaging might really be \textbackslash Omega(\textbackslash log(T)/T), and this is not just an artifact of the analysis. On the flip side, we show that a simple modification of the averaging step suffices to recover the O(1/T) rate, and no other change of the algorithm is necessary. We also present experimental results which support our findings, and point out open problems.},
  archivePrefix = {arXiv},
  eprint = {1109.5647},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LI2CJIWN\\Rakhlin et al. - 2012 - Making Gradient Descent Optimal for Strongly Conve.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\CQXHT7L9\\1109.html},
  journal = {arXiv:1109.5647 [cs, math]},
  keywords = {Computer Science - Machine Learning,Mathematics - Optimization and Control,read-next},
  note = {Comment: Updated version which fixes a bug in the proof of lemma 1 and modifies the step size choice. As a result, constants are changed throughout the paper},
  primaryClass = {cs, math}
}

@article{ramachandranSearchingActivationFunctions2017,
  title = {Searching for {{Activation Functions}}},
  author = {Ramachandran, Prajit and Zoph, Barret and Le, Quoc V.},
  year = {2017},
  month = oct,
  abstract = {The choice of activation functions in deep networks has a significant effect on the training dynamics and task performance. Currently, the most successful and widely-used activation function is the Rectified Linear Unit (ReLU). Although various hand-designed alternatives to ReLU have been proposed, none have managed to replace it due to inconsistent gains. In this work, we propose to leverage automatic search techniques to discover new activation functions. Using a combination of exhaustive and reinforcement learning-based search, we discover multiple novel activation functions. We verify the effectiveness of the searches by conducting an empirical evaluation with the best discovered activation function. Our experiments show that the best discovered activation function, \$f(x) = x \textbackslash cdot \textbackslash text\{sigmoid\}(\textbackslash beta x)\$, which we name Swish, tends to work better than ReLU on deeper models across a number of challenging datasets. For example, simply replacing ReLUs with Swish units improves top-1 classification accuracy on ImageNet by 0.9\textbackslash\% for Mobile NASNet-A and 0.6\textbackslash\% for Inception-ResNet-v2. The simplicity of Swish and its similarity to ReLU make it easy for practitioners to replace ReLUs with Swish units in any neural network.},
  archivePrefix = {arXiv},
  eprint = {1710.05941},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7KYZI3PP\\Ramachandran et al. - 2017 - Searching for Activation Functions.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\2RUN4IFC\\1710.html},
  journal = {arXiv:1710.05941 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-half,read-soon},
  note = {Comment: Updated version of "Swish: a Self-Gated Activation Function"},
  primaryClass = {cs}
}

@misc{RandomSearchHyperparameter,
  title = {Random Search for Hyper-Parameter Optimization | {{The Journal}} of {{Machine Learning Research}}},
  howpublished = {https://dl.acm.org/doi/10.5555/2188385.2188395}
}

@article{raschkaModelEvaluationModel2018,
  title = {Model {{Evaluation}}, {{Model Selection}}, and {{Algorithm Selection}} in {{Machine Learning}}},
  author = {Raschka, Sebastian},
  year = {2018},
  month = dec,
  abstract = {The correct use of model evaluation, model selection, and algorithm selection techniques is vital in academic machine learning research as well as in many industrial settings. This article reviews different techniques that can be used for each of these three subtasks and discusses the main advantages and disadvantages of each technique with references to theoretical and empirical studies. Further, recommendations are given to encourage best yet feasible practices in research and applications of machine learning. Common methods such as the holdout method for model evaluation and selection are covered, which are not recommended when working with small datasets. Different flavors of the bootstrap technique are introduced for estimating the uncertainty of performance estimates, as an alternative to confidence intervals via normal approximation if bootstrapping is computationally feasible. Common cross-validation techniques such as leave-oneout cross-validation and k-fold cross-validation are reviewed, the bias-variance trade-off for choosing k is discussed, and practical tips for the optimal choice of k are given based on empirical evidence. Different statistical tests for algorithm comparisons are presented, and strategies for dealing with multiple comparisons such as omnibus tests and multiple-comparison corrections are discussed. Finally, alternative methods for algorithm selection, such as the combined F -test 5x2 crossvalidation and nested cross-validation, are recommended for comparing machine learning algorithms when datasets are small.},
  archivePrefix = {arXiv},
  eprint = {1811.12808},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\R2FEZSEV\\Raschka - 2018 - Model Evaluation, Model Selection, and Algorithm S.pdf},
  journal = {arXiv:1811.12808 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-soon,Statistics - Machine Learning},
  language = {en},
  note = {Comment: v2: minor typo fixes},
  primaryClass = {cs, stat}
}

@article{rebuffelDualenergyXrayImaging2007,
  title = {Dual-Energy {{X}}-Ray Imaging: Benefits and Limits},
  shorttitle = {Dual-Energy {{X}}-Ray Imaging},
  author = {Rebuffel, V and Dinten, J-M},
  year = {2007},
  month = oct,
  volume = {49},
  pages = {589--594},
  issn = {1354-2575},
  doi = {10.1784/insi.2007.49.10.589},
  abstract = {Both in radiographic and tomographic mode, conventional X-ray imaging provides information about the examined object which is not sufficient to characterize it precisely. Dual-energy X-ray technique, which consists in combining two radiographs acquired at two distinct energies, allows to obtain both density and atomic number, thus to provide information about material composition, or at least to improve image contrast. Available systems usually perform energetic separation at source level, but separation at detector level is also possible for linear detectors, especially those devoted to translating objects control.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FRGEZGWT\\Rebuffel and Dinten - 2007 - Dual-energy X-ray imaging benefits and limits.pdf},
  journal = {Insight - Non-Destructive Testing and Condition Monitoring},
  keywords = {dual-energy},
  language = {en},
  number = {10}
}

@misc{RecipeTrainingNeural,
  title = {A {{Recipe}} for {{Training Neural Networks}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\S2K4TGAC\\recipe.html},
  howpublished = {http://karpathy.github.io/2019/04/25/recipe/}
}

@article{reddiCONVERGENCEADAM2018,
  title = {{{ON THE CONVERGENCE OF ADAM AND BEYOND}}},
  author = {Reddi, Sashank J and Kale, Satyen and Kumar, Sanjiv},
  year = {2018},
  pages = {23},
  abstract = {Several recently proposed stochastic optimization methods that have been successfully used in training deep networks such as RMSPROP, ADAM, ADADELTA, NADAM are based on using gradient updates scaled by square roots of exponential moving averages of squared past gradients. In many applications, e.g. learning with large output spaces, it has been empirically observed that these algorithms fail to converge to an optimal solution (or a critical point in nonconvex settings). We show that one cause for such failures is the exponential moving average used in the algorithms. We provide an explicit example of a simple convex optimization setting where ADAM does not converge to the optimal solution, and describe the precise problems with the previous analysis of ADAM algorithm. Our analysis suggests that the convergence issues can be fixed by endowing such algorithms with ``long-term memory'' of past gradients, and propose new variants of the ADAM algorithm which not only fix the convergence issues but often also lead to improved empirical performance.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SKCT2K7G\\Reddi et al. - 2018 - ON THE CONVERGENCE OF ADAM AND BEYOND.pdf},
  keywords = {lr,read},
  language = {en}
}

@article{redmonYouOnlyLook2016,
  title = {You {{Only Look Once}}: {{Unified}}, {{Real}}-{{Time Object Detection}}},
  shorttitle = {You {{Only Look Once}}},
  author = {Redmon, Joseph and Divvala, Santosh and Girshick, Ross and Farhadi, Ali},
  year = {2016},
  month = may,
  abstract = {We present YOLO, a new approach to object detection. Prior work on object detection repurposes classifiers to perform detection. Instead, we frame object detection as a regression problem to spatially separated bounding boxes and associated class probabilities. A single neural network predicts bounding boxes and class probabilities directly from full images in one evaluation. Since the whole detection pipeline is a single network, it can be optimized end-to-end directly on detection performance. Our unified architecture is extremely fast. Our base YOLO model processes images in real-time at 45 frames per second. A smaller version of the network, Fast YOLO, processes an astounding 155 frames per second while still achieving double the mAP of other real-time detectors. Compared to state-of-the-art detection systems, YOLO makes more localization errors but is far less likely to predict false detections where nothing exists. Finally, YOLO learns very general representations of objects. It outperforms all other detection methods, including DPM and R-CNN, by a wide margin when generalizing from natural images to artwork on both the Picasso Dataset and the People-Art Dataset.},
  archivePrefix = {arXiv},
  eprint = {1506.02640},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JK35L6RM\\Redmon et al. - 2016 - You Only Look Once Unified, Real-Time Object Dete.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\ZZCC9K3T\\1506.html},
  journal = {arXiv:1506.02640 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{rendleDifficultyEvaluatingBaselines2019,
  title = {On the {{Difficulty}} of {{Evaluating Baselines}}: {{A Study}} on {{Recommender Systems}}},
  shorttitle = {On the {{Difficulty}} of {{Evaluating Baselines}}},
  author = {Rendle, Steffen and Zhang, Li and Koren, Yehuda},
  year = {2019},
  month = may,
  abstract = {Numerical evaluations with comparisons to baselines play a central role when judging research in recommender systems. In this paper, we show that running baselines properly is difficult. We demonstrate this issue on two extensively studied datasets. First, we show that results for baselines that have been used in numerous publications over the past five years for the Movielens 10M benchmark are suboptimal. With a careful setup of a vanilla matrix factorization baseline, we are not only able to improve upon the reported results for this baseline but even outperform the reported results of any newly proposed method. Secondly, we recap the tremendous effort that was required by the community to obtain high quality results for simple methods on the Netflix Prize. Our results indicate that empirical findings in research papers are questionable unless they were obtained on standardized benchmarks where baselines have been tuned extensively by the research community.},
  archivePrefix = {arXiv},
  eprint = {1905.01395},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TGS6H4GJ\\Rendle et al. - 2019 - On the Difficulty of Evaluating Baselines A Study.pdf},
  journal = {arXiv:1905.01395 [cs]},
  keywords = {Computer Science - Information Retrieval,Computer Science - Machine Learning},
  language = {en},
  primaryClass = {cs}
}

@article{renFasterRCNNRealTime2017,
  title = {Faster {{R}}-{{CNN}}: {{Towards Real}}-{{Time Object Detection}} with {{Region Proposal Networks}}},
  shorttitle = {Faster {{R}}-{{CNN}}},
  author = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
  year = {2017},
  month = jun,
  volume = {39},
  pages = {1137--1149},
  issn = {0162-8828, 2160-9292},
  doi = {10.1109/TPAMI.2016.2577031},
  abstract = {State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet [7] and Fast R-CNN [5] have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully-convolutional network that simultaneously predicts object bounds and objectness scores at each position. RPNs are trained end-to-end to generate highquality region proposals, which are used by Fast R-CNN for detection. With a simple alternating optimization, RPN and Fast R-CNN can be trained to share convolutional features. For the very deep VGG-16 model [19], our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007 (73.2\% mAP) and 2012 (70.4\% mAP) using 300 proposals per image. Code is available at https://github.com/ShaoqingRen/faster\_rcnn.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\5Q5L3UPE\\Ren et al. - 2017 - Faster R-CNN Towards Real-Time Object Detection w.pdf},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  keywords = {read-soon},
  language = {en},
  number = {6}
}

@article{renGALGlobalAttributesAssisted2016,
  ids = {renGALGlobalAttributesAssisted2016a},
  title = {{{GAL}}: {{A Global}}-{{Attributes Assisted Labeling System}} for {{Outdoor Scenes}}},
  shorttitle = {{{GAL}}},
  author = {Ren, Yuzhuo and Chen, Chen and Li, Shangwen and Kuo, C.-C. Jay},
  year = {2016},
  month = apr,
  abstract = {An approach that extracts global attributes from outdoor images to facilitate geometric layout labeling is investigated in this work. The proposed Global-attributes Assisted Labeling (GAL) system exploits both local features and global attributes. First, by following a classical method, we use local features to provide initial labels for all super-pixels. Then, we develop a set of techniques to extract global attributes from 2D outdoor images. They include sky lines, ground lines, vanishing lines, etc. Finally, we propose the GAL system that integrates global attributes in the conditional random field (CRF) framework to improve initial labels so as to offer a more robust labeling result. The performance of the proposed GAL system is demonstrated and benchmarked with several state-of-the-art algorithms against a popular outdoor scene layout dataset.},
  archivePrefix = {arXiv},
  eprint = {1604.00606},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6J7MP9VI\\Ren et al. - 2016 - GAL A Global-Attributes Assisted Labeling System .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\MFYNXQEB\\Ren et al. - 2016 - GAL A Global-Attributes Assisted Labeling System .pdf},
  journal = {arXiv:1604.00606 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  language = {en},
  primaryClass = {cs}
}

@article{ridnikTResNetHighPerformance2020,
  title = {{{TResNet}}: {{High Performance GPU}}-{{Dedicated Architecture}}},
  shorttitle = {{{TResNet}}},
  author = {Ridnik, Tal and Lawen, Hussam and Noy, Asaf and Friedman, Itamar and Baruch, Emanuel Ben and Sharir, Gilad},
  year = {2020},
  month = jun,
  abstract = {Many deep learning models, developed in recent years, reach higher ImageNet accuracy than ResNet50, with fewer or comparable FLOPS count. While FLOPs are often seen as a proxy for network efficiency, when measuring actual GPU training and inference throughput, vanilla ResNet50 is usually significantly faster than its recent competitors, offering better throughput-accuracy trade-off.},
  archivePrefix = {arXiv},
  eprint = {2003.13630},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NHJMHBJ8\\Ridnik et al. - 2020 - TResNet High Performance GPU-Dedicated Architectu.pdf},
  journal = {arXiv:2003.13630 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,read-next},
  language = {en},
  note = {Comment: 11 pages, 5 figures},
  primaryClass = {cs, eess}
}

@article{robbinsStochasticApproximationMethod2007,
  title = {A {{Stochastic Approximation Method}}},
  author = {Robbins, H.},
  year = {2007},
  doi = {10.1214/aoms/1177729586},
  abstract = {Let M(x) denote the expected value at level x of the response to a certain experiment. M(x) is assumed to be a monotone function of x but is unknown tot he experiment, and it is desire to find the solution x=0 of the equation M(x) = a, where x is a given constant. we give a method for making successive experiments at levels x1, x2,... in such a way that x, will tend to 0 in probability.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\UHV9AT3B\\Robbins - 2007 - A Stochastic Approximation Method.pdf},
  keywords = {lr,SGD citation}
}

@article{ronnebergerUNetConvolutionalNetworks2015,
  title = {U-{{Net}}: {{Convolutional Networks}} for {{Biomedical Image Segmentation}}},
  shorttitle = {U-{{Net}}},
  author = {Ronneberger, Olaf and Fischer, Philipp and Brox, Thomas},
  year = {2015},
  month = may,
  abstract = {There is large consent that successful training of deep networks requires many thousand annotated training samples. In this paper, we present a network and training strategy that relies on the strong use of data augmentation to use the available annotated samples more efficiently. The architecture consists of a contracting path to capture context and a symmetric expanding path that enables precise localization. We show that such a network can be trained end-to-end from very few images and outperforms the prior best method (a sliding-window convolutional network) on the ISBI challenge for segmentation of neuronal structures in electron microscopic stacks. Using the same network trained on transmitted light microscopy images (phase contrast and DIC) we won the ISBI cell tracking challenge 2015 in these categories by a large margin. Moreover, the network is fast. Segmentation of a 512x512 image takes less than a second on a recent GPU. The full implementation (based on Caffe) and the trained networks are available at http://lmb.informatik.uni-freiburg.de/people/ronneber/u-net .},
  archivePrefix = {arXiv},
  eprint = {1505.04597},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ZWEN3IEN\\Ronneberger et al. - 2015 - U-Net Convolutional Networks for Biomedical Image.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\QYE4ZI78\\1505.html},
  journal = {arXiv:1505.04597 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  note = {Comment: conditionally accepted at MICCAI 2015},
  primaryClass = {cs}
}

@article{rosenfeldElephantRoom2018,
  title = {The Elephant in the Room},
  author = {Rosenfeld, Amir and Zemel, Richard and Tsotsos, John K.},
  year = {2018},
  archivePrefix = {arXiv},
  eprint = {1808.03305},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\QQT2WTDK\\Rosenfeld et al. - 2018 - The elephant in the room.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\UIEJT35W\\1808.html},
  journal = {arXiv preprint arXiv:1808.03305},
  keywords = {read-soon}
}

@article{rubinRotationalProperties211980,
  title = {Rotational Properties of 21 {{SC}} Galaxies with a Large Range of Luminosities and Radii, from {{NGC}} 4605 /{{R}} = 4kpc/ to {{UGC}} 2885 /{{R}} = 122 Kpc/},
  author = {Rubin, V. C. and Ford, Jr., W. K. and Thonnard, N.},
  year = {1980},
  month = jun,
  volume = {238},
  pages = {471--487},
  issn = {0004-637X},
  doi = {10.1086/158003},
  abstract = {For 21 Sc galaxies whose properties encompass a wide range of radii,  masses, and luminosities, we have obtained major axis spectra extending to the faint outer regions, and have deduced rotation curves. The galaxies are of high inclination, so uncertainties in the angle of inclination to the line of sight and in the position angle of the major axis are minimized. Their radii range from 4 to 122 kpc (H = 50km s-1 Mpc-1); in general, the rotation curves extend to 83\% or R25i.b. When plotted on a linear scale with no scaling, the rotation curves for the smallest galaxies fall upon the initial parts of the rotation curves for the larger galaxies. All curves show a fairly rapid velocity rise to V {$\sim$} 125 km s-1 at R {$\sim$} 5 kpc, and a slower rise thereafter. Most rotation curves are rising slowly even at the farthest measured point. Neither high nor low luminosity Sc galaxies have falling rotation curves. Sc galaxies of all luminosities must have significant mass located beyond the optical image. A linear relation between log Vmax and log R follows from the shape of the common rotation curve for all Sc's, and the tendency of smaller galaxies, at any R, to have lower velocities than the large galaxies at that R. The significantly shallower slope discovered for this relation by Tully and Fisher is attributed to their use of galaxies of various Hubble types and the known correlation of Vmax with Hubble type. The galaxies with very large central velocity gradients tend to be large, of high luminosity, with massive, dense nuclei. Often their nuclear spectra show a strong stellar continuum in the red, with emission lines of [N II] stronger than H{$\alpha$}. These galaxies also tend to be 13 cm radio continuum sources. Because of the form of the rotation curves, small galaxies undergo many short-period, very differential, rotations. Large galaxies undergo (in their outer parts) few, only slightly differential, rotations. This suggests a relation between morphology, rotational properties, and the van den Bergh luminosity classification, which is discussed. UGC 2885, the largest Sc in the sample, has undergone fewer than 10 rotations in its outer parts since the origin of the universe but has a regular two-armed spiral pattern and no significant velocity asymmetries. This observation puts constraints on models of galaxy formation and evolution.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2JUAT4G2\\Rubin et al. - 1980 - Rotational properties of 21 SC galaxies with a lar.pdf},
  journal = {The Astrophysical Journal},
  keywords = {Astronomical Spectroscopy,Dynamic Characteristics,Galactic Evolution,Galactic Nuclei,Galactic Rotation,Galaxies,Morphology,Radial Velocity,Radii,Stellar Luminosity}
}

@article{ruderNeuralTransferLearning,
  title = {Neural {{Transfer Learning}} for {{Natural Language Processing}}},
  author = {Ruder, Sebastian},
  pages = {329},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6WM9BM7U\\Ruder - Neural Transfer Learning for Natural Language Proc.pdf},
  language = {en}
}

@article{ruderOverviewGradientDescent2017,
  ids = {ruderOverviewGradientDescent2017a},
  title = {An Overview of Gradient Descent Optimization Algorithms},
  author = {Ruder, Sebastian},
  year = {2017},
  month = jun,
  abstract = {Gradient descent optimization algorithms, while increasingly popular, are often used as black-box optimizers, as practical explanations of their strengths and weaknesses are hard to come by. This article aims to provide the reader with intuitions with regard to the behaviour of different algorithms that will allow her to put them to use. In the course of this overview, we look at different variants of gradient descent, summarize challenges, introduce the most common optimization algorithms, review architectures in a parallel and distributed setting, and investigate additional strategies for optimizing gradient descent.},
  archivePrefix = {arXiv},
  eprint = {1609.04747},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\72FHKUMW\\Ruder - 2017 - An overview of gradient descent optimization algor.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\EB332V5Z\\Ruder - 2017 - An overview of gradient descent optimization algor.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\QB5MCH6S\\1609.html},
  journal = {arXiv:1609.04747 [cs]},
  keywords = {Computer Science - Machine Learning,lr,read},
  language = {en},
  note = {Comment: Added derivations of AdaMax and Nadam
\par
Comment: Added derivations of AdaMax and Nadam},
  primaryClass = {cs}
}

@article{ruderOverviewMultiTaskLearning2017,
  title = {An {{Overview}} of {{Multi}}-{{Task Learning}} in {{Deep Neural Networks}}},
  author = {Ruder, Sebastian},
  year = {2017},
  month = jun,
  abstract = {Multi-task learning (MTL) has led to successes in many applications of machine learning, from natural language processing and speech recognition to computer vision and drug discovery. This article aims to give a general overview of MTL, particularly in deep neural networks. It introduces the two most common methods for MTL in Deep Learning, gives an overview of the literature, and discusses recent advances. In particular, it seeks to help ML practitioners apply MTL by shedding light on how MTL works and providing guidelines for choosing appropriate auxiliary tasks.},
  archivePrefix = {arXiv},
  eprint = {1706.05098},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VMPGJ82E\\Ruder - 2017 - An Overview of Multi-Task Learning in Deep Neural .pdf},
  journal = {arXiv:1706.05098 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-maybe,Statistics - Machine Learning},
  language = {en},
  note = {Comment: 14 pages, 8 figures},
  primaryClass = {cs, stat}
}

@article{sadighImprovingResolutionCNN2018,
  title = {Improving the {{Resolution}} of {{CNN Feature Maps Efficiently}} with {{Multisampling}}},
  author = {Sadigh, Shayan and Sen, Pradeep},
  year = {2018},
  month = may,
  abstract = {We describe a new class of subsampling techniques for CNNs, termed multisampling, that significantly increases the amount of information kept by feature maps through subsampling layers. One version of our method, which we call checkered subsampling, significantly improves the accuracy of state-of-the-art architectures such as DenseNet and ResNet without any additional parameters and, remarkably, improves the accuracy of certain pretrained ImageNet models without any training or fine-tuning. We glean new insight into the nature of data augmentations and demonstrate, for the first time, that coarse feature maps are significantly bottlenecking the performance of neural networks in image classification.},
  archivePrefix = {arXiv},
  eprint = {1805.10766},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6KQT8RGN\\Sadigh and Sen - 2018 - Improving the Resolution of CNN Feature Maps Effic.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\GRYAEV7E\\1805.html},
  journal = {arXiv:1805.10766 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-soon},
  note = {Comment: Preprint},
  primaryClass = {cs}
}

@article{salimansWeightNormalizationSimple,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  author = {Salimans, Tim and Kingma, Durk P},
  pages = {9},
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ILY9M7QU\\Salimans and Kingma - Weight Normalization A Simple Reparameterization .pdf},
  keywords = {read},
  language = {en}
}

@article{salimansWeightNormalizationSimple2016,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  shorttitle = {Weight {{Normalization}}},
  author = {Salimans, Tim and Kingma, Durk P.},
  year = {2016},
  volume = {29},
  pages = {901--909},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RX2G34LD\\Salimans and Kingma - 2016 - Weight Normalization A Simple Reparameterization .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\4HS2VAIT\\ed265bc903a5a097f61d3ec064d96d2e-Abstract.html},
  journal = {Advances in Neural Information Processing Systems},
  language = {en}
}

@article{salimansWeightNormalizationSimplea,
  title = {Weight {{Normalization}}: {{A Simple Reparameterization}} to {{Accelerate Training}} of {{Deep Neural Networks}}},
  author = {Salimans, Tim and Kingma, Durk P},
  pages = {9},
  abstract = {We present weight normalization: a reparameterization of the weight vectors in a neural network that decouples the length of those weight vectors from their direction. By reparameterizing the weights in this way we improve the conditioning of the optimization problem and we speed up convergence of stochastic gradient descent. Our reparameterization is inspired by batch normalization but does not introduce any dependencies between the examples in a minibatch. This means that our method can also be applied successfully to recurrent models such as LSTMs and to noise-sensitive applications such as deep reinforcement learning or generative models, for which batch normalization is less well suited. Although our method is much simpler, it still provides much of the speed-up of full batch normalization. In addition, the computational overhead of our method is lower, permitting more optimization steps to be taken in the same amount of time. We demonstrate the usefulness of our method on applications in supervised image recognition, generative modelling, and deep reinforcement learning.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SBZP23M3\\Salimans and Kingma - Weight Normalization A Simple Reparameterization .pdf},
  language = {en}
}

@article{santurkarHowDoesBatch2019,
  title = {How {{Does Batch Normalization Help Optimization}}?},
  author = {Santurkar, Shibani and Tsipras, Dimitris and Ilyas, Andrew and Madry, Aleksander},
  year = {2019},
  month = apr,
  abstract = {Batch Normalization (BatchNorm) is a widely adopted technique that enables faster and more stable training of deep neural networks (DNNs). Despite its pervasiveness, the exact reasons for BatchNorm's effectiveness are still poorly understood. The popular belief is that this effectiveness stems from controlling the change of the layers' input distributions during training to reduce the so-called "internal covariate shift". In this work, we demonstrate that such distributional stability of layer inputs has little to do with the success of BatchNorm. Instead, we uncover a more fundamental impact of BatchNorm on the training process: it makes the optimization landscape significantly smoother. This smoothness induces a more predictive and stable behavior of the gradients, allowing for faster training.},
  archivePrefix = {arXiv},
  eprint = {1805.11604},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YDJFNHAI\\Santurkar et al. - 2019 - How Does Batch Normalization Help Optimization.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\4JP3UPLU\\1805.html},
  journal = {arXiv:1805.11604 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-soon,Statistics - Machine Learning},
  note = {Comment: In NeurIPS'18},
  primaryClass = {cs, stat}
}

@article{schmidhuberDeepLearningNeural2015,
  title = {Deep {{Learning}} in {{Neural Networks}}: {{An Overview}}},
  shorttitle = {Deep {{Learning}} in {{Neural Networks}}},
  author = {Schmidhuber, Juergen},
  year = {2015},
  month = jan,
  volume = {61},
  pages = {85--117},
  issn = {08936080},
  doi = {10.1016/j.neunet.2014.09.003},
  abstract = {In recent years, deep artificial neural networks (including recurrent ones) have won numerous contests in pattern recognition and machine learning. This historical survey compactly summarises relevant work, much of it from the previous millennium. Shallow and deep learners are distinguished by the depth of their credit assignment paths, which are chains of possibly learnable, causal links between actions and effects. I review deep supervised learning (also recapitulating the history of backpropagation), unsupervised learning, reinforcement learning \& evolutionary computation, and indirect search for short programs encoding deep and large networks.},
  archivePrefix = {arXiv},
  eprint = {1404.7828},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\D2KVDUNS\\Schmidhuber - 2015 - Deep Learning in Neural Networks An Overview.pdf},
  journal = {Neural Networks},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  language = {en},
  note = {Comment: 88 pages, 888 references}
}

@article{schmidhuberDeepLearningOur2020,
  title = {Deep {{Learning}}: {{Our Miraculous Year}} 1990-1991},
  shorttitle = {Deep {{Learning}}},
  author = {Schmidhuber, Juergen},
  year = {2020},
  month = may,
  abstract = {In 2020, we will celebrate that many of the basic ideas behind the deep learning revolution were published three decades ago within fewer than 12 months in our "Annus Mirabilis" or "Miraculous Year" 1990-1991 at TU Munich. Back then, few people were interested, but a quarter century later, neural networks based on these ideas were on over 3 billion devices such as smartphones, and used many billions of times per day, consuming a significant fraction of the world's compute.},
  archivePrefix = {arXiv},
  eprint = {2005.05744},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2VKHMRY8\\Schmidhuber - 2020 - Deep Learning Our Miraculous Year 1990-1991.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\QVY58EQF\\2005.html},
  journal = {arXiv:2005.05744 [cs]},
  keywords = {Computer Science - Neural and Evolutionary Computing},
  note = {Comment: 37 pages, 188 references, based on work of 4 Oct 2019},
  primaryClass = {cs}
}

@article{schmidhuberGoedelMachinesSelfReferential2006,
  title = {Goedel {{Machines}}: {{Self}}-{{Referential Universal Problem Solvers Making Provably Optimal Self}}-{{Improvements}}},
  shorttitle = {Goedel {{Machines}}},
  author = {Schmidhuber, Juergen},
  year = {2006},
  month = dec,
  abstract = {We present the first class of mathematically rigorous, general, fully self-referential, self-improving, optimally efficient problem solvers. Inspired by Kurt Goedel's celebrated self-referential formulas (1931), such a problem solver rewrites any part of its own code as soon as it has found a proof that the rewrite is useful, where the problem-dependent utility function and the hardware and the entire initial code are described by axioms encoded in an initial proof searcher which is also part of the initial code. The searcher systematically and efficiently tests computable proof techniques (programs whose outputs are proofs) until it finds a provably useful, computable self-rewrite. We show that such a self-rewrite is globally optimal - no local maxima! - since the code first had to prove that it is not useful to continue the proof search for alternative self-rewrites. Unlike previous non-self-referential methods based on hardwired proof searchers, ours not only boasts an optimal order of complexity but can optimally reduce any slowdowns hidden by the O()-notation, provided the utility of such speed-ups is provable at all.},
  archivePrefix = {arXiv},
  eprint = {cs/0309048},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\XNK69SHS\\Schmidhuber - 2006 - Goedel Machines Self-Referential Universal Proble.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\424LQ99M\\0309048.html},
  journal = {arXiv:cs/0309048},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Logic in Computer Science,F.4.1},
  note = {Comment: 29 pages, 1 figure, minor improvements, updated references}
}

@article{scholkopfCausalityMachineLearning2019,
  title = {Causality for {{Machine Learning}}},
  author = {Sch{\"o}lkopf, Bernhard},
  year = {2019},
  month = dec,
  abstract = {Graphical causal inference as pioneered by Judea Pearl arose from research on artificial intelligence (AI), and for a long time had little connection to the field of machine learning. This article discusses where links have been and should be established, introducing key concepts along the way. It argues that the hard open problems of machine learning and AI are intrinsically related to causality, and explains how the field is beginning to understand them.},
  archivePrefix = {arXiv},
  eprint = {1911.10500},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\L85MGGY9\\1911.10500.pdf},
  journal = {arXiv:1911.10500 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2,I.2; I.5; K.4,I.5,K.4,read-soon,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@inproceedings{schroffFaceNetUnifiedEmbedding2015,
  title = {{{FaceNet}}: {{A}} Unified Embedding for Face Recognition and Clustering},
  shorttitle = {{{FaceNet}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
  year = {2015},
  month = jun,
  pages = {815--823},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298682},
  abstract = {Despite significant recent advances in the field of face recognition [10, 14, 15, 17], implementing face verification and recognition efficiently at scale presents serious challenges to current approaches. In this paper we present a system, called FaceNet, that directly learns a mapping from face images to a compact Euclidean space where distances directly correspond to a measure of face similarity. Once this space has been produced, tasks such as face recognition, verification and clustering can be easily implemented using standard techniques with FaceNet embeddings as feature vectors.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\H6IHJF8D\\Schroff et al. - 2015 - FaceNet A unified embedding for face recognition .pdf},
  isbn = {978-1-4673-6964-0},
  keywords = {read-soon},
  language = {en}
}

@article{schwarzerDataEfficientReinforcementLearning2020,
  title = {Data-{{Efficient Reinforcement Learning}} with {{Momentum Predictive Representations}}},
  author = {Schwarzer, Max and Anand, Ankesh and Goel, Rishab and Hjelm, R. Devon and Courville, Aaron and Bachman, Philip},
  year = {2020},
  month = jul,
  abstract = {While deep reinforcement learning excels at solving tasks where large amounts of data can be collected through virtually unlimited interaction with the environment, learning from limited interaction remains a key challenge. We posit that an agent can learn more efficiently if we augment reward maximization with self-supervised objectives based on structure in its visual input and sequential interaction with the environment. Our method, Momentum Predictive Representations (MPR), trains an agent to predict its own latent state representations multiple steps into the future. We compute target representations for future states using an encoder which is an exponential moving average of the agent's parameters, and we make predictions using a learned transition model. On its own, this future prediction objective outperforms prior methods for sample-efficient deep RL from pixels. We further improve performance by adding data augmentation to the future prediction loss, which forces the agent's representations to be consistent across multiple views of an observation. Our full self-supervised objective, which combines future prediction and data augmentation, achieves a median human-normalized score of 0.444 on Atari in a setting limited to 100K steps of environment interaction, which is a 66\% relative improvement over the previous state-of-the-art. Moreover, even in this limited data regime, MPR exceeds expert human scores on 6 out of 26 games.},
  archivePrefix = {arXiv},
  eprint = {2007.05929},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SRELL5W2\\Schwarzer et al. - 2020 - Data-Efficient Reinforcement Learning with Momentu.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\8VIPQ7BY\\2007.html},
  journal = {arXiv:2007.05929 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  note = {Comment: The first two authors contributed equally to this work},
  primaryClass = {cs, stat}
}

@article{sculleyHiddenTechnicalDebt,
  title = {Hidden {{Technical Debt}} in {{Machine Learning Systems}}},
  author = {Sculley, D and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-Fran{\c c}ois and Dennison, Dan},
  pages = {9},
  abstract = {Machine learning offers a fantastically powerful toolkit for building useful complex prediction systems quickly. This paper argues it is dangerous to think of these quick wins as coming for free. Using the software engineering framework of technical debt, we find it is common to incur massive ongoing maintenance costs in real-world ML systems. We explore several ML-specific risk factors to account for in system design. These include boundary erosion, entanglement, hidden feedback loops, undeclared consumers, data dependencies, configuration issues, changes in the external world, and a variety of system-level anti-patterns.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6RHPWEJR\\Sculley et al. - Hidden Technical Debt in Machine Learning Systems.pdf},
  keywords = {read},
  language = {en}
}

@article{semerakStaticAxisymmetricRings2016,
  title = {Static Axisymmetric Rings in General Relativity: {{How}} Diverse They Are},
  shorttitle = {Static Axisymmetric Rings in General Relativity},
  author = {Semer{\'a}k, O.},
  year = {2016},
  month = nov,
  volume = {94},
  pages = {104021},
  issn = {2470-0010, 2470-0029},
  doi = {10.1103/PhysRevD.94.104021},
  abstract = {Three static and axially symmetric (Weyl-type) ring singularities -- the Majumdar-Papapetrou--type (extremally charged) ring, the Bach-Weyl ring and the Appell ring -- are studied in general relativity in order to show how remarkably the geometries in their vicinity differ from each other. This is demonstrated on basic measures of the rings and on invariant characteristics given by the metric and by its first and second derivatives (lapse, gravitational acceleration and curvature), and also on geodesic motion. The results are also compared against the Kerr space-time which possesses a ring singularity too. The Kerr solution is only stationary, not static, but in spite of the consequent complication by dragging, its ring appears to be simpler than the static rings. We show that this mainly applies to the Bach-Weyl ring, although this straightforward counter-part of the Newtonian homogeneous circular ring is by default being taken as the simplest ring solution, and although the other two static ring sources may seem more "artificial". The weird, directional deformation around the Bach-Weyl ring probably indicates that a more adequate coordinate representation and interpretation of this source should exists.},
  archivePrefix = {arXiv},
  eprint = {1611.03299},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FEQWVYVY\\Semerák - 2016 - Static axisymmetric rings in general relativity H.pdf},
  journal = {Physical Review D},
  keywords = {General Relativity and Quantum Cosmology},
  language = {en},
  note = {Comment: 25 pages, 9 figures},
  number = {10}
}

@article{sermanetOverFeatIntegratedRecognition2014,
  title = {{{OverFeat}}: {{Integrated Recognition}}, {{Localization}} and {{Detection}} Using {{Convolutional Networks}}},
  shorttitle = {{{OverFeat}}},
  author = {Sermanet, Pierre and Eigen, David and Zhang, Xiang and Mathieu, Michael and Fergus, Rob and LeCun, Yann},
  year = {2014},
  month = feb,
  abstract = {We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat.},
  archivePrefix = {arXiv},
  eprint = {1312.6229},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\EICFH59D\\Sermanet et al. - 2014 - OverFeat Integrated Recognition, Localization and.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\JPXQZ9AB\\1312.html},
  journal = {arXiv:1312.6229 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read},
  note = {Sliding window paper},
  primaryClass = {cs}
}

@article{shankerEffectDataStandardization1996,
  title = {Effect of Data Standardization on Neural Network Training},
  author = {Shanker, M. and Hu, M. Y. and Hung, M. S.},
  year = {1996},
  month = aug,
  volume = {24},
  pages = {385--397},
  issn = {0305-0483},
  doi = {10.1016/0305-0483(96)00010-2},
  abstract = {Data transformation is a popular option in training neural networks. This study evaluates the effectiveness of two well-known transformation methods: linear transformation and statistical standardization. These two are referred to as data standardization. A carefully designed experiment is used in which data from two-group classification problems were trained by feedforward networks. Different kinds of classification problems, from relatively simple to hard, were generated. Other experimental factors include network architecture, sample size, and sample proportion of group 1 members. Three performance measurements for the effect of data standardization are employed. The results suggest that networks trained on standardized data yield better results in general, but the advantage diminishes as network and sample size become large. In other words, neural networks exhibit a self-scaling capability. In addition, impact of data standardization on the performance of training algorithm in terms of computation time and number of iterations is evaluated. The results indicate that, overall, data standardization slows down training. Finally, these results are illustrated with a data set obtained from the American Telephone and Telegraph Company.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RNBSSFYD\\0305048396000102.html},
  journal = {Omega},
  keywords = {modelling,neural networks,normalization,read-someday},
  language = {en},
  number = {4}
}

@article{sheferStateArtCT2013,
  title = {State of the {{Art}} of {{CT Detectors}} and {{Sources}}: {{A Literature Review}}},
  shorttitle = {State of the {{Art}} of {{CT Detectors}} and {{Sources}}},
  author = {Shefer, Efrat and Altman, Ami and Behling, Rolf and Goshen, Raffy and Gregorian, Lev and Roterman, Yalon and Uman, Igor and Wainer, Naor and Yagil, Yoad and Zarchin, Oren},
  year = {2013},
  month = mar,
  volume = {1},
  pages = {76--91},
  issn = {2167-4825},
  doi = {10.1007/s40134-012-0006-4},
  abstract = {The three CT components with the greatest impact on image quality are the X-ray source, detection system and reconstruction algorithms. In this paper, we focus on the first two. We describe the state-of-the-art of CT detection systems, their calibrations, software corrections and common performance metrics. The components of CT detection systems, such as scintillator materials, photodiodes, data acquisition electronics and anti-scatter grids, are discussed. Their impact on CT image quality, their most important characteristics, as well as emerging future technology trends for each, are reviewed. The use of detection for multi-energy CT imaging is described. An overview of current CT X-ray sources, their evolution to support major trends in CT imaging and future trends is provided.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2TIPIDRM\\Shefer et al. - 2013 - State of the Art of CT Detectors and Sources A Li.pdf},
  journal = {Current Radiology Reports},
  language = {en},
  note = {L\ae s dual-energy del},
  number = {1}
}

@article{sheferStateArtCT2013a,
  title = {State of the {{Art}} of {{CT Detectors}} and {{Sources}}: {{A Literature Review}}},
  shorttitle = {State of the {{Art}} of {{CT Detectors}} and {{Sources}}},
  author = {Shefer, Efrat and Altman, Ami and Behling, Rolf and Goshen, Raffy and Gregorian, Lev and Roterman, Yalon and Uman, Igor and Wainer, Naor and Yagil, Yoad and Zarchin, Oren},
  year = {2013},
  volume = {1},
  pages = {76--91},
  issn = {21674825},
  doi = {10.1007/s40134-012-0006-4},
  abstract = {(2013) Shefer et al. Current Radiology Reports. The three CT components with the greatest impact on image quality are the X-ray source, detection system and reconstruction algorithms. In this paper...},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\MU5PLZ4P\\Shefer et al. - 2013 - State of the Art of CT Detectors and Sources A Li.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\M6T2ZJ6U\\66553d30-8ecd-3349-bbc8-3ba29b6fff6e.html},
  journal = {Current Radiology Reports},
  language = {en-GB},
  number = {1}
}

@article{shiInformativeDropoutRobust,
  title = {Informative {{Dropout}} for {{Robust Representation Learning}}: {{A Shape}}-Bias {{Perspective}}},
  author = {Shi, Baifeng and Zhang, Dinghuai and Dai, Qi and Zhu, Zhanxing and Mu, Yadong and Wang, Jingdong},
  pages = {12},
  abstract = {Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at https: //github.com/bfshi/InfoDrop.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\9CAX5SY8\\Shi et al. - Informative Dropout for Robust Representation Lear.pdf},
  keywords = {read-next},
  language = {en}
}

@article{shocherSemanticPyramidImage2020,
  title = {Semantic {{Pyramid}} for {{Image Generation}}},
  author = {Shocher, Assaf and Gandelsman, Yossi and Mosseri, Inbar and Yarom, Michal and Irani, Michal and Freeman, William T. and Dekel, Tali},
  year = {2020},
  month = mar,
  abstract = {We present a novel GAN-based model that utilizes the space of deep features learned by a pre-trained classification model. Inspired by classical image pyramid representations, we construct our model as a Semantic Generation Pyramid -- a hierarchical framework which leverages the continuum of semantic information encapsulated in such deep features; this ranges from low level information contained in fine features to high level, semantic information contained in deeper features. More specifically, given a set of features extracted from a reference image, our model generates diverse image samples, each with matching features at each semantic level of the classification model. We demonstrate that our model results in a versatile and flexible framework that can be used in various classic and novel image generation tasks. These include: generating images with a controllable extent of semantic similarity to a reference image, and different manipulation tasks such as semantically-controlled inpainting and compositing; all achieved with the same model, with no further training.},
  archivePrefix = {arXiv},
  eprint = {2003.06221},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LULE94FV\\Shocher et al. - 2020 - Semantic Pyramid for Image Generation.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\IWQ3HB9Q\\2003.html},
  journal = {arXiv:2003.06221 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  primaryClass = {cs}
}

@article{shrivastavaTrainingRegionbasedObject2016,
  title = {Training {{Region}}-Based {{Object Detectors}} with {{Online Hard Example Mining}}},
  author = {Shrivastava, Abhinav and Gupta, Abhinav and Girshick, Ross},
  year = {2016},
  month = apr,
  abstract = {The field of object detection has made significant advances riding on the wave of region-based ConvNets, but their training procedure still includes many heuristics and hyperparameters that are costly to tune. We present a simple yet surprisingly effective online hard example mining (OHEM) algorithm for training region-based ConvNet detectors. Our motivation is the same as it has always been \textendash detection datasets contain an overwhelming number of easy examples and a small number of hard examples. Automatic selection of these hard examples can make training more effective and efficient. OHEM is a simple and intuitive algorithm that eliminates several heuristics and hyperparameters in common use. But more importantly, it yields consistent and significant boosts in detection performance on benchmarks like PASCAL VOC 2007 and 2012. Its effectiveness increases as datasets become larger and more difficult, as demonstrated by the results on the MS COCO dataset. Moreover, combined with complementary advances in the field, OHEM leads to state-of-the-art results of 78.9\% and 76.3\% mAP on PASCAL VOC 2007 and 2012 respectively.},
  archivePrefix = {arXiv},
  eprint = {1604.03540},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\F26PFZRJ\\Shrivastava et al. - 2016 - Training Region-based Object Detectors with Online.pdf},
  journal = {arXiv:1604.03540 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-someday},
  language = {en},
  note = {Comment: To appear in Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2016. (oral)},
  primaryClass = {cs}
}

@article{silverMasteringGameGo2016,
  title = {Mastering the Game of {{Go}} with Deep Neural Networks and Tree Search},
  author = {Silver, David and Huang, Aja and Maddison, Chris J. and Guez, Arthur and Sifre, Laurent and van den Driessche, George and Schrittwieser, Julian and Antonoglou, Ioannis and Panneershelvam, Veda and Lanctot, Marc and Dieleman, Sander and Grewe, Dominik and Nham, John and Kalchbrenner, Nal and Sutskever, Ilya and Lillicrap, Timothy and Leach, Madeleine and Kavukcuoglu, Koray and Graepel, Thore and Hassabis, Demis},
  year = {2016},
  month = jan,
  volume = {529},
  pages = {484--489},
  publisher = {{Nature Publishing Group}},
  issn = {1476-4687},
  doi = {10.1038/nature16961},
  abstract = {A computer Go program based on deep neural networks defeats a human professional player to achieve one of the grand challenges of artificial intelligence.},
  copyright = {2016 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FULRG8T8\\nature16961.html},
  journal = {Nature},
  language = {en},
  number = {7587}
}

@article{simonyanVeryDeepConvolutional2015,
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archivePrefix = {arXiv},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Z8LL5AIK\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\EHERXPI5\\1409.html},
  journal = {arXiv:1409.1556 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{simonyanVeryDeepConvolutional2015a,
  title = {Very {{Deep Convolutional Networks}} for {{Large}}-{{Scale Image Recognition}}},
  author = {Simonyan, Karen and Zisserman, Andrew},
  year = {2015},
  month = apr,
  abstract = {In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision.},
  archivePrefix = {arXiv},
  eprint = {1409.1556},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\37UP6ZYV\\Simonyan and Zisserman - 2015 - Very Deep Convolutional Networks for Large-Scale I.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\LV4NHSZM\\1409.html},
  journal = {arXiv:1409.1556 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  primaryClass = {cs}
}

@article{sitzmannImplicitNeuralRepresentations2020,
  title = {Implicit {{Neural Representations}} with {{Periodic Activation Functions}}},
  author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
  year = {2020},
  month = jun,
  abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions. Please see the project website for a video overview of the proposed method and all applications.},
  archivePrefix = {arXiv},
  eprint = {2006.09661},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2PWZYX4L\\Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Acti.pdf},
  journal = {arXiv:2006.09661 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  language = {en},
  note = {Comment: Project website: https://vsitzmann.github.io/siren/ Project video: https://youtu.be/Q2fLWGBeaiI},
  primaryClass = {cs, eess}
}

@article{sitzmannImplicitNeuralRepresentations2020a,
  title = {Implicit {{Neural Representations}} with {{Periodic Activation Functions}}},
  author = {Sitzmann, Vincent and Martel, Julien N. P. and Bergman, Alexander W. and Lindell, David B. and Wetzstein, Gordon},
  year = {2020},
  month = jun,
  abstract = {Implicitly defined, continuous, differentiable signal representations parameterized by neural networks have emerged as a powerful paradigm, offering many possible benefits over conventional representations. However, current network architectures for such implicit neural representations are incapable of modeling signals with fine detail, and fail to represent a signal's spatial and temporal derivatives, despite the fact that these are essential to many physical signals defined implicitly as the solution to partial differential equations. We propose to leverage periodic activation functions for implicit neural representations and demonstrate that these networks, dubbed sinusoidal representation networks or SIRENs, are ideally suited for representing complex natural signals and their derivatives. We analyze SIREN activation statistics to propose a principled initialization scheme and demonstrate the representation of images, wavefields, video, sound, and their derivatives. Further, we show how SIRENs can be leveraged to solve challenging boundary value problems, such as particular Eikonal equations (yielding signed distance functions), the Poisson equation, and the Helmholtz and wave equations. Lastly, we combine SIRENs with hypernetworks to learn priors over the space of SIREN functions. Please see the project website for a video overview of the proposed method and all applications.},
  archivePrefix = {arXiv},
  eprint = {2006.09661},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\V9V8XMHG\\Sitzmann et al. - 2020 - Implicit Neural Representations with Periodic Acti.pdf},
  journal = {arXiv:2006.09661 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing},
  language = {en},
  note = {Comment: Project website: https://vsitzmann.github.io/siren/ Project video: https://youtu.be/Q2fLWGBeaiI},
  primaryClass = {cs, eess}
}

@inproceedings{smithBayesianPerspectiveGeneralization2018,
  title = {A {{Bayesian Perspective}} on {{Generalization}} and {{Stochastic Gradient Descent}}},
  author = {Smith, Sam and Le, Quoc V.},
  year = {2018},
  keywords = {batch_size,lr,read}
}

@article{smithCyclicalLearningRates2017,
  title = {Cyclical {{Learning Rates}} for {{Training Neural Networks}}},
  author = {Smith, Leslie N.},
  year = {2017},
  month = apr,
  abstract = {It is known that the learning rate is the most important hyper-parameter to tune for training deep neural networks. This paper describes a new method for setting the learning rate, named cyclical learning rates, which practically eliminates the need to experimentally find the best values and schedule for the global learning rates. Instead of monotonically decreasing the learning rate, this method lets the learning rate cyclically vary between reasonable boundary values. Training with cyclical learning rates instead of fixed values achieves improved classification accuracy without a need to tune and often in fewer iterations. This paper also describes a simple way to estimate ``reasonable bounds'' \textendash{} linearly increasing the learning rate of the network for a few epochs. In addition, cyclical learning rates are demonstrated on the CIFAR-10 and CIFAR-100 datasets with ResNets, Stochastic Depth networks, and DenseNets, and the ImageNet dataset with the AlexNet and GoogLeNet architectures. These are practical tools for everyone who trains neural networks.},
  archivePrefix = {arXiv},
  eprint = {1506.01186},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\AIUFFZ2U\\Smith - 2017 - Cyclical Learning Rates for Training Neural Networ.pdf},
  journal = {arXiv:1506.01186 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,read-half},
  language = {en},
  note = {Comment: Presented at WACV 2017; see https://github.com/bckenstler/CLR for instructions to implement CLR in Keras},
  primaryClass = {cs}
}

@article{smithDonDecayLearning2018,
  title = {Don't {{Decay}} the {{Learning Rate}}, {{Increase}} the {{Batch Size}}},
  author = {Smith, Samuel L. and Kindermans, Pieter-Jan and Ying, Chris and Le, Quoc V.},
  year = {2018},
  month = feb,
  abstract = {It is common practice to decay the learning rate. Here we show one can usually obtain the same learning curve on both training and test sets by instead increasing the batch size during training. This procedure is successful for stochastic gradient descent (SGD), SGD with momentum, Nesterov momentum, and Adam. It reaches equivalent test accuracies after the same number of training epochs, but with fewer parameter updates, leading to greater parallelism and shorter training times. We can further reduce the number of parameter updates by increasing the learning rate \$\textbackslash epsilon\$ and scaling the batch size \$B \textbackslash propto \textbackslash epsilon\$. Finally, one can increase the momentum coefficient \$m\$ and scale \$B \textbackslash propto 1/(1-m)\$, although this tends to slightly reduce the test accuracy. Crucially, our techniques allow us to repurpose existing training schedules for large batch training with no hyper-parameter tuning. We train ResNet-50 on ImageNet to \$76.1\textbackslash\%\$ validation accuracy in under 30 minutes.},
  archivePrefix = {arXiv},
  eprint = {1711.00489},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\9495HWNG\\Smith et al. - 2018 - Don't Decay the Learning Rate, Increase the Batch .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\UWHTLWWA\\1711.html},
  journal = {arXiv:1711.00489 [cs, stat]},
  keywords = {batch_size,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Distributed; Parallel; and Cluster Computing,Computer Science - Machine Learning,lr,Optimization,read,Statistics - Machine Learning},
  note = {Comment: 11 pages, 8 figures. Published as a conference paper at ICLR 2018},
  primaryClass = {cs, stat}
}

@article{snoekPracticalBayesianOptimization,
  title = {Practical {{Bayesian Optimization}} of {{Machine Learning Algorithms}}},
  author = {Snoek, Jasper and Larochelle, Hugo and Adams, Ryan P},
  pages = {9},
  abstract = {The use of machine learning algorithms frequently involves careful tuning of learning parameters and model hyperparameters. Unfortunately, this tuning is often a ``black art'' requiring expert experience, rules of thumb, or sometimes bruteforce search. There is therefore great appeal for automatic approaches that can optimize the performance of any given learning algorithm to the problem at hand. In this work, we consider this problem through the framework of Bayesian optimization, in which a learning algorithm's generalization performance is modeled as a sample from a Gaussian process (GP). We show that certain choices for the nature of the GP, such as the type of kernel and the treatment of its hyperparameters, can play a crucial role in obtaining a good optimizer that can achieve expertlevel performance. We describe new algorithms that take into account the variable cost (duration) of learning algorithm experiments and that can leverage the presence of multiple cores for parallel experimentation. We show that these proposed algorithms improve on previous automatic procedures and can reach or surpass human expert-level optimization for many algorithms including latent Dirichlet allocation, structured SVMs and convolutional neural networks.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LLB9NKB7\\Snoek et al. - Practical Bayesian Optimization of Machine Learnin.pdf},
  keywords = {read-soon},
  language = {en}
}

@article{springenbergStrivingSimplicityAll2015,
  title = {Striving for {{Simplicity}}: {{The All Convolutional Net}}},
  shorttitle = {Striving for {{Simplicity}}},
  author = {Springenberg, Jost Tobias and Dosovitskiy, Alexey and Brox, Thomas and Riedmiller, Martin},
  year = {2015},
  month = apr,
  abstract = {Most modern convolutional neural networks (CNNs) used for object recognition are built using the same principles: Alternating convolution and max-pooling layers followed by a small number of fully connected layers. We re-evaluate the state of the art for object recognition from small images with convolutional networks, questioning the necessity of different components in the pipeline. We find that max-pooling can simply be replaced by a convolutional layer with increased stride without loss in accuracy on several image recognition benchmarks. Following this finding -- and building on other recent work for finding simple network structures -- we propose a new architecture that consists solely of convolutional layers and yields competitive or state of the art performance on several object recognition datasets (CIFAR-10, CIFAR-100, ImageNet). To analyze the network we introduce a new variant of the "deconvolution approach" for visualizing features learned by CNNs, which can be applied to a broader range of network structures than existing approaches.},
  archivePrefix = {arXiv},
  eprint = {1412.6806},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ERALBV9G\\Springenberg et al. - 2015 - Striving for Simplicity The All Convolutional Net.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\KV3P7KVJ\\1412.html},
  journal = {arXiv:1412.6806 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  note = {Comment: accepted to ICLR-2015 workshop track; no changes other than style},
  primaryClass = {cs}
}

@article{srinivasGaussianProcessOptimization,
  title = {Gaussian {{Process Optimization}} in the {{Bandit Setting}}:  {{No Regret}} and {{Experimental Design}}},
  author = {Srinivas, Niranjan and Krause, Andreas and Kakade, Sham and Seeger, Matthias},
  pages = {8},
  abstract = {Many applications require optimizing an unknown, noisy function that is expensive to evaluate. We formalize this task as a multiarmed bandit problem, where the payoff function is either sampled from a Gaussian process (GP) or has low RKHS norm. We resolve the important open problem of deriving regret bounds for this setting, which imply novel convergence rates for GP optimization. We analyze GP-UCB, an intuitive upper-confidence based algorithm, and bound its cumulative regret in terms of maximal information gain, establishing a novel connection between GP optimization and experimental design. Moreover, by bounding the latter in terms of operator spectra, we obtain explicit sublinear regret bounds for many commonly used covariance functions. In some important cases, our bounds have surprisingly weak dependence on the dimensionality. In our experiments on real sensor data, GP-UCB compares favorably with other heuristical GP optimization approaches.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\BU5JDLRN\\Srinivas et al. - Gaussian Process Optimization in the Bandit Settin.pdf},
  language = {en}
}

@article{srivastavaDropoutSimpleWay2014,
  title = {Dropout: A Simple Way to Prevent Neural Networks from Overfitting},
  shorttitle = {Dropout},
  author = {Srivastava, Nitish and Hinton, Geoffrey and Krizhevsky, Alex and Sutskever, Ilya and Salakhutdinov, Ruslan},
  year = {2014},
  month = jan,
  volume = {15},
  pages = {1929--1958},
  issn = {1532-4435},
  abstract = {Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different "thinned" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4WTRE62Z\\Srivastava et al. - 2014 - Dropout a simple way to prevent neural networks f.pdf},
  journal = {The Journal of Machine Learning Research},
  keywords = {deep learning,model combination,neural networks,read,regularization},
  number = {1}
}

@article{stackeCloserLookDomain,
  title = {A {{Closer Look}} at {{Domain Shift}} for {{Deep Learning}} in {{Histopathology}}},
  author = {Stacke, Karin and Eilertsen, Gabriel and Unger, Jonas and Lundstrom, Claes},
  pages = {8},
  abstract = {Domain shift is a significant problem in histopathology. There can be large differences in data characteristics of whole-slide images between medical centers and scanners, making generalization of deep learning to unseen data difficult. To gain a better understanding of the problem, we present a study on convolutional neural networks trained for tumor classification of H\&E stained whole-slide images. We analyze how augmentation and normalization strategies affect performance and learned representations, and what features a trained model respond to. Most centrally, we present a novel measure for evaluating the distance between domains in the context of the learned representation of a particular model. This measure can reveal how sensitive a model is to domain variations, and can be used to detect new data that a model will have problems generalizing to. The results show how learning is heavily influenced by the preparation of training data, and that the latent representation used to do classification is sensitive to changes in data distribution, especially when training without augmentation or normalization.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ED5KXAYS\\Stacke et al. - A Closer Look at Domain Shift for Deep Learning in.pdf},
  keywords = {domain shift},
  language = {en}
}

@article{staghojRETSSTATENOgCOVID19,
  title = {{RETSSTATEN og COVID-19}},
  author = {Stagh{\o}j, S{\o}ren Brun{\o}},
  pages = {112},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\NZ4KZMSL\\Staghøj - RETSSTATEN og COVID-19.pdf},
  language = {da}
}

@book{stevensDEEPLEARNINGPYTORCH2019,
  title = {{{DEEP LEARNING WITH PYTORCH}}.},
  author = {STEVENS, ELI},
  year = {2019},
  publisher = {{O'REILLY MEDIA}},
  address = {{Place of publication not identified}},
  annotation = {OCLC: 1091844649},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\WZDACR8Y\\Deep-Learning-with-PyTorch.pdf},
  isbn = {978-1-61729-526-3},
  language = {en}
}

@misc{StrangeDefeat,
  title = {Strange {{Defeat}}},
  abstract = {A renowned historian and Resistance fighter \&\#8212; later executed by the Nazis \&\#8212; analyzes at first hand why France fell in 1940., Strange Defeat, Marc Bloch, 9780393319118},
  howpublished = {https://wwnorton.com/books/Strange-Defeat/},
  language = {en}
}

@misc{StructuredProcrastination,
  title = {Structured {{Procrastination}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\ERXEP3DN\\www.structuredprocrastination.com.html},
  howpublished = {http://www.structuredprocrastination.com/?fbclid=IwAR1RKN6BUqPxp5mLCf9JaY2Nt\_lM\_NomtllduFKrF89B2y9QDfD-5EsOT\_4}
}

@misc{SugarToxicAgent,
  title = {Sugar Is a Toxic Agent That Creates Conditions for Disease \textendash{} {{Gary Taubes}} | {{Aeon Essays}}},
  abstract = {A potent toxin that alters hormones and metabolism, sugar sets the stage for epidemic levels of obesity and diabetes},
  chapter = {Science},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FSMJJGL3\\sugar-is-a-toxic-agent-that-creates-conditions-for-disease.html},
  howpublished = {https://aeon.co/essays/sugar-is-a-toxic-agent-that-creates-conditions-for-disease},
  journal = {Aeon},
  language = {en}
}

@inproceedings{sunAdaptiveGradientMethods2018,
  title = {Adaptive {{Gradient Methods}} with {{Dynamic Bound}} of {{Learning Rate}}},
  booktitle = {International {{Conference}} on {{Learning Representations}}},
  author = {Sun, Xu},
  year = {2018},
  month = sep,
  abstract = {Novel variants of optimization methods that combine the benefits of both adaptive and non-adaptive methods.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GJ8YYVPG\\Sun - 2018 - Adaptive Gradient Methods with Dynamic Bound of Le.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\L78BYM2Z\\forum.html},
  keywords = {AdaBound}
}

@article{sutskeverImportanceInitializationMomentum,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  pages = {14},
  abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\I4RRMJN7\\Sutskever et al. - On the importance of initialization and momentum i.pdf},
  keywords = {read-soon},
  language = {en}
}

@article{sutskeverImportanceInitializationMomentuma,
  title = {On the Importance of Initialization and Momentum in Deep Learning},
  author = {Sutskever, Ilya and Martens, James and Dahl, George and Hinton, Geoffrey},
  pages = {14},
  abstract = {Deep and recurrent neural networks (DNNs and RNNs respectively) are powerful models that were considered to be almost impossible to train using stochastic gradient descent with momentum. In this paper, we show that when stochastic gradient descent with momentum uses a well-designed random initialization and a particular type of slowly increasing schedule for the momentum parameter, it can train both DNNs and RNNs (on datasets with long-term dependencies) to levels of performance that were previously achievable only with Hessian-Free optimization. We find that both the initialization and the momentum are crucial since poorly initialized networks cannot be trained with momentum and well-initialized networks perform markedly worse when the momentum is absent or poorly tuned.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\M2XI8LB7\\Sutskever et al. - On the importance of initialization and momentum i.pdf},
  keywords = {lr,read-soon},
  language = {en}
}

@book{suttonReinforcementLearningIntroduction2018,
  ids = {suttonReinforcementLearningIntroduction2018a,suttonReinforcementLearningIntroduction2018b},
  title = {Reinforcement Learning: An Introduction},
  shorttitle = {Reinforcement Learning},
  author = {Sutton, Richard S. and Barto, Andrew G.},
  year = {2018},
  edition = {Second edition},
  publisher = {{The MIT Press}},
  address = {{Cambridge, Massachusetts}},
  abstract = {"Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives while interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the field's key ideas and algorithms."--},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\F4TN4WNQ\\Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\MPVG4FHU\\Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\YILD8HGQ\\Sutton and Barto - 2018 - Reinforcement learning an introduction.pdf},
  isbn = {978-0-262-03924-6},
  keywords = {Reinforcement learning},
  language = {en},
  lccn = {Q325.6 .R45 2018},
  series = {Adaptive Computation and Machine Learning Series}
}

@misc{syncedMicrosoftDemocratizesDeepSpeed2020,
  title = {Microsoft {{Democratizes DeepSpeed With Four New Technologies}}},
  author = {{Synced}},
  year = {2020},
  month = sep,
  abstract = {Microsoft has released four additional DeepSpeed technologies to enable even faster training times, whether on supercomputers or a single GPU.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\9ATNW7XM\\microsoft-democratizes-deepspeed-with-four-new-technologies.html},
  howpublished = {https://syncedreview.com/2020/09/14/microsoft-democratizes-deepspeed-with-four-new-technologies/},
  journal = {Synced},
  language = {en-US}
}

@inproceedings{szegedyGoingDeeperConvolutions2015,
  title = {Going Deeper with Convolutions},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Szegedy, Christian and {Wei Liu} and {Yangqing Jia} and Sermanet, Pierre and Reed, Scott and Anguelov, Dragomir and Erhan, Dumitru and Vanhoucke, Vincent and Rabinovich, Andrew},
  year = {2015},
  month = jun,
  pages = {1--9},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298594},
  abstract = {We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YQJCU2TC\\Szegedy et al. - 2015 - Going deeper with convolutions.pdf},
  isbn = {978-1-4673-6964-0},
  keywords = {read-half},
  language = {en}
}

@inproceedings{szegedyInceptionv4InceptionResNetImpact2017,
  title = {Inception-v4, {{Inception}}-{{ResNet}} and the {{Impact}} of {{Residual Connections}} on {{Learning}}},
  booktitle = {Thirty-{{First AAAI Conference}} on {{Artificial Intelligence}}},
  author = {Szegedy, Christian and Ioffe, Sergey and Vanhoucke, Vincent and Alemi, Alexander A.},
  year = {2017},
  month = feb,
  abstract = {Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08\% top-5 error on the test set of the ImageNet classification (CLS) challenge.},
  copyright = {Authors who publish a paper in this conference agree to the following terms:   Author(s) agree to transfer their copyrights in their article/paper to the Association for the Advancement of Artificial Intelligence (AAAI), in order to deal with future requests for reprints, translations, anthologies, reproductions, excerpts, and other publications. This grant will include, without limitation, the entire copyright in the article/paper in all countries of the world, including all renewals, extensions, and reversions thereof, whether such rights current exist or hereafter come into effect, and also the exclusive right to create electronic versions of the article/paper, to the extent that such right is not subsumed under copyright.  The author(s) warrants that they are the sole author and owner of the copyright in the above article/paper, except for those portions shown to be in quotations; that the article/paper is original throughout; and that the undersigned right to make the grants set forth above is complete and unencumbered.  The author(s) agree that if anyone brings any claim or action alleging facts that, if true, constitute a breach of any of the foregoing warranties, the author(s) will hold harmless and indemnify AAAI, their grantees, their licensees, and their distributors against any liability, whether under judgment, decree, or compromise, and any legal fees and expenses arising out of that claim or actions, and the undersigned will cooperate fully in any defense AAAI may make to such claim or action. Moreover, the undersigned agrees to cooperate in any claim or other action seeking to protect or enforce any right the undersigned has granted to AAAI in the article/paper. If any such claim or action fails because of facts that constitute a breach of any of the foregoing warranties, the undersigned agrees to reimburse whomever brings such claim or action for expenses and attorneys' fees incurred therein.  Author(s) retain all proprietary rights other than copyright (such as patent rights).  Author(s) may make personal reuse of all or portions of the above article/paper in other works of their own authorship.  Author(s) may reproduce, or have reproduced, their article/paper for the author's personal use, or for company use provided that AAAI copyright and the source are indicated, and that the copies are not used in a way that implies AAAI endorsement of a product or service of an employer, and that the copies per se are not offered for sale. The foregoing right shall not permit the posting of the article/paper in electronic or digital form on any computer network, except by the author or the author's employer, and then only on the author's or the employer's own web page or ftp site. Such web page or ftp site, in addition to the aforementioned requirements of this Paragraph, must provide an electronic reference or link back to the AAAI electronic server, and shall not post other AAAI copyrighted materials not of the author's or the employer's creation (including tables of contents with links to other papers) without AAAI's written permission.  Author(s) may make limited distribution of all or portions of their article/paper prior to publication.  In the case of work performed under U.S. Government contract, AAAI grants the U.S. Government royalty-free permission to reproduce all or portions of the above article/paper, and to authorize others to do so, for U.S. Government purposes.  In the event the above article/paper is not accepted and published by AAAI, or is withdrawn by the author(s) before acceptance by AAAI, this agreement becomes null and void.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\35ZI6JL8\\Szegedy et al. - 2017 - Inception-v4, Inception-ResNet and the Impact of R.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\FQZIT6N6\\14806.html},
  keywords = {read-next},
  language = {en}
}

@article{szegedyRethinkingInceptionArchitecture2015,
  title = {Rethinking the {{Inception Architecture}} for {{Computer Vision}}},
  author = {Szegedy, Christian and Vanhoucke, Vincent and Ioffe, Sergey and Shlens, Jonathon and Wojna, Zbigniew},
  year = {2015},
  month = dec,
  abstract = {Convolutional networks are at the core of most state-of-the-art computer vision solutions for a wide variety of tasks. Since 2014 very deep convolutional networks started to become mainstream, yielding substantial gains in various benchmarks. Although increased model size and computational cost tend to translate to immediate quality gains for most tasks (as long as enough labeled data is provided for training), computational efficiency and low parameter count are still enabling factors for various use cases such as mobile vision and big-data scenarios. Here we explore ways to scale up networks in ways that aim at utilizing the added computation as efficiently as possible by suitably factorized convolutions and aggressive regularization. We benchmark our methods on the ILSVRC 2012 classification challenge validation set demonstrate substantial gains over the state of the art: 21.2\% top-1 and 5.6\% top-5 error for single frame evaluation using a network with a computational cost of 5 billion multiply-adds per inference and with using less than 25 million parameters. With an ensemble of 4 models and multi-crop evaluation, we report 3.5\% top-5 error on the validation set (3.6\% error on the test set) and 17.3\% top-1 error on the validation set.},
  archivePrefix = {arXiv},
  eprint = {1512.00567},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TJ3NHPJZ\\Szegedy et al. - 2015 - Rethinking the Inception Architecture for Computer.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\T9DBZCDA\\1512.html},
  journal = {arXiv:1512.00567 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-soon},
  primaryClass = {cs}
}

@article{tanEfficientDetScalableEfficient2020,
  title = {{{EfficientDet}}: {{Scalable}} and {{Efficient Object Detection}}},
  shorttitle = {{{EfficientDet}}},
  author = {Tan, Mingxing and Pang, Ruoming and Le, Quoc V.},
  year = {2020},
  month = apr,
  abstract = {Model efficiency has become increasingly important in computer vision. In this paper, we systematically study neural network architecture design choices for object detection and propose several key optimizations to improve efficiency. First, we propose a weighted bi-directional feature pyramid network (BiFPN), which allows easy and fast multi-scale feature fusion; Second, we propose a compound scaling method that uniformly scales the resolution, depth, and width for all backbone, feature network, and box/class prediction networks at the same time. Based on these optimizations and EfficientNet backbones, we have developed a new family of object detectors, called EfficientDet, which consistently achieve much better efficiency than prior art across a wide spectrum of resource constraints. In particular, with single-model and single-scale, our EfficientDet-D7 achieves state-of-the-art 52.2 AP on COCO test-dev with 52M parameters and 325B FLOPs, being 4x - 9x smaller and using 13x - 42x fewer FLOPs than previous detectors. Code is available at https://github.com/google/automl/tree/master/efficientdet.},
  archivePrefix = {arXiv},
  eprint = {1911.09070},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\WWDB9X6D\\Tan et al. - 2020 - EfficientDet Scalable and Efficient Object Detect.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\EQICSHCV\\1911.html},
  journal = {arXiv:1911.09070 [cs, eess]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Image and Video Processing,read-soon},
  note = {Comment: CVPR 2020},
  primaryClass = {cs, eess}
}

@article{tanEfficientNetRethinkingModel2019,
  title = {{{EfficientNet}}: {{Rethinking Model Scaling}} for {{Convolutional Neural Networks}}},
  shorttitle = {{{EfficientNet}}},
  author = {Tan, Mingxing and Le, Quoc V.},
  year = {2019},
  month = nov,
  abstract = {Convolutional Neural Networks (ConvNets) are commonly developed at a fixed resource budget, and then scaled up for better accuracy if more resources are available. In this paper, we systematically study model scaling and identify that carefully balancing network depth, width, and resolution can lead to better performance. Based on this observation, we propose a new scaling method that uniformly scales all dimensions of depth/width/resolution using a simple yet highly effective compound coefficient. We demonstrate the effectiveness of this method on scaling up MobileNets and ResNet.},
  archivePrefix = {arXiv},
  eprint = {1905.11946},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Y42UE52G\\Tan and Le - 2019 - EfficientNet Rethinking Model Scaling for Convolu.pdf},
  journal = {arXiv:1905.11946 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read,Statistics - Machine Learning},
  language = {en},
  note = {Comment: ICML 2019},
  primaryClass = {cs, stat}
}

@inproceedings{tanMnasNetPlatformAwareNeural2019,
  title = {{{MnasNet}}: {{Platform}}-{{Aware Neural Architecture Search}} for {{Mobile}}},
  shorttitle = {{{MnasNet}}},
  booktitle = {2019 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tan, Mingxing and Chen, Bo and Pang, Ruoming and Vasudevan, Vijay and Sandler, Mark and Howard, Andrew and Le, Quoc V.},
  year = {2019},
  month = jun,
  pages = {2815--2823},
  publisher = {{IEEE}},
  address = {{Long Beach, CA, USA}},
  doi = {10.1109/CVPR.2019.00293},
  abstract = {Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2\% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\texttimes{} faster than MobileNetV2 [29] with 0.5\% higher accuracy and 2.3\texttimes{} faster than NASNet [36] with 1.2\% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/ tree/master/models/official/mnasnet.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\V6994YVD\\Tan et al. - 2019 - MnasNet Platform-Aware Neural Architecture Search.pdf},
  isbn = {978-1-72813-293-8},
  keywords = {read-next},
  language = {en}
}

@article{thickstunTransformerModelEquations,
  title = {The {{Transformer Model}} in {{Equations}}},
  author = {Thickstun, John},
  pages = {5},
  abstract = {This document presents a precise mathematical definition of the transformer model introduced by Vaswani et al. [2017], along with some discussion of the terminology and intuitions commonly associated with the transformer. We also draw some connections between the transformer and lstm, based on observations by Levy et al. [2018].},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\XY5GEVFM\\Thickstun - The Transformer Model in Equations.pdf},
  keywords = {attention,transformer},
  language = {en},
  note = {attention}
}

@inproceedings{tianFCOSFullyConvolutional2019,
  title = {{{FCOS}}: {{Fully Convolutional One}}-{{Stage Object Detection}}},
  shorttitle = {{{FCOS}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Tian, Zhi and Shen, Chunhua and Chen, Hao and He, Tong},
  year = {2019},
  month = oct,
  pages = {9626--9635},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00972},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\E9STPZR5\\Tian et al. - 2019 - FCOS Fully Convolutional One-Stage Object Detecti.pdf},
  isbn = {978-1-72814-803-8},
  keywords = {read-soon},
  language = {en}
}

@article{tobinDomainRandomizationTransferring2017,
  title = {Domain {{Randomization}} for {{Transferring Deep Neural Networks}} from {{Simulation}} to the {{Real World}}},
  author = {Tobin, Josh and Fong, Rachel and Ray, Alex and Schneider, Jonas and Zaremba, Wojciech and Abbeel, Pieter},
  year = {2017},
  month = mar,
  abstract = {Bridging the 'reality gap' that separates simulated robotics from experiments on hardware could accelerate robotic research through improved data availability. This paper explores domain randomization, a simple technique for training models on simulated images that transfer to real images by randomizing rendering in the simulator. With enough variability in the simulator, the real world may appear to the model as just another variation. We focus on the task of object localization, which is a stepping stone to general robotic manipulation skills. We find that it is possible to train a real-world object detector that is accurate to \$1.5\$cm and robust to distractors and partial occlusions using only data from a simulator with non-realistic random textures. To demonstrate the capabilities of our detectors, we show they can be used to perform grasping in a cluttered environment. To our knowledge, this is the first successful transfer of a deep neural network trained only on simulated RGB images (without pre-training on real images) to the real world for the purpose of robotic control.},
  archivePrefix = {arXiv},
  eprint = {1703.06907},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4H9TU3P4\\Tobin et al. - 2017 - Domain Randomization for Transferring Deep Neural .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\NMN5IIPI\\1703.html},
  journal = {arXiv:1703.06907 [cs]},
  keywords = {Computer Science - Machine Learning,Computer Science - Robotics,read-next},
  note = {Comment: 8 pages, 7 figures. Submitted to 2017 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS 2017)},
  primaryClass = {cs}
}

@inproceedings{tompsonEfficientObjectLocalization2015,
  title = {Efficient Object Localization Using {{Convolutional Networks}}},
  booktitle = {2015 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Tompson, Jonathan and Goroshin, Ross and Jain, Arjun and LeCun, Yann and Bregler, Christoph},
  year = {2015},
  month = jun,
  pages = {648--656},
  publisher = {{IEEE}},
  address = {{Boston, MA, USA}},
  doi = {10.1109/CVPR.2015.7298664},
  abstract = {Recent state-of-the-art performance on human-body pose estimation has been achieved with Deep Convolutional Networks (ConvNets). Traditional ConvNet architectures include pooling and sub-sampling layers which reduce computational requirements, introduce invariance and prevent over-training. These benefits of pooling come at the cost of reduced localization accuracy. We introduce a novel architecture which includes an efficient `position refinement' model that is trained to estimate the joint offset location within a small region of the image. This refinement model is jointly trained in cascade with a state-of-the-art ConvNet model [21] to achieve improved accuracy in human joint location estimation. We show that the variance of our detector approaches the variance of human annotations on the FLIC [20] dataset and outperforms all existing approaches on the MPII-human-pose dataset [1].},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CPIHU6S3\\Tompson et al. - 2015 - Efficient object localization using Convolutional .pdf},
  isbn = {978-1-4673-6964-0},
  keywords = {read},
  language = {en}
}

@article{tongUniversityCambridgePart,
  title = {University of {{Cambridge Part III Mathematical Tripos}}},
  author = {Tong, David},
  pages = {35},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\KG84NT5C\\Tong - University of Cambridge Part III Mathematical Trip.pdf},
  language = {en}
}

@misc{topicAdaptiveXrayInspection,
  title = {Adaptive {{X}}-Ray {{Inspection System}} ({{AXIS}})},
  author = {Topic, Aleksandar},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\WTN55VNN\\Topic - Adaptive X-ray Inspection System (AXIS).pdf},
  language = {en}
}

@misc{TorchvisionTransformsPyTorch,
  title = {Torchvision.Transforms \textemdash{} {{PyTorch}} 1.7.0 Documentation},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\W9PVCFS2\\transforms.html},
  howpublished = {https://pytorch.org/docs/stable/torchvision/transforms.html},
  journal = {torchvision.transforms},
  keywords = {pytorch}
}

@incollection{touvronFixingTraintestResolution2019,
  title = {Fixing the Train-Test Resolution Discrepancy},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and Jegou, Herve},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {8252--8262},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\MLR4FXXF\\Touvron et al. - 2019 - Fixing the train-test resolution discrepancy.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\4S9MDANC\\9035-fixing-the-train-test-resolution-discrepancy.html},
  keywords = {read-soon}
}

@article{touvronFixingTraintestResolution2020,
  ids = {touvronFixingTraintestResolution2020a},
  title = {Fixing the Train-Test Resolution Discrepancy: {{FixEfficientNet}}},
  shorttitle = {Fixing the Train-Test Resolution Discrepancy},
  author = {Touvron, Hugo and Vedaldi, Andrea and Douze, Matthijs and J{\'e}gou, Herv{\'e}},
  year = {2020},
  month = apr,
  abstract = {This note complements the paper ``Fixing the train-test resolution discrepancy'' that introduced the FixRes method. First, we show that this strategy is advantageously combined with recent training recipes from the literature. Most importantly, we provide new results for the EfficientNet architecture1.},
  archivePrefix = {arXiv},
  eprint = {2003.08237},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\5MUUZ346\\Touvron et al. - 2020 - Fixing the train-test resolution discrepancy FixE.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\KRKEZ9EK\\Touvron et al. - 2020 - Fixing the train-test resolution discrepancy FixE.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\PSVNJ9XH\\Touvron et al. - 2020 - Fixing the train-test resolution discrepancy FixE.pdf},
  journal = {arXiv:2003.08237 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-soon},
  language = {en},
  primaryClass = {cs}
}

@article{TufteStyleBook,
  title = {A {{Tufte}}-{{Style Book}}},
  pages = {42},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\S5WMZVV9\\A Tufte-Style Book.pdf},
  language = {en}
}

@article{udellORIE4741Learning,
  title = {{{ORIE}} 4741: {{Learning}} with {{Big Messy Data Generalization}}},
  author = {Udell, Professor},
  pages = {57},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JGFCLB85\\Udell - ORIE 4741 Learning with Big Messy Data Generaliza.pdf},
  language = {en}
}

@misc{UsingItertoolsProduct,
  title = {Using Itertools.Product with Dictionaries},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PCLBQPFY\\product-dict.html},
  howpublished = {http://stephantul.github.io/python/2019/07/20/product-dict/}
}

@article{vaillantOriginalApproachLocalisation1994,
  title = {Original Approach for the Localisation of Objects in Images},
  author = {Vaillant, R. and Monrocq, C. and Cun, Y. Le},
  year = {1994},
  month = aug,
  volume = {141},
  pages = {245--250},
  publisher = {{IET Digital Library}},
  issn = {1359-7108},
  doi = {10.1049/ip-vis:19941301},
  abstract = {An original approach is presented for the localisation of objects in an image which approach is neuronal and has two steps. In the first step, a rough localisation is performed by presenting each pixel with its neighbourhood to a neural net which is able to indicate whether this pixel and its neighbourhood are the image of the search object. This first filter does not discriminate for position. From its result, areas which might contain an image of the object can be selected. In the second step, these areas are presented to another neural net which can determine the exact position of the object in each area. This algorithm is applied to the problem of localising faces in images.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CEV98BGS\\ip-vis_19941301.html},
  journal = {IEE Proceedings - Vision, Image and Signal Processing},
  language = {en},
  number = {4}
}

@article{vanraamsdonkBuildingSpacetimeQuantum2010,
  title = {Building up Spacetime with Quantum Entanglement},
  author = {Van Raamsdonk, Mark},
  year = {2010},
  month = dec,
  volume = {19},
  pages = {2429--2435},
  issn = {0218-2718, 1793-6594},
  doi = {10.1142/S0218271810018529},
  abstract = {In this essay, we argue that the emergence of classically connected spacetimes is intimately related to the quantum entanglement of degrees of freedom in a non-perturbative description of quantum gravity. Disentangling the degrees of freedom associated with two regions of spacetime results in these regions pulling apart and pinching off from each other in a way that can be quantified by standard measures of entanglement.},
  archivePrefix = {arXiv},
  eprint = {1005.3035},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\WWFHGYUE\\Van Raamsdonk - 2010 - Building up spacetime with quantum entanglement.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\64UBB9JJ\\1005.html},
  journal = {International Journal of Modern Physics D},
  keywords = {General Relativity and Quantum Cosmology,High Energy Physics - Theory,Quantum Physics},
  note = {Comment: Gravity Research Foundation essay, 7 pages, LaTeX, 5 figures},
  number = {14}
}

@article{vapnikOverviewStatisticalLearning1999,
  title = {An Overview of Statistical Learning Theory},
  author = {Vapnik, V.N.},
  year = {Sept./1999},
  volume = {10},
  pages = {988--999},
  issn = {10459227},
  doi = {10.1109/72.788640},
  abstract = {Statistical learning theory was introduced in the late 1960's. Until the 1990's it was a purely theoretical analysis of the problem of function estimation from a given collection of data. In the middle of the 1990's new types of learning algorithms (called support vector machines) based on the developed theory were proposed. This made statistical learning theory not only a tool for the theoretical analysis but also a tool for creating practical algorithms for estimating multidimensional functions. This article presents a very general overview of statistical learning theory including both theoretical and algorithmic aspects of the theory. The goal of this overview is to demonstrate how the abstract learning theory established conditions for generalization which are more general than those discussed in classical statistical paradigms and how the understanding of these conditions inspired new algorithmic approaches to function estimation problems. A more detailed overview of the theory (without proofs) can be found in Vapnik (1995). In Vapnik (1998) one can find detailed description of the theory (including proofs).},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\CPG42DIA\\Vapnik - 1999 - An overview of statistical learning theory.pdf},
  journal = {IEEE Transactions on Neural Networks},
  language = {en},
  number = {5}
}

@incollection{vaswaniAttentionAllYou2017,
  title = {Attention Is {{All}} You {{Need}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {5998--6008},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\N439PRCW\\Vaswani et al. - 2017 - Attention is All you Need.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\4KJB9A5B\\7181-attention-is-all-you-need.html},
  keywords = {read-next}
}

@misc{victorHumaneRepresentationThought2014,
  title = {The {{Humane Representation}} of {{Thought}}},
  author = {Victor, Bret},
  year = {2014},
  month = dec,
  abstract = {Closing keynote at the UIST and SPLASH conferences, October 2014. Preface: http://worrydream.com/TheHumaneRepresentationOfThought/note.html   References to baby-steps\&hellip;},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HF2TWDMX\\115154289.html}
}

@article{vincentStackedDenoisingAutoencoders,
  title = {Stacked {{Denoising Autoencoders}}: {{Learning Useful Representations}} in a {{Deep Network}} with a {{Local Denoising Criterion}}},
  author = {Vincent, Pascal and Larochelle, Hugo and Lajoie, Isabelle and Bengio, Yoshua and Manzagol, Pierre-Antoine},
  pages = {38},
  abstract = {We explore an original strategy for building deep networks, based on stacking layers of denoising autoencoders which are trained locally to denoise corrupted versions of their inputs. The resulting algorithm is a straightforward variation on the stacking of ordinary autoencoders. It is however shown on a benchmark of classification problems to yield significantly lower classification error, thus bridging the performance gap with deep belief networks (DBN), and in several cases surpassing it. Higher level representations learnt in this purely unsupervised fashion also help boost the performance of subsequent SVM classifiers. Qualitative experiments show that, contrary to ordinary autoencoders, denoising autoencoders are able to learn Gabor-like edge detectors from natural image patches and larger stroke detectors from digit images. This work clearly establishes the value of using a denoising criterion as a tractable unsupervised objective to guide the learning of useful higher level representations.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\MCDKA48C\\Vincent et al. - Stacked Denoising Autoencoders Learning Useful Re.pdf},
  language = {en}
}

@inproceedings{violaRapidObjectDetection2001,
  title = {Rapid Object Detection Using a Boosted Cascade of Simple Features},
  booktitle = {Proceedings of the 2001 {{IEEE Computer Society Conference}} on {{Computer Vision}} and {{Pattern Recognition}}. {{CVPR}} 2001},
  author = {Viola, P. and Jones, M.},
  year = {2001},
  month = dec,
  volume = {1},
  pages = {I-I},
  issn = {1063-6919},
  doi = {10.1109/CVPR.2001.990517},
  abstract = {This paper describes a machine learning approach for visual object detection which is capable of processing images extremely rapidly and achieving high detection rates. This work is distinguished by three key contributions. The first is the introduction of a new image representation called the "integral image" which allows the features used by our detector to be computed very quickly. The second is a learning algorithm, based on AdaBoost, which selects a small number of critical visual features from a larger set and yields extremely efficient classifiers. The third contribution is a method for combining increasingly more complex classifiers in a "cascade" which allows background regions of the image to be quickly discarded while spending more computation on promising object-like regions. The cascade can be viewed as an object specific focus-of-attention mechanism which unlike previous approaches provides statistical guarantees that discarded regions are unlikely to contain the object of interest. In the domain of face detection the system yields detection rates comparable to the best previous systems. Used in real-time applications, the detector runs at 15 frames per second without resorting to image differencing or skin color detection.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\A883FIPK\\Viola and Jones - 2001 - Rapid object detection using a boosted cascade of .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\GA6R77T6\\990517.html},
  keywords = {AdaBoost,background regions,boosted simple feature cascade,classifiers,Detectors,face detection,Face detection,feature extraction,Filters,Focusing,image classification,image processing,image representation,Image representation,integral image,learning (artificial intelligence),machine learning,Machine learning,object detection,Object detection,object specific focus-of-attention mechanism,Pixel,rapid object detection,read-next,real-time applications,Robustness,Skin,sliding-window,statistical guarantees,visual object detection}
}

@misc{wadaWkentaroLabelme2020,
  title = {Wkentaro/Labelme},
  author = {Wada, Kentaro},
  year = {2020},
  month = oct,
  abstract = {Image Polygonal Annotation with Python (polygon, rectangle, circle, line, point and image-level flag annotation).},
  copyright = {View license         ,                 View license},
  keywords = {annotations,classification,computer-vision,deep-learning,image-annotation,instance-segmentation,label,python,semantic-segmentation,video-annotation}
}

@article{wangAccuratePosttrainingNetwork,
  title = {Towards {{Accurate Post}}-Training {{Network Quantization}} via {{Bit}}-{{Split}} and {{Stitching}}},
  author = {Wang, Peisong and Chen, Qiang and He, Xiangyu and Cheng, Jian},
  pages = {10},
  abstract = {Network quantization is essential for deploying deep models to IoT devices due to its high efficiency. Most existing quantization approaches rely on the full training datasets and the timeconsuming fine-tuning to retain accuracy. Posttraining quantization does not have these problems, however, it has mainly been shown effective for 8-bit quantization due to the simple optimization strategy. In this paper, we propose a Bit-Split and Stitching framework (Bit-split) for lower-bit post-training quantization with minimal accuracy degradation. The proposed framework is validated on a variety of computer vision tasks, including image classification, object detection, instance segmentation, with various network architectures. Specifically, Bit-split can achieve near-original model performance even when quantizing FP32 models to INT3 without fine-tuning.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\28TUI5QI\\Wang et al. - Towards Accurate Post-training Network Quantizatio.pdf},
  language = {en}
}

@article{wangCNNExplainerLearning2020,
  title = {{{CNN Explainer}}: {{Learning Convolutional Neural Networks}} with {{Interactive Visualization}}},
  shorttitle = {{{CNN Explainer}}},
  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},
  year = {2020},
  month = apr,
  abstract = {Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. Users can interactively visualize and inspect the data transformation and flow of intermediate results in a CNN. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level operations (e.g., mathematical computations) and high-level outcomes (e.g., class predictions). To better understand our tool's benefits, we conducted a qualitative user study, which shows that CNN Explainer can help users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.},
  archivePrefix = {arXiv},
  eprint = {2004.15004},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JSDTQL67\\2004.15004v2.pdf},
  journal = {arXiv:2004.15004 [cs]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,read-maybe},
  language = {en},
  note = {Comment: 11 pages, 14 figures. For a demo video, see https://youtu.be/HnWIHWFbuUQ For a live demo, visit https://poloclub.github.io/cnn-explainer/},
  primaryClass = {cs}
}

@article{wangCNNExplainerLearning2020a,
  title = {{{CNN Explainer}}: {{Learning Convolutional Neural Networks}} with {{Interactive Visualization}}},
  shorttitle = {{{CNN Explainer}}},
  author = {Wang, Zijie J. and Turko, Robert and Shaikh, Omar and Park, Haekyu and Das, Nilaksh and Hohman, Fred and Kahng, Minsuk and Chau, Duen Horng},
  year = {2020},
  pages = {1--1},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2020.3030418},
  abstract = {Deep learning's great success motivates many practitioners and students to learn about this exciting technology. However, it is often challenging for beginners to take their first step due to the complexity of understanding and applying deep learning. We present CNN Explainer, an interactive visualization tool designed for non-experts to learn and examine convolutional neural networks (CNNs), a foundational deep learning model architecture. Our tool addresses key challenges that novices face while learning about CNNs, which we identify from interviews with instructors and a survey with past students. CNN Explainer tightly integrates a model overview that summarizes a CNN's structure, and on-demand, dynamic visual explanation views that help users understand the underlying components of CNNs. Through smooth transitions across levels of abstraction, our tool enables users to inspect the interplay between low-level mathematical operations and high-level model structures. A qualitative user study shows that CNN Explainer helps users more easily understand the inner workings of CNNs, and is engaging and enjoyable to use. We also derive design lessons from our study. Developed using modern web technologies, CNN Explainer runs locally in users' web browsers without the need for installation or specialized hardware, broadening the public's education access to modern deep learning techniques.},
  archivePrefix = {arXiv},
  eprint = {2004.15004},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\WY2GKN93\\Wang et al. - 2020 - CNN Explainer Learning Convolutional Neural Netwo.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\CYS3F45Q\\2004.html},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  note = {Comment: 11 pages, 14 figures, to be presented at IEEE VIS 2020. For a demo video, see https://youtu.be/HnWIHWFbuUQ . For a live demo, visit https://poloclub.github.io/cnn-explainer/}
}

@article{wangECANetEfficientChannel2020,
  title = {{{ECA}}-{{Net}}: {{Efficient Channel Attention}} for {{Deep Convolutional Neural Networks}}},
  shorttitle = {{{ECA}}-{{Net}}},
  author = {Wang, Qilong and Wu, Banggu and Zhu, Pengfei and Li, Peihua and Zuo, Wangmeng and Hu, Qinghua},
  year = {2020},
  month = apr,
  abstract = {Recently, channel attention mechanism has demonstrated to offer great potential in improving the performance of deep convolutional neural networks (CNNs). However, most existing methods dedicate to developing more sophisticated attention modules for achieving better performance, which inevitably increase model complexity. To overcome the paradox of performance and complexity trade-off, this paper proposes an Efficient Channel Attention (ECA) module, which only involves a handful of parameters while bringing clear performance gain. By dissecting the channel attention module in SENet, we empirically show avoiding dimensionality reduction is important for learning channel attention, and appropriate cross-channel interaction can preserve performance while significantly decreasing model complexity. Therefore, we propose a local cross-channel interaction strategy without dimensionality reduction, which can be efficiently implemented via \$1D\$ convolution. Furthermore, we develop a method to adaptively select kernel size of \$1D\$ convolution, determining coverage of local cross-channel interaction. The proposed ECA module is efficient yet effective, e.g., the parameters and computations of our modules against backbone of ResNet50 are 80 vs. 24.37M and 4.7e-4 GFLOPs vs. 3.86 GFLOPs, respectively, and the performance boost is more than 2\% in terms of Top-1 accuracy. We extensively evaluate our ECA module on image classification, object detection and instance segmentation with backbones of ResNets and MobileNetV2. The experimental results show our module is more efficient while performing favorably against its counterparts.},
  archivePrefix = {arXiv},
  eprint = {1910.03151},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\IVWKARND\\Wang et al. - 2020 - ECA-Net Efficient Channel Attention for Deep Conv.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\T7DFCDSD\\1910.html},
  journal = {arXiv:1910.03151 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-soon},
  note = {Comment: Accepted to CVPR 2020; Project Page: https://github.com/BangguWu/ECANet},
  primaryClass = {cs}
}

@misc{WeightsBiases,
  ids = {WeightsBiasesa},
  title = {Weights \& {{Biases}}},
  abstract = {Weights \& Biases, developer tools for machine learning},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JMP6HRKZ\\Log-ROC,-PR-curves-and-Confusion-Matrices-with-W&B--Vmlldzo3NzQ3MQ.html;C\:\\Users\\ext1150\\Zotero\\storage\\Y7SJIRTR\\Meaning-and-Noise-in-Hyperparameter-Search--Vmlldzo0Mzk5MQ.html},
  howpublished = {https://app.wandb.ai/stacey/pytorch\_intro/reports/Meaning-and-Noise-in-Hyperparameter-Search--Vmlldzo0Mzk5MQ},
  journal = {W\&B},
  language = {en}
}

@article{weissSurveyTransferLearning2016,
  title = {A Survey of Transfer Learning},
  author = {Weiss, Karl and Khoshgoftaar, Taghi M. and Wang, DingDing},
  year = {2016},
  month = may,
  volume = {3},
  pages = {9},
  issn = {2196-1115},
  doi = {10.1186/s40537-016-0043-6},
  abstract = {Machine learning and data mining techniques have been used in numerous real-world applications. An assumption of traditional machine learning methodologies is the training data and testing data are taken from the same domain, such that the input feature space and data distribution characteristics are the same. However, in some real-world machine learning scenarios, this assumption does not hold. There are cases where training data is expensive or difficult to collect. Therefore, there is a need to create high-performance learners trained with more easily obtained data from different domains. This methodology is referred to as transfer learning. This survey paper formally defines transfer learning, presents information on current solutions, and reviews applications applied to transfer learning. Lastly, there is information listed on software downloads for various transfer learning solutions and a discussion of possible future research work. The transfer learning solutions surveyed are independent of data size and can be applied to big data environments.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Z5CMYLX2\\Weiss et al. - 2016 - A survey of transfer learning.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\MEUP7XCP\\s40537-016-0043-6.html},
  journal = {Journal of Big Data},
  number = {1}
}

@misc{westDamageWeRe2020,
  title = {The {{Damage We}}'re {{Not Attending To}}},
  author = {West, David Krakauer \& Geoffrey},
  year = {2020},
  month = jul,
  abstract = {World War II bomber planes returned from their missions riddled with bullet holes. The first response was, not surprisingly, to add\&\#8230;},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\DVTUGQHU\\the-damage-were-not-attending-to.html},
  howpublished = {http://nautil.us/issue/87/risk/the-damage-were-not-attending-to},
  journal = {Nautilus}
}

@article{westLeveragePointsImproving2014,
  title = {Leverage Points for Improving Global Food Security and the Environment},
  author = {West, P. C. and Gerber, J. S. and Engstrom, P. M. and Mueller, N. D. and Brauman, K. A. and Carlson, K. M. and Cassidy, E. S. and Johnston, M. and MacDonald, G. K. and Ray, D. K. and Siebert, S.},
  year = {2014},
  month = jul,
  volume = {345},
  pages = {325--328},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.1246067},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RL5J8EX3\\West et al. - 2014 - Leverage points for improving global food security.pdf},
  journal = {Science},
  language = {en},
  number = {6194}
}

@inbook{wheelerInformationPhysicsQuantum2018,
  title = {Information, {{Physics}}, {{Quantum}}: {{The Search}} for {{Links}}},
  shorttitle = {Information, {{Physics}}, {{Quantum}}},
  booktitle = {Feynman and {{Computation}}},
  author = {Wheeler, John Archibald},
  year = {2018},
  month = mar,
  edition = {First},
  pages = {309--336},
  publisher = {{CRC Press}},
  doi = {10.1201/9780429500459-19},
  abstract = {This report reviews what quantum physics and information theory have to tell us about the age-old question, How come existence? No escape is evident from four conclusions: (1) The world cannot be a giant machine, ruled by any preestablished continuum physical law. (2) There is no such thing at the microscopic level as space or time or spacetime continuum. (3) The familiar probability function or functional, and wave equation or functional wave equation, of standard quantum theory provide mere continuum idealizations and by reason of this circumstance conceal the information-theoretic source from which they derive. (4) No element in the description of physics shows itself as closer to primordial than the elementary quantum phenomenon, that is, the elementary device-intermediated act of posing a yes-no physical question and eliciting an answer or, in brief, the elementary act of observer-participancy. Otherwise stated, every physical quantity, every it, derives its ultimate significance from bits, binary yes-or-no indications, a conclusion which we epitomize in the phrase, it from bit.},
  collaborator = {Hey, Anthony},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SSHKGVNL\\Wheeler - 2018 - Information, Physics, Quantum The Search for Link.pdf},
  isbn = {978-0-429-50045-9},
  language = {en}
}

@article{wignerUnitaryRepresentationsInhomogeneous1939,
  title = {On {{Unitary Representations}} of the {{Inhomogeneous Lorentz Group}}},
  author = {Wigner, E.},
  year = {1939},
  volume = {40},
  pages = {149--204},
  publisher = {{Annals of Mathematics}},
  issn = {0003-486X},
  doi = {10.2307/1968551},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\TR9THE5W\\Wigner - 1939 - On Unitary Representations of the Inhomogeneous Lo.pdf},
  journal = {Annals of Mathematics},
  number = {1}
}

@article{WilhelmConradRontgen2020,
  title = {{Wilhelm Conrad R\"ontgen}},
  year = {2020},
  month = nov,
  abstract = {Wilhelm Conrad R\"ontgen (f\o dt 27. marts 1845, d\o d 10. februar 1923) var en tysk fysiker fra universitetet i W\"urzburg. Han modtog som den f\o rste i 1901 nobelprisen i fysik for sin opdagelse af r\o ntgenstr\aa ling. Han blev f\o dt i en landsby lidt \o st for D\"usseldorf, men hans mor Charlotte Constance var hollandsk, og da R\"ontgen var tre \aa r, flyttede hele familien til Nederlandene, hvor han voksede op. Han var et skolelys, men da han gik ud fra den tekniske h\o jskole i Utrecht som nitten\aa rig, fik han paradoksalt nok slet karakter i \'et fag \textendash{} nemlig fysik, ovenik\o bet zeer slecht (= meget d\aa rligt). Kort tid efter blev han udvist fra skolen, efter at en medelev havde tegnet en karikatur af en upopul\ae r l\ae rer p\aa{} tavlen, og R\"ontgen ikke ville sige, hvem den skyldige var. Dermed var muligheden for en universitetsuddannelse udelukket, indtil en ven af familien fortalte dem, at den tekniske h\o jskole i Z\"urich tog studenter uden formelle papirer ind, bare de bestod en optagelsespr\o ve. Og den bestod R\"ontgen med glans. I november 1865 begyndte han sine studier ved skolen. R\"ontgen studerede p\aa{} Eidgen\"ossische Technische Hochschule i Z\"urich, hvor han tog doktorgraden i 1869. I 1875 blev han professor i fysik i Hohenheim, fra 1876 i Stra\ss burg, fra 1879 i Gie\ss en, fra 1888 i W\"urzburg og endelig fra 1900 til 1920 i M\"unchen. Han besk\ae ftigede sig med krystallografi, termodynamik og elektrodynamik. Mens han eksperimenterede med elektricitet, opdagede han den 8. november 1895 en ny form for str\aa ling, som han kaldte X-str\aa ler, da de var ukendte. Denne betegnelse h\ae nger fortsat ved i det meste af verden, men i visse sprog, blandt andet dansk og tysk, anvendes betegnelsen r\o ntgenstr\aa ling, selvom R\"ontgen selv ikke \o nskede sit navn anvendt i betegnelsen for str\aa lingen. Selv kaldte han str\aa lerne "X-str\aa ler", og det navn har de beholdt p\aa{} engelsk (X-rays).  Det hele begyndte med forskning p\aa{} katodestr\aa ler (= elektriske udladninger i glasr\o r der indeholder fortyndet gas). Andre, der besk\ae ftigede sig med emnet, var nordmanden Christian Birkeland, der lagde grundlaget for teorierne omkring nordlys ved hj\ae lp af sin forskning p\aa{} katodestr\aa ler, og Edison. R\"ontgen p\aa{} sin side eksperimenterede en aften i et m\o rklagt v\ae relse med et katoder\o r, der var indpakket i sort pap. En plade med barium platinocyanid, der l\aa{} p\aa{} et bord lige ved, lyste op med luminescens, hver gang der var en udladning i katoder\o ret. Kun bly og platina var uigennemtr\ae ngelige for de g\aa defulde str\aa ler. R\"ontgen opdagede ogs\aa, at hvis han holdt sin h\aa nd op mellem r\o ret og pladen, kunne han se konturene af knoglerne i h\aa nden. Han studerede nu f\ae nomenet i syv uger, hvor han b\aa de spiste og sov i sit laboratorium. Han bad s\aa{} sin kone Anna Bertha om hj\ae lp: En fotografisk plade blev placeret bagved hendes h\aa nd, der blev bestr\aa let med katoder\o ret i femten minutter. Da pladen blev fremkaldt, s\aa{} man knoglerne i Anna Berthas h\aa nd og de to ringe, hun bar p\aa{} sin ringfinger. Det f\o rste r\o ntgenr\o r blev fremstillet i St\"utzerbach i Th\"uringen efter R\"ontgens anvisninger. Grundstoffet r\o ntgenium er opkaldt efter ham. R\"ontgen tog modvilligt til Stockholm for at modtage sin nobelpris. Nobelforedraget ville han dog ikke holde, da han stod fast p\aa, at det her var jo "gammelt nyt". Han ville heller ikke tage patent p\aa{} sin opfindelse og tjene penge p\aa{} den, s\aa{} Edison greb chancen og gjorde det i stedet for.},
  annotation = {Page Version ID: 10512589},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PMAB42DW\\index.html},
  journal = {Wikipedia, den frie encyklop\ae di},
  language = {da}
}

@incollection{wilsonMarginalValueAdaptive2017,
  title = {The {{Marginal Value}} of {{Adaptive Gradient Methods}} in {{Machine Learning}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 30},
  author = {Wilson, Ashia C and Roelofs, Rebecca and Stern, Mitchell and Srebro, Nati and Recht, Benjamin},
  editor = {Guyon, I. and Luxburg, U. V. and Bengio, S. and Wallach, H. and Fergus, R. and Vishwanathan, S. and Garnett, R.},
  year = {2017},
  pages = {4148--4158},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6HSK8MF5\\Wilson et al. - 2017 - The Marginal Value of Adaptive Gradient Methods in.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\BWALSK86\\7003-the-marginal-value-of-adaptive-gradient-methods-in-machine-learning.html},
  keywords = {lr}
}

@article{winkels3DGCNNsPulmonary2018,
  title = {{{3D G}}-{{CNNs}} for Pulmonary Nodule Detection},
  author = {Winkels, Marysia and Cohen, Taco S.},
  year = {2018},
  archivePrefix = {arXiv},
  eprint = {1804.04656},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YR4UCKN2\\Winkels and Cohen - 2018 - 3D G-CNNs for pulmonary nodule detection.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\WPRY2MKC\\1804.html},
  journal = {arXiv preprint arXiv:1804.04656},
  keywords = {read-maybe}
}

@misc{wolchoverWhyGravityNot,
  title = {Why {{Gravity Is Not Like}} the {{Other Forces}}},
  author = {Wolchover, Natalie},
  abstract = {We asked four physicists why gravity stands out among the forces of nature. We got four different answers.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\R3EYIAHC\\why-gravity-is-not-like-the-other-forces-20200615.html},
  howpublished = {https://www.quantamagazine.org/why-gravity-is-not-like-the-other-forces-20200615/},
  journal = {Quanta Magazine},
  language = {en}
}

@article{wolpertLackPrioriDistinctions,
  title = {The {{Lack}} of {{A Priori Distinctions Between Learning Algorithms}}},
  author = {Wolpert, David H},
  pages = {52},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\FMM9MHZL\\Wolpert - The Lack of A Priori Distinctions Between Learning.pdf},
  language = {en}
}

@article{wooCBAMConvolutionalBlock2018,
  title = {{{CBAM}}: {{Convolutional Block Attention Module}}},
  shorttitle = {{{CBAM}}},
  author = {Woo, Sanghyun and Park, Jongchan and Lee, Joon-Young and Kweon, In So},
  year = {2018},
  month = jul,
  abstract = {We propose Convolutional Block Attention Module (CBAM), a simple yet effective attention module for feed-forward convolutional neural networks. Given an intermediate feature map, our module sequentially infers attention maps along two separate dimensions, channel and spatial, then the attention maps are multiplied to the input feature map for adaptive feature refinement. Because CBAM is a lightweight and general module, it can be integrated into any CNN architectures seamlessly with negligible overheads and is end-to-end trainable along with base CNNs. We validate our CBAM through extensive experiments on ImageNet-1K, MS\textasciitilde COCO detection, and VOC\textasciitilde 2007 detection datasets. Our experiments show consistent improvements in classification and detection performances with various models, demonstrating the wide applicability of CBAM. The code and models will be publicly available.},
  archivePrefix = {arXiv},
  eprint = {1807.06521},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\U88TDL9K\\Woo et al. - 2018 - CBAM Convolutional Block Attention Module.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\WXU8IKAN\\1807.html},
  journal = {arXiv:1807.06521 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-soon},
  note = {Comment: Accepted to ECCV 2018},
  primaryClass = {cs}
}

@misc{woodHowMathematicalHocusPocus,
  title = {How {{Mathematical}} `{{Hocus}}-{{Pocus}}' {{Saved Particle Physics}}},
  author = {Wood, Charlie},
  abstract = {Renormalization has become perhaps the single most important advance in theoretical physics in 50 years.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\Z62QTV8P\\how-renormalization-saved-particle-physics-20200917.html},
  howpublished = {https://www.quantamagazine.org/how-renormalization-saved-particle-physics-20200917/},
  journal = {Quanta Magazine},
  language = {en}
}

@misc{wrightLessw2020RangerDeepLearningOptimizer2020,
  title = {Lessw2020/{{Ranger}}-{{Deep}}-{{Learning}}-{{Optimizer}}},
  author = {Wright, Less},
  year = {2020},
  month = oct,
  abstract = {Ranger},
  copyright = {Apache-2.0 License         ,                 Apache-2.0 License},
  keywords = {lr}
}

@article{wuCurrentTimeSeries,
  title = {Current {{Time Series Anomaly Detection Benchmarks}} Are {{Flawed}} and Are {{Creating}} the {{Illusion}} of {{Progress}}},
  author = {Wu, Renjie and Keogh, Eamonn J},
  pages = {9},
  abstract = {Time series anomaly detection has been a perennially important topic in data science, with papers dating back to the 1950s. However, in recent years there has been an explosion of interest in this topic, much of it driven by the success of deep learning in other domains and for other time series tasks. Most of these papers test on one or more of a handful of popular benchmark datasets, created by Yahoo, Numenta, NASA, etc. In this work we make a surprising claim. The majority of the individual exemplars in these datasets suffer from one or more of four flaws. Because of these four flaws, we believe that many published comparisons of anomaly detection algorithms may be unreliable, and more importantly, much of the apparent progress in recent years may be illusionary. In addition to demonstrating these claims, with this paper we introduce the UCR Time Series Anomaly Datasets. We believe that this resource will perform a similar role as the UCR Time Series Classification Archive, by providing the community with a benchmark that allows meaningful comparisons between approaches and a meaningful gauge of overall progress.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GPIYQXHY\\Wu and Keogh - Current Time Series Anomaly Detection Benchmarks a.pdf},
  keywords = {read-next},
  language = {en}
}

@article{wuGroupNormalization2018,
  title = {Group {{Normalization}}},
  author = {Wu, Yuxin and He, Kaiming},
  year = {2018},
  month = jun,
  abstract = {Batch Normalization (BN) is a milestone technique in the development of deep learning, enabling various networks to train. However, normalizing along the batch dimension introduces problems --- BN's error increases rapidly when the batch size becomes smaller, caused by inaccurate batch statistics estimation. This limits BN's usage for training larger models and transferring features to computer vision tasks including detection, segmentation, and video, which require small batches constrained by memory consumption. In this paper, we present Group Normalization (GN) as a simple alternative to BN. GN divides the channels into groups and computes within each group the mean and variance for normalization. GN's computation is independent of batch sizes, and its accuracy is stable in a wide range of batch sizes. On ResNet-50 trained in ImageNet, GN has 10.6\% lower error than its BN counterpart when using a batch size of 2; when using typical batch sizes, GN is comparably good with BN and outperforms other normalization variants. Moreover, GN can be naturally transferred from pre-training to fine-tuning. GN can outperform its BN-based counterparts for object detection and segmentation in COCO, and for video classification in Kinetics, showing that GN can effectively replace the powerful BN in a variety of tasks. GN can be easily implemented by a few lines of code in modern libraries.},
  archivePrefix = {arXiv},
  eprint = {1803.08494},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\HMXG5SPA\\Wu and He - 2018 - Group Normalization.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\6NXA7RUJ\\1803.html},
  journal = {arXiv:1803.08494 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-soon},
  note = {Comment: v3: Update trained-from-scratch results in COCO to 41.0AP. Code and models at https://github.com/facebookresearch/Detectron/blob/master/projects/GN},
  primaryClass = {cs}
}

@inproceedings{xieAggregatedResidualTransformations2017,
  title = {Aggregated {{Residual Transformations}} for {{Deep Neural Networks}}},
  booktitle = {2017 {{IEEE Conference}} on {{Computer Vision}} and {{Pattern Recognition}} ({{CVPR}})},
  author = {Xie, Saining and Girshick, Ross and Dollar, Piotr and Tu, Zhuowen and He, Kaiming},
  year = {2017},
  month = jul,
  pages = {5987--5995},
  publisher = {{IEEE}},
  address = {{Honolulu, HI}},
  doi = {10.1109/CVPR.2017.634},
  abstract = {We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call ``cardinality'' (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online1.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PKQPA7LN\\Xie et al. - 2017 - Aggregated Residual Transformations for Deep Neura.pdf},
  isbn = {978-1-5386-0457-1},
  keywords = {read-next},
  language = {en}
}

@article{xieExplainableDeepLearning2020,
  title = {Explainable {{Deep Learning}}: {{A Field Guide}} for the {{Uninitiated}}},
  shorttitle = {Explainable {{Deep Learning}}},
  author = {Xie, Ning and Ras, Gabrielle and {van Gerven}, Marcel and Doran, Derek},
  year = {2020},
  month = apr,
  abstract = {The deep neural network (DNN) is an indispensable machine learning tool for achieving human-level performance on many learning tasks. Yet, due to its black-box nature, it is inherently difficult to understand which aspects of the input data drive the decisions of the network. There are various real-world scenarios in which humans need to make actionable decisions based on the output of a decision support system that makes use of DNNs. These decision support systems can be found in critical domains, such as legislation, law enforcement, and healthcare. It is important that the humans making high-level decisions can be sure that the DNN decisions are driven by combinations of data features that are appropriate in the context of the deployment of the decision support system and that the decisions made are legally or ethically defensible. Due to the incredible pace at which DNN technology is being developed and adopted, the development of new methods and studies on explaining the decision-making process of DNNs has blossomed into an active research field. A practitioner beginning to study explainable deep learning may be intimidated by the plethora of orthogonal directions the field is taking. This complexity is further exacerbated by the general confusion that exists in defining what it means to be able to explain the actions of a deep learning system and to evaluate a system's ``ability to explain''. To alleviate this problem, this article offers a ``field guide'' to deep learning explainability for those uninitiated in the field. The field guide: i) Discusses the traits of a deep learning system that researchers enhance in explainability research, ii) places explainability in the context of other related deep learning research areas, and iii) introduces three simple dimensions defining the space of foundational methods that contribute to explainable deep learning. The guide is designed as an easy-to-digest starting point for those just embarking in the field.},
  archivePrefix = {arXiv},
  eprint = {2004.14545},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2L45HNMK\\2004.14545.pdf},
  journal = {arXiv:2004.14545 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,read-someday,Statistics - Machine Learning},
  language = {en},
  note = {Comment: Survey paper on Explainable Deep Learning, 54 pages including references},
  primaryClass = {cs, stat}
}

@inproceedings{xieExploringRandomlyWired2019,
  title = {Exploring {{Randomly Wired Neural Networks}} for {{Image Recognition}}},
  booktitle = {2019 {{IEEE}}/{{CVF International Conference}} on {{Computer Vision}} ({{ICCV}})},
  author = {Xie, Saining and Kirillov, Alexander and Girshick, Ross and He, Kaiming},
  year = {2019},
  month = oct,
  pages = {1284--1293},
  publisher = {{IEEE}},
  address = {{Seoul, Korea (South)}},
  doi = {10.1109/ICCV.2019.00137},
  abstract = {Neural networks for image recognition have evolved through extensive manual design from simple chain-like models to structures with multiple wiring paths. The success of ResNets [12] and DenseNets [17] is due in large part to their innovative wiring plans. Now, neural architecture search (NAS) studies are exploring the joint optimization of wiring and operation types, however, the space of possible wirings is constrained and still driven by manual design despite being searched. In this paper, we explore a more diverse set of connectivity patterns through the lens of randomly wired neural networks. To do this, we first define the concept of a stochastic network generator that encapsulates the entire network generation process. Encapsulation provides a unified view of NAS and randomly wired networks. Then, we use three classical random graph models to generate randomly wired graphs for networks. The results are surprising: several variants of these random generators yield network instances that have competitive accuracy on the ImageNet benchmark. These results suggest that new efforts focusing on designing better network generators may lead to new breakthroughs by exploring less constrained search spaces with more room for novel design. The code is publicly available online1.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LSFJVE8Q\\Xie et al. - 2019 - Exploring Randomly Wired Neural Networks for Image.pdf},
  isbn = {978-1-72814-803-8},
  keywords = {read-next},
  language = {en}
}

@article{Xray2020,
  title = {X-Ray},
  year = {2020},
  month = may,
  abstract = {X-rays make up X-radiation, a form of high-energy electromagnetic radiation. Most X-rays have a wavelength ranging from 10 picometres to 10 nanometres, corresponding to frequencies in the range 30 petahertz to 30 exahertz (3\texttimes 1016 Hz to 3\texttimes 1019 Hz) and energies in the range 100 eV to 200 keV. X-ray wavelengths are shorter than those of UV rays and typically longer than those of gamma rays. In many languages, X-radiation is referred to as R\"ontgen radiation, after the German scientist Wilhelm R\"ontgen, who discovered it on November 8, 1895. He named it X-radiation to signify an unknown type of radiation. Spellings of X-ray(s) in English include the variants x-ray(s), xray(s), and X ray(s).},
  annotation = {Page Version ID: 959268120},
  copyright = {Creative Commons Attribution-ShareAlike License},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\EFBTWIKS\\index.html},
  journal = {Wikipedia},
  language = {en}
}

@misc{XrayDetectionDefects,
  title = {X-Ray Detection of Defects and Contaminants in the Food Industry | {{SpringerLink}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\4PPUQP4B\\s11694-008-9059-8.html},
  howpublished = {https://link.springer.com/article/10.1007/s11694-008-9059-8},
  keywords = {introduction}
}

@misc{XRayOpticsCalculator,
  title = {X-{{Ray Optics Calculator}}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\P7HNXSDW\\ref_index.html},
  howpublished = {http://purple.ipmt-hpm.ac.ru/xcalc/xcalc\_mysql/ref\_index.php}
}

@article{xuHowNeuralNetworks2020,
  title = {How {{Neural Networks Extrapolate}}: {{From Feedforward}} to {{Graph Neural Networks}}},
  shorttitle = {How {{Neural Networks Extrapolate}}},
  author = {Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  year = {2020},
  month = sep,
  abstract = {We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while multilayer perceptrons (MLPs) do not extrapolate well in simple tasks, Graph Neural Networks (GNNs), a structured network with MLP modules, have some success in more complex tasks. We provide a theoretical explanation and identify conditions under which MLPs and GNNs extrapolate well. We start by showing ReLU MLPs trained by gradient descent converge quickly to linear functions along any direction from the origin, which suggests ReLU MLPs cannot extrapolate well in most non-linear tasks. On the other hand, ReLU MLPs can provably converge to a linear target function when the training distribution is "diverse" enough. These observations lead to a hypothesis: GNNs can extrapolate well in dynamic programming (DP) tasks if we encode appropriate non-linearity in the architecture and input representation. We provide theoretical and empirical support for the hypothesis. Our theory explains previous extrapolation success and suggest their limitations: successful extrapolation relies on incorporating task-specific non-linearity, which often requires domain knowledge or extensive model search.},
  archivePrefix = {arXiv},
  eprint = {2009.11848},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\6IAXURAJ\\Xu et al. - 2020 - How Neural Networks Extrapolate From Feedforward .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\5ZNKRTDY\\2009.html},
  journal = {arXiv:2009.11848 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-next,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{xuHowNeuralNetworks2020a,
  title = {How {{Neural Networks Extrapolate}}: {{From Feedforward}} to {{Graph Neural Networks}}},
  shorttitle = {How {{Neural Networks Extrapolate}}},
  author = {Xu, Keyulu and Li, Jingling and Zhang, Mozhi and Du, Simon S. and Kawarabayashi, Ken-ichi and Jegelka, Stefanie},
  year = {2020},
  month = sep,
  abstract = {We study how neural networks trained by gradient descent extrapolate, i.e., what they learn outside the support of the training distribution. Previous works report mixed empirical results when extrapolating with neural networks: while multilayer perceptrons (MLPs) do not extrapolate well in certain simple tasks, Graph Neural Network (GNN), a structured network with MLP modules, has shown some success in more complex tasks. Working towards a theoretical explanation, we identify conditions under which MLPs and GNNs extrapolate well. First, we quantify the observation that ReLU MLPs quickly converge to linear functions along any direction from the origin, which implies that ReLU MLPs do not extrapolate most non-linear functions. But, they can provably learn a linear target function when the training distribution is sufficiently "diverse". Second, in connection to analyzing successes and limitations of GNNs, these results suggest a hypothesis for which we provide theoretical and empirical evidence: the success of GNNs in extrapolating algorithmic tasks to new data (e.g., larger graphs or edge weights) relies on encoding task-specific non-linearities in the architecture or features.},
  archivePrefix = {arXiv},
  eprint = {2009.11848},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VH8NUQUX\\Xu et al. - 2020 - How Neural Networks Extrapolate From Feedforward .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\RWV74RTH\\2009.html},
  journal = {arXiv:2009.11848 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{yamashitaConvolutionalNeuralNetworks2018,
  title = {Convolutional Neural Networks: An Overview and Application in Radiology},
  shorttitle = {Convolutional Neural Networks},
  author = {Yamashita, Rikiya and Nishio, Mizuho and Do, Richard Kinh Gian and Togashi, Kaori},
  year = {2018},
  month = aug,
  volume = {9},
  pages = {611--629},
  issn = {1869-4101},
  doi = {10.1007/s13244-018-0639-9},
  abstract = {Convolutional neural network (CNN), a class of artificial neural networks that has become dominant in various computer vision tasks, is attracting interest across a variety of domains, including radiology. CNN is designed to automatically and adaptively learn spatial hierarchies of features through backpropagation by using multiple building blocks, such as convolution layers, pooling layers, and fully connected layers. This review article offers a perspective on the basic concepts of CNN and its application to various radiological tasks, and discusses its challenges and future directions in the field of radiology. Two challenges in applying CNN to radiological tasks, small dataset and overfitting, will also be covered in this article, as well as techniques to minimize them. Being familiar with the concepts and advantages, as well as limitations, of CNN is essential to leverage its potential in diagnostic radiology, with the goal of augmenting the performance of radiologists and improving patient care.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\IZ3QQMBG\\Yamashita et al. - 2018 - Convolutional neural networks an overview and app.pdf},
  journal = {Insights into Imaging},
  keywords = {read-half},
  language = {en},
  number = {4}
}

@article{yangQlibAIorientedQuantitative2020,
  title = {Qlib: {{An AI}}-Oriented {{Quantitative Investment Platform}}},
  shorttitle = {Qlib},
  author = {Yang, Xiao and Liu, Weiqing and Zhou, Dong and Bian, Jiang and Liu, Tie-Yan},
  year = {2020},
  month = sep,
  abstract = {Quantitative investment aims to maximize the return and minimize the risk in a sequential trading period over a set of financial instruments. Recently, inspired by rapid development and great potential of AI technologies in generating remarkable innovation in quantitative investment, there has been increasing adoption of AI-driven workflow for quantitative research and practical investment. In the meantime of enriching the quantitative investment methodology, AI technologies have raised new challenges to the quantitative investment system. Particularly, the new learning paradigms for quantitative investment call for an infrastructure upgrade to accommodate the renovated workflow; moreover, the data-driven nature of AI technologies indeed indicates a requirement of the infrastructure with more powerful performance; additionally, there exist some unique challenges for applying AI technologies to solve different tasks in the financial scenarios. To address these challenges and bridge the gap between AI technologies and quantitative investment, we design and develop Qlib that aims to realize the potential, empower the research, and create the value of AI technologies in quantitative investment.},
  archivePrefix = {arXiv},
  eprint = {2009.11189},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RIJJ76SA\\Yang et al. - 2020 - Qlib An AI-oriented Quantitative Investment Platf.pdf},
  journal = {arXiv:2009.11189 [cs, q-fin]},
  keywords = {Computer Science - Machine Learning,Quantitative Finance - General Finance,Quantitative Finance - Portfolio Management},
  language = {en},
  primaryClass = {cs, q-fin}
}

@article{yangRethinkingValueLabels2020,
  title = {Rethinking the {{Value}} of {{Labels}} for {{Improving Class}}-{{Imbalanced Learning}}},
  author = {Yang, Yuzhe and Xu, Zhi},
  year = {2020},
  month = sep,
  abstract = {Real-world data often exhibits long-tailed distributions with heavy class imbalance, posing great challenges for deep recognition models. We identify a persisting dilemma on the value of labels in the context of imbalanced learning: on the one hand, supervision from labels typically leads to better results than its unsupervised counterparts; on the other hand, heavily imbalanced data naturally incurs "label bias" in the classifier, where the decision boundary can be drastically altered by the majority classes. In this work, we systematically investigate these two facets of labels. We demonstrate, theoretically and empirically, that class-imbalanced learning can significantly benefit in both semi-supervised and self-supervised manners. Specifically, we confirm that (1) positively, imbalanced labels are valuable: given more unlabeled data, the original labels can be leveraged with the extra data to reduce label bias in a semi-supervised manner, which greatly improves the final classifier; (2) negatively however, we argue that imbalanced labels are not useful always: classifiers that are first pre-trained in a self-supervised manner consistently outperform their corresponding baselines. Extensive experiments on large-scale imbalanced datasets verify our theoretically grounded strategies, showing superior performance over previous state-of-the-arts. Our intriguing findings highlight the need to rethink the usage of imbalanced labels in realistic long-tailed tasks. Code is available at https://github.com/YyzHarry/imbalanced-semi-self.},
  archivePrefix = {arXiv},
  eprint = {2006.07529},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\YNJZWCPE\\Yang and Xu - 2020 - Rethinking the Value of Labels for Improving Class.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\EVBPB7YZ\\2006.html},
  journal = {arXiv:2006.07529 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,read-next,Statistics - Machine Learning},
  note = {Comment: NeurIPS 2020},
  primaryClass = {cs, stat}
}

@article{yaoADAHESSIANAdaptiveSecond2020,
  title = {{{ADAHESSIAN}}: {{An Adaptive Second Order Optimizer}} for {{Machine Learning}}},
  shorttitle = {{{ADAHESSIAN}}},
  author = {Yao, Zhewei and Gholami, Amir and Shen, Sheng and Keutzer, Kurt and Mahoney, Michael W.},
  year = {2020},
  month = jun,
  abstract = {We introduce AdaHessian, a second order stochastic optimization algorithm which dynamically incorporates the curvature of the loss function via ADAptive estimates of the Hessian. Second order algorithms are among the most powerful optimization algorithms with superior convergence properties as compared to first order methods such as SGD and ADAM. The main disadvantage of traditional second order methods is their heavier per-iteration computation and poor accuracy as compared to first order methods. To address these, we incorporate several novel approaches in AdaHessian, including: (i) a new variance reduction estimate of the Hessian diagonal with low computational overhead; (ii) a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal across different iterations; and (iii) a block diagonal averaging to reduce the variance of Hessian diagonal elements. We show that AdaHessian achieves new state-of-the-art results by a large margin as compared to other adaptive optimization methods, including variants of ADAM. In particular, we perform extensive tests on CV, NLP, and recommendation system tasks and find that AdaHessian: (i) achieves 1.80\textbackslash\%/1.45\textbackslash\% higher accuracy on ResNets20/32 on Cifar10, and 5.55\textbackslash\% higher accuracy on ImageNet as compared to ADAM; (ii) outperforms ADAMW for transformers by 0.27/0.33 BLEU score on IWSLT14/WMT14 and 1.8/1.0 PPL on PTB/Wikitext-103; and (iii) achieves 0.032\textbackslash\% better score than AdaGrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the cost per iteration of AdaHessian is comparable to first-order methods, and that it exhibits robustness towards its hyperparameters. The code for AdaHessian is open-sourced and publicly available.},
  archivePrefix = {arXiv},
  eprint = {2006.00719},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\D3C6AJEH\\Yao et al. - 2020 - ADAHESSIAN An Adaptive Second Order Optimizer for.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\TQ2P89Z6\\2006.html},
  journal = {arXiv:2006.00719 [cs, math, stat]},
  keywords = {Computer Science - Machine Learning,lr,Mathematics - Numerical Analysis,Statistics - Machine Learning},
  primaryClass = {cs, math, stat}
}

@article{yaoPyHessianNeuralNetworks2020,
  title = {{{PyHessian}}: {{Neural Networks Through}} the {{Lens}} of the {{Hessian}}},
  shorttitle = {{{PyHessian}}},
  author = {Yao, Zhewei and Gholami, Amir and Keutzer, Kurt and Mahoney, Michael},
  year = {2020},
  month = mar,
  abstract = {We present PYHESSIAN, a new scalable framework that enables fast computation of Hessian (i.e., second-order derivative) information for deep neural networks. PYHESSIAN enables fast computations of the top Hessian eigenvalues, the Hessian trace, and the full Hessian eigenvalue/spectral density, and it supports distributed-memory execution on cloud/supercomputer systems and is available as open source [1]. This general framework can be used to analyze neural network models, including the topology of the loss landscape (i.e., curvature information) to gain insight into the behavior of different models/optimizers. To illustrate this, we analyze the effect of residual connections and Batch Normalization layers on the trainability of neural networks. One recent claim, based on simpler first-order analysis, is that residual connections and Batch Normalization make the loss landscape ``smoother'', thus making it easier for Stochastic Gradient Descent to converge to a good solution. Our extensive analysis shows new finer-scale insights, demonstrating that, while conventional wisdom is sometimes validated, in other cases it is simply incorrect. In particular, we find that Batch Normalization does not necessarily make the loss landscape smoother, especially for shallower networks.},
  archivePrefix = {arXiv},
  eprint = {1912.07145},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\5NLH8KUU\\Yao et al. - 2020 - PyHessian Neural Networks Through the Lens of the.pdf},
  journal = {arXiv:1912.07145 [cs, math]},
  keywords = {Computer Science - Machine Learning,Mathematics - Numerical Analysis},
  language = {en},
  primaryClass = {cs, math}
}

@misc{yellapragadaUnderstandingLossFunctions2020,
  title = {Understanding {{Loss Functions}} in {{Computer Vision}}!},
  author = {Yellapragada, Sowmya},
  year = {2020},
  month = feb,
  abstract = {Choosing the right loss function can optimize the model convergence, also help to focus on the right set of features in model training.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VC3ZCHWB\\winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a.html},
  howpublished = {https://medium.com/ml-cheat-sheet/winning-at-loss-functions-2-important-loss-functions-in-computer-vision-b2b9d293e15a},
  journal = {Medium},
  keywords = {read-soon},
  language = {en}
}

@article{yongGradientCentralizationNew2020,
  title = {Gradient {{Centralization}}: {{A New Optimization Technique}} for {{Deep Neural Networks}}},
  shorttitle = {Gradient {{Centralization}}},
  author = {Yong, Hongwei and Huang, Jianqiang and Hua, Xiansheng and Zhang, Lei},
  year = {2020},
  month = apr,
  abstract = {Optimization techniques are of great importance to effectively and efficiently train a deep neural network (DNN). It has been shown that using the first and second order statistics (e.g., mean and variance) to perform Z-score standardization on network activations or weight vectors, such as batch normalization (BN) and weight standardization (WS), can improve the training performance. Different from these existing methods that mostly operate on activations or weights, we present a new optimization technique, namely gradient centralization (GC), which operates directly on gradients by centralizing the gradient vectors to have zero mean. GC can be viewed as a projected gradient descent method with a constrained loss function. We show that GC can regularize both the weight space and output feature space so that it can boost the generalization performance of DNNs. Moreover, GC improves the Lipschitzness of the loss function and its gradient so that the training process becomes more efficient and stable. GC is very simple to implement and can be easily embedded into existing gradient based DNN optimizers with only one line of code. It can also be directly used to fine-tune the pre-trained DNNs. Our experiments on various applications, including general image classification, fine-grained image classification, detection and segmentation, demonstrate that GC can consistently improve the performance of DNN learning. The code of GC can be found at https://github.com/Yonghongwei/Gradient-Centralization.},
  archivePrefix = {arXiv},
  eprint = {2004.01461},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\AFID4KQF\\Yong et al. - 2020 - Gradient Centralization A New Optimization Techni.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\2BZKIQDV\\2004.html},
  journal = {arXiv:2004.01461 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,lr},
  note = {Comment: 20 pages, 7 figures, conference},
  primaryClass = {cs}
}

@article{yosinskiHowTransferableAre,
  title = {How Transferable Are Features in Deep Neural Networks?},
  author = {Yosinski, Jason and Clune, Jeff and Bengio, Yoshua and Lipson, Hod},
  pages = {9},
  abstract = {Many deep neural networks trained on natural images exhibit a curious phenomenon in common: on the first layer they learn features similar to Gabor filters and color blobs. Such first-layer features appear not to be specific to a particular dataset or task, but general in that they are applicable to many datasets and tasks. Features must eventually transition from general to specific by the last layer of the network, but this transition has not been studied extensively. In this paper we experimentally quantify the generality versus specificity of neurons in each layer of a deep convolutional neural network and report a few surprising results. Transferability is negatively affected by two distinct issues: (1) the specialization of higher layer neurons to their original task at the expense of performance on the target task, which was expected, and (2) optimization difficulties related to splitting networks between co-adapted neurons, which was not expected. In an example network trained on ImageNet, we demonstrate that either of these two issues may dominate, depending on whether features are transferred from the bottom, middle, or top of the network. We also document that the transferability of features decreases as the distance between the base task and target task increases, but that transferring features even from distant tasks can be better than using random features. A final surprising result is that initializing a network with transferred features from almost any number of layers can produce a boost to generalization that lingers even after fine-tuning to the target dataset.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SLL7ZM6M\\Yosinski et al. - How transferable are features in deep neural netwo.pdf},
  language = {en}
}

@article{yosinskiUnderstandingNeuralNetworks2015,
  title = {Understanding Neural Networks through Deep Visualization},
  author = {Yosinski, Jason and Clune, Jeff and Nguyen, Anh and Fuchs, Thomas and Lipson, Hod},
  year = {2015},
  archivePrefix = {arXiv},
  eprint = {1506.06579},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\SQCDX8IB\\Yosinski et al. - 2015 - Understanding neural networks through deep visuali.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\9X46LPVB\\1506.html},
  journal = {arXiv preprint arXiv:1506.06579}
}

@incollection{youGateDecoratorGlobal2019,
  title = {Gate {{Decorator}}: {{Global Filter Pruning Method}} for {{Accelerating Deep Convolutional Neural Networks}}},
  shorttitle = {Gate {{Decorator}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 32},
  author = {You, Zhonghui and Yan, Kun and Ye, Jinmian and Ma, Meng and Wang, Ping},
  editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and d{\textbackslash}textquotesingle {Alch{\'e}-Buc}, F. and Fox, E. and Garnett, R.},
  year = {2019},
  pages = {2133--2144},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GAP4WAW7\\You et al. - 2019 - Gate Decorator Global Filter Pruning Method for A.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\WK69Y59H\\8486-gate-decorator-global-filter-pruning-method-for-accelerating-deep-convolutional-neural-net.html},
  keywords = {read}
}

@incollection{zaheerAdaptiveMethodsNonconvex2018,
  title = {Adaptive {{Methods}} for {{Nonconvex Optimization}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Zaheer, Manzil and Reddi, Sashank and Sachan, Devendra and Kale, Satyen and Kumar, Sanjiv},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {9793--9803},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\JSLI76A8\\Zaheer et al. - 2018 - Adaptive Methods for Nonconvex Optimization.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\HP9UMLV9\\8186-adaptive-methods-for-nonconvex-optimization.html},
  keywords = {lr,read,YOGI}
}

@article{zeilerVisualizingUnderstandingConvolutional2013,
  title = {Visualizing and {{Understanding Convolutional Networks}}},
  author = {Zeiler, Matthew D. and Fergus, Rob},
  year = {2013},
  month = nov,
  abstract = {Large Convolutional Network models have recently demonstrated impressive classification performance on the ImageNet benchmark. However there is no clear understanding of why they perform so well, or how they might be improved. In this paper we address both issues. We introduce a novel visualization technique that gives insight into the function of intermediate feature layers and the operation of the classifier. We also perform an ablation study to discover the performance contribution from different model layers. This enables us to find model architectures that outperform Krizhevsky \textbackslash etal on the ImageNet classification benchmark. We show our ImageNet model generalizes well to other datasets: when the softmax classifier is retrained, it convincingly beats the current state-of-the-art results on Caltech-101 and Caltech-256 datasets.},
  archivePrefix = {arXiv},
  eprint = {1311.2901},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VC3589A7\\Zeiler and Fergus - 2013 - Visualizing and Understanding Convolutional Networ.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\VL8QLG9U\\1311.html},
  journal = {arXiv:1311.2901 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read-next},
  primaryClass = {cs}
}

@misc{ZettelkastenMethodLessWrong,
  title = {The {{Zettelkasten Method}} - {{LessWrong}} 2.0},
  abstract = {Early this year, Conor White-Sullivan introduced me to the Zettelkasten method of note-taking. I would say that this significantly increased my research productivity. I've been saying ``at least 2x''. Naturally, this sort of thing is difficult to quantify. The truth is, I think it may be more like 3x, especially along the dimension of ``producing ideas'' and also ``early-stage development of ideas''. (What I mean by this will become clearer as I describe how I think about research productivity more generally.) However, it is also very possible that the method produces serious biases in the types of ideas produced/developed, which should be considered. (This would be difficult to quantify at the best of times, but also, it should be noted that other factors have dramatically  decreased my overall research productivity. So, unfortunately, someone looking in from outside would not see an overall boost. Still, my impression is that it's been very useful.) I think there are some specific reasons why Zettelkasten has worked so well for me. I'll try to make those clear, to help readers decide whether it would work for them. However, I honestly didn't think Zettelkasten sounded like a good idea before I tried it. It only took me about 30 minutes of working with the cards to decide that it was really good. So, if you're like me, this is a cheap experiment. I think a lot of people should actually try it to see how they like it, even if it sounds terrible. My plan for this document is to first give a short summary and then an overview of Zettelkasten, so that readers know roughly what I'm talking about, and can possibly experiment with it without reading any further. I'll then launch into a longer discussion of why it worked well for me, explaining the specific habits which I think contributed, including some descriptions of my previous approaches to keeping research notes. I expect some of this may be useful even if you don't use Zettelkasten -- if Zettelkasten isn't for you, may},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\AL4CPIF2\\the-zettelkasten-method-1.html},
  howpublished = {https://www.lesswrong.com/posts/NfdHG6oHBJ8Qxc26s/the-zettelkasten-method-1}
}

@article{zhangEffectivenessScaledExponentiallyRegularized2018,
  title = {Effectiveness of {{Scaled Exponentially}}-{{Regularized Linear Units}} ({{SERLUs}})},
  author = {Zhang, G. and Li, H.},
  year = {2018},
  month = jul,
  abstract = {Recently, self-normalizing neural networks (SNNs) have been proposed with the intention to avoid batch or weight normalization. The key step in SNNs is to properly scale the exponential linear unit (referred to as SELU) to inherently incorporate normalization based on central limit theory. SELU is a monotonically increasing function, where it has an approximately constant negative output for large negative input. In this work, we propose a new activation function to break the monotonicity property of SELU while still preserving the self-normalizing property. Differently from SELU, the new function introduces a bump-shaped function in the region of negative input by regularizing a linear function with a scaled exponential function, which is referred to as a scaled exponentially-regularized linear unit (SERLU). The bump-shaped function has approximately zero response to large negative input while being able to push the output of SERLU towards zero mean statistically. To effectively combat over-fitting, we develop a so-called shift-dropout for SERLU, which includes standard dropout as a special case. Experimental results on MNIST, CIFAR10 and CIFAR100 show that SERLU-based neural networks provide consistently promising results in comparison to other 5 activation functions including ELU, SELU, Swish, Leakly ReLU and ReLU.},
  archivePrefix = {arXiv},
  eprint = {1807.10117},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\PHT72CHQ\\Zhang and Li - 2018 - Effectiveness of Scaled Exponentially-Regularized .pdf},
  journal = {arXiv:1807.10117 [cs, stat]},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Statistics - Machine Learning},
  language = {en},
  note = {Comment: 9 pages},
  primaryClass = {cs, stat}
}

@article{zhangErasingAppearancePreservation,
  title = {Erasing {{Appearance Preservation}} in {{Optimization}}-Based {{Smoothing}}},
  author = {Zhang, Lvmin and Li, Chengze and Ji, Yi and Liu, Chunping and Wong, Tien-tsin},
  pages = {16},
  abstract = {Optimization-based image smoothing is routinely formulated as the game between a smoothing energy and an appearance preservation energy. Achieving adequate smoothing is a fundamental goal of these image smoothing algorithms. We show that partially ``erasing'' the appearance preservation facilitate adequate image smoothing. In this paper, we call this manipulation as Erasing Appearance Preservation (EAP). We conduct an user study, allowing users to indicate the ``erasing'' positions by drawing scribbles interactively, to verify the correctness and effectiveness of EAP. We observe the characteristics of human-indicated ``erasing'' positions, and then formulate a simple and effective 0-1 knapsack to automatically synthesize the ``erasing'' positions. We test our synthesized erasing positions in a majority of image smoothing methods. Experimental results and large-scale perceptual human judgments show that the EAP solution tends to encourage the pattern separation or elimination capabilities of image smoothing algorithms. We further study the performance of the EAP solution in many image decomposition problems to decompose textures, shadows, and the challenging specular reflections. We also present examinations of diversiform image manipulation applications like texture removal, retexturing, intrinsic decomposition, layer extraction, recoloring, material manipulation, etc. Due to the widespread applicability of image smoothing, the EAP is also likely to be used in more image editing applications.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\LT36MRTW\\Zhang et al. - Erasing Appearance Preservation in Optimization-ba.pdf},
  keywords = {read-soon},
  language = {en}
}

@article{zhangLookaheadOptimizerSteps2019,
  title = {Lookahead {{Optimizer}}: K Steps Forward, 1 Step Back},
  shorttitle = {Lookahead {{Optimizer}}},
  author = {Zhang, Michael R. and Lucas, James and Hinton, Geoffrey and Ba, Jimmy},
  year = {2019},
  month = dec,
  abstract = {The vast majority of successful deep neural networks are trained using variants of stochastic gradient descent (SGD) algorithms. Recent attempts to improve SGD can be broadly categorized into two approaches: (1) adaptive learning rate schemes, such as AdaGrad and Adam, and (2) accelerated schemes, such as heavy-ball and Nesterov momentum. In this paper, we propose a new optimization algorithm, Lookahead, that is orthogonal to these previous approaches and iteratively updates two sets of weights. Intuitively, the algorithm chooses a search direction by looking ahead at the sequence of fast weights generated by another optimizer. We show that Lookahead improves the learning stability and lowers the variance of its inner optimizer with negligible computation and memory cost. We empirically demonstrate Lookahead can significantly improve the performance of SGD and Adam, even with their default hyperparameter settings on ImageNet, CIFAR-10/100, neural machine translation, and Penn Treebank.},
  archivePrefix = {arXiv},
  eprint = {1907.08610},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\J4MDTWFF\\Zhang et al. - 2019 - Lookahead Optimizer k steps forward, 1 step back.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\KWCD94G8\\1907.html},
  journal = {arXiv:1907.08610 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,lr,Statistics - Machine Learning},
  note = {Comment: Accepted to Neural Information Processing Systems 2019. Code available at: https://github.com/michaelrzhang/lookahead},
  primaryClass = {cs, stat}
}

@article{zhangResNeStSplitAttentionNetworks2020,
  title = {{{ResNeSt}}: {{Split}}-{{Attention Networks}}},
  shorttitle = {{{ResNeSt}}},
  author = {Zhang, Hang and Wu, Chongruo and Zhang, Zhongyue and Zhu, Yi and Zhang, Zhi and Lin, Haibin and Sun, Yue and He, Tong and Mueller, Jonas and Manmatha, R. and Li, Mu and Smola, Alexander},
  year = {2020},
  month = apr,
  abstract = {While image classification models have recently continued to advance, most downstream applications such as object detection and semantic segmentation still employ ResNet variants as the backbone network due to their simple and modular structure. We present a modular Split-Attention block that enables attention across feature-map groups. By stacking these Split-Attention blocks ResNet-style, we obtain a new ResNet variant which we call ResNeSt. Our network preserves the overall ResNet structure to be used in downstream tasks straightforwardly without introducing additional computational costs.},
  archivePrefix = {arXiv},
  eprint = {2004.08955},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2FHFSMJB\\2004.08955v1.pdf},
  journal = {arXiv:2004.08955 [cs]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,read},
  language = {en},
  primaryClass = {cs}
}

@article{zhangStudyOverfittingDeep2018,
  title = {A {{Study}} on {{Overfitting}} in {{Deep Reinforcement Learning}}},
  author = {Zhang, Chiyuan and Vinyals, Oriol and Munos, Remi and Bengio, Samy},
  year = {2018},
  month = apr,
  abstract = {Recent years have witnessed significant progresses in deep Reinforcement Learning (RL). Empowered with large scale neural networks, carefully designed architectures, novel training algorithms and massively parallel computing devices, researchers are able to attack many challenging RL problems. However, in machine learning, more training power comes with a potential risk of more overfitting. As deep RL techniques are being applied to critical problems such as healthcare and finance, it is important to understand the generalization behaviors of the trained agents. In this paper, we conduct a systematic study of standard RL agents and find that they could overfit in various ways. Moreover, overfitting could happen ``robustly'': commonly used techniques in RL that add stochasticity do not necessarily prevent or detect overfitting. In particular, the same agents and learning algorithms could have drastically different test performance, even when all of them achieve optimal rewards during training. The observations call for more principled and careful evaluation protocols in RL. We conclude with a general discussion on overfitting in RL and a study of the generalization behaviors from the perspective of inductive bias.},
  archivePrefix = {arXiv},
  eprint = {1804.06893},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VB8BPWA3\\Zhang et al. - 2018 - A Study on Overfitting in Deep Reinforcement Learn.pdf},
  journal = {arXiv:1804.06893 [cs, stat]},
  keywords = {Computer Science - Machine Learning,read-next,Statistics - Machine Learning},
  language = {en},
  primaryClass = {cs, stat}
}

@article{zhangUNDERSTANDINGDEEPLEARNING2017,
  title = {{{UNDERSTANDING DEEP LEARNING REQUIRES RE}}- {{THINKING GENERALIZATION}}},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz},
  year = {2017},
  pages = {15},
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VHGV8J9I\\Zhang et al. - 2017 - UNDERSTANDING DEEP LEARNING REQUIRES RE- THINKING .pdf},
  keywords = {read-next},
  language = {en}
}

@article{zhangUnderstandingDeepLearning2017,
  title = {Understanding Deep Learning Requires Rethinking Generalization},
  author = {Zhang, Chiyuan and Bengio, Samy and Hardt, Moritz and Recht, Benjamin and Vinyals, Oriol},
  year = {2017},
  month = feb,
  abstract = {Despite their massive size, successful deep artificial neural networks can exhibit a remarkably small difference between training and test performance. Conventional wisdom attributes small generalization error either to properties of the model family, or to the regularization techniques used during training. Through extensive systematic experiments, we show how these traditional approaches fail to explain why large neural networks generalize well in practice. Specifically, our experiments establish that state-of-the-art convolutional networks for image classification trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. We corroborate these experimental findings with a theoretical construction showing that simple depth two neural networks already have perfect finite sample expressivity as soon as the number of parameters exceeds the number of data points as it usually does in practice. We interpret our experimental findings by comparison with traditional models.},
  archivePrefix = {arXiv},
  eprint = {1611.03530},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\RHQ5FSDS\\Zhang et al. - 2017 - Understanding deep learning requires rethinking ge.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\6VC85S9K\\1611.html},
  journal = {arXiv:1611.03530 [cs]},
  keywords = {Computer Science - Machine Learning,generalization},
  note = {Comment: Published in ICLR 2017},
  primaryClass = {cs}
}

@article{zhuangAdaBeliefOptimizerAdapting2020,
  title = {{{AdaBelief Optimizer}}: {{Adapting Stepsizes}} by the {{Belief}} in {{Observed Gradients}}},
  shorttitle = {{{AdaBelief Optimizer}}},
  author = {Zhuang, Juntang and Tang, Tommy and Tatikonda, Sekhar and Dvornek, Nicha and Ding, Yifan and Papademetris, Xenophon and Duncan, James S.},
  year = {2020},
  month = oct,
  abstract = {Most popular optimizers for deep learning can be broadly categorized as adaptive methods (e.g. Adam) and accelerated schemes (e.g. stochastic gradient descent (SGD) with momentum). For many models such as convolutional neural networks (CNNs), adaptive methods typically converge faster but generalize worse compared to SGD; for complex settings such as generative adversarial networks (GANs), adaptive methods are typically the default because of their stability.We propose AdaBelief to simultaneously achieve three goals: fast convergence as in adaptive methods, good generalization as in SGD, and training stability. The intuition for AdaBelief is to adapt the stepsize according to the "belief" in the current gradient direction. Viewing the exponential moving average (EMA) of the noisy gradient as the prediction of the gradient at the next time step, if the observed gradient greatly deviates from the prediction, we distrust the current observation and take a small step; if the observed gradient is close to the prediction, we trust it and take a large step. We validate AdaBelief in extensive experiments, showing that it outperforms other methods with fast convergence and high accuracy on image classification and language modeling. Specifically, on ImageNet, AdaBelief achieves comparable accuracy to SGD. Furthermore, in the training of a GAN on Cifar10, AdaBelief demonstrates high stability and improves the quality of generated samples compared to a well-tuned Adam optimizer. Code is available at https://github.com/juntang-zhuang/Adabelief-Optimizer},
  archivePrefix = {arXiv},
  eprint = {2010.07468},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\2RR9WT7H\\Zhuang et al. - 2020 - AdaBelief Optimizer Adapting Stepsizes by the Bel.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\2ZV33HZC\\2010.html},
  journal = {arXiv:2010.07468 [cs, stat]},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,lr,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@incollection{zhuangDiscriminationawareChannelPruning2018,
  title = {Discrimination-Aware {{Channel Pruning}} for {{Deep Neural Networks}}},
  booktitle = {Advances in {{Neural Information Processing Systems}} 31},
  author = {Zhuang, Zhuangwei and Tan, Mingkui and Zhuang, Bohan and Liu, Jing and Guo, Yong and Wu, Qingyao and Huang, Junzhou and Zhu, Jinhui},
  editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and {Cesa-Bianchi}, N. and Garnett, R.},
  year = {2018},
  pages = {875--886},
  publisher = {{Curran Associates, Inc.}},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\VAEETDW6\\Zhuang et al. - 2018 - Discrimination-aware Channel Pruning for Deep Neur.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\8E3W52KT\\7367-discrimination-aware-channel-pruning-for-deep-neural-networks.html},
  keywords = {pruning,read-someday}
}

@article{zhuPruneNotPrune2017,
  title = {To Prune, or Not to Prune: Exploring the Efficacy of Pruning for Model Compression},
  shorttitle = {To Prune, or Not to Prune},
  author = {Zhu, Michael and Gupta, Suyog},
  year = {2017},
  month = nov,
  abstract = {Model pruning seeks to induce sparsity in a deep neural network's various connection matrices, thereby reducing the number of nonzero-valued parameters in the model. Recent reports (Han et al., 2015; Narang et al., 2017) prune deep networks at the cost of only a marginal loss in accuracy and achieve a sizable reduction in model size. This hints at the possibility that the baseline models in these experiments are perhaps severely over-parameterized at the outset and a viable alternative for model compression might be to simply reduce the number of hidden units while maintaining the model's dense connection structure, exposing a similar trade-off in model size and accuracy. We investigate these two distinct paths for model compression within the context of energy-efficient inference in resource-constrained environments and propose a new gradual pruning technique that is simple and straightforward to apply across a variety of models/datasets with minimal tuning and can be seamlessly incorporated within the training process. We compare the accuracy of large, but pruned models (large-sparse) and their smaller, but dense (small-dense) counterparts with identical memory footprint. Across a broad range of neural network architectures (deep CNNs, stacked LSTM, and seq2seq LSTM models), we find large-sparse models to consistently outperform small-dense models and achieve up to 10x reduction in number of non-zero parameters with minimal loss in accuracy.},
  archivePrefix = {arXiv},
  eprint = {1710.01878},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\S95G3UYZ\\Zhu and Gupta - 2017 - To prune, or not to prune exploring the efficacy .pdf;C\:\\Users\\ext1150\\Zotero\\storage\\Q62A6WY6\\1710.html},
  journal = {arXiv:1710.01878 [cs, stat]},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  primaryClass = {cs, stat}
}

@article{zingalesDoesFinanceBenefit,
  title = {Does {{Finance Benefit Society}}?},
  author = {Zingales, Luigi},
  pages = {42},
  abstract = {Academics' view of the benefits of finance vastly exceeds societal perception. This dissonance is at least partly explained by an under-appreciation by academia of how, without proper rules, finance can easily degenerate into a rent-seeking activity. I outline what finance academics can do, from a research point of view and from an educational point of view, to promote good finance and minimize the bad.},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\GL4YCQY6\\Zingales - Does Finance Benefit Society.pdf},
  language = {en}
}

@article{zintgrafVisualizingDeepNeural2017,
  title = {Visualizing Deep Neural Network Decisions: {{Prediction}} Difference Analysis},
  shorttitle = {Visualizing Deep Neural Network Decisions},
  author = {Zintgraf, Luisa M. and Cohen, Taco S. and Adel, Tameem and Welling, Max},
  year = {2017},
  archivePrefix = {arXiv},
  eprint = {1702.04595},
  eprinttype = {arxiv},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7TLA6JV9\\Zintgraf et al. - 2017 - Visualizing deep neural network decisions Predict.pdf;C\:\\Users\\ext1150\\Zotero\\storage\\EHPRRGMI\\1702.html},
  journal = {arXiv preprint arXiv:1702.04595},
  keywords = {read-next}
}

@misc{ZoteroYourPersonal,
  title = {Zotero | {{Your}} Personal Research Assistant},
  howpublished = {https://www.zotero.org/start}
}

@misc{ZuserechnenderraumPdf,
  title = {Zuserechnenderraum.Pdf},
  file = {C\:\\Users\\ext1150\\Zotero\\storage\\7TNQPJIN\\zuserechnenderraum.pdf}
}


